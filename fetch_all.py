#!/usr/bin/env python
# -*- coding: utf-8 -*-

# fetch_all.py â€” TDnetã®ã¿ã§æ±ºç®—å–å¾— + PDFè‡ªå‹•ã‚µãƒãƒª + è‰¯å¦åˆ¤å®šï¼ˆåˆ¤å®šç†ç”±ã‚’UIè¡¨ç¤ºï¼‰
# æ—¥åˆ¥æ•´ç†ã®UIæ”¹å–„ã‚’å«ã‚€å®Œå…¨ç‰ˆ

import json
from pathlib import Path
from datetime import datetime, timezone, timedelta
from typing import List, Dict, Any, Optional

import io
import re
import html
import requests
import sqlite3

# --- æ—¢å­˜ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ï¼ˆç’°å¢ƒã®ã¾ã¾ï¼‰ ---
from src.trends import fetch_trends
from src.news import fetch_news
from src.bbs import fetch_bbs_stats, fetch_leak_comments, fetch_tob_comments
from src.discovery import build_universe
from src.common import ensure_dir, dump_json, now_iso, load_config

ROOT = Path(__file__).resolve().parent
OUT_DIR = ROOT / "out"
OUT_BBS_DIR = ROOT / "out_bbs"
DASH_DIR = ROOT / "dashboard"
JST = timezone(timedelta(hours=9))
DB_PATH = r"H:\desctop\æ ªæ”»ç•¥\1-ã‚¹ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°è‡ªå‹•åŒ–ãƒ—ãƒ­ã‚°ãƒ©ãƒ \kani2.db"

# ------------------ ç°¡æ˜“ã‚»ãƒ³ãƒãƒ¡ãƒ³ãƒˆ ------------------
POS_KW = ["ä¸Šæ–¹ä¿®æ­£","å¢—ç›Š","æœ€é«˜ç›Š","éå»æœ€é«˜","å¢—é…","é»’å­—è»¢æ›","ä¸ŠæŒ¯ã‚Œ","å¥½èª¿","å¥½æ±ºç®—","ä¸Šä¹—ã›","å¤§å¹…å¢—","å…¬é–‹è²·ä»˜ã‘","å…¬é–‹è²·ã„ä»˜ã‘","TOB","ï¼´ï¼¯ï¼¢"]
NEG_KW = ["ä¸‹æ–¹ä¿®æ­£","æ¸›ç›Š","èµ¤å­—","æ¸›é…","æœªé”","ä¸‹æŒ¯ã‚Œ","ç‰¹æ","ä¸é©æ­£","ç›£ç†","å¤§å¹…æ¸›","æ¥­ç¸¾æ‚ªåŒ–"]

def _judge_sentiment(title: str) -> (str, int, str):
    t = title or ""
    for kw in POS_KW:
        if kw in t: return ("positive", +2, kw)
    for kw in NEG_KW:
        if kw in t: return ("negative", -2, kw)
    return ("neutral", 0, "")

# ------------------ TDnet: å–å¾— ------------------
TDNET_LIST_JSON = "https://webapi.yanoshin.jp/webapi/tdnet/list/{date_from}-{date_to}.json"

# ç”¨é€”åˆ¥ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã«åˆ†é›¢
EARNINGS_KW = ["æ±ºç®—","çŸ­ä¿¡","å››åŠæœŸ","é€šæœŸ","ä¸Šæ–¹ä¿®æ­£","ä¸‹æ–¹ä¿®æ­£","æ¥­ç¸¾","é…å½“","é€²æ—"]
# ï¼ˆå¢—è³‡ç³»ã¯æ—¢ã«ä¸‹ã® OFFERING_KW ãŒå®šç¾©æ¸ˆã¿ï¼‰


def _unescape_text(s: str) -> str:
    if not s: return ""
    try:
        if r"\u" in s:
            s = s.encode("utf-8").decode("unicode_escape")
    except Exception: pass
    return html.unescape(s).strip()


from datetime import datetime, timedelta, timezone
from typing import Dict, Any, List
import time



from urllib.parse import quote

def _http_get_json(url: str, retries: int = 3, sleep_sec: float = 0.8):
    import requests, time
    for i in range(retries):
        try:
            r = requests.get(url, timeout=15, headers={"User-Agent": "Mozilla/5.0"})
            r.raise_for_status()
            return r.json()
        except Exception:
            if i == retries - 1:
                return None
            time.sleep(sleep_sec)
    return None

def fetch_earnings_tdnet_only(days: int = 90, per_day_limit: int = 300,
                              slice_escalation=(12, 6, 3, 1, 0.5)) -> List[Dict[str, Any]]:
    # æ±ç”¨é–¢æ•°ã« EARNINGS_KW ã‚’æ¸¡ã™ã ã‘
    return fetch_tdnet_by_keywords(
        days=days,
        keywords=EARNINGS_KW,
        per_day_limit=per_day_limit,
        slice_escalation=slice_escalation
    )



def ensure_earnings_schema(conn):
    """
    earnings_events ã‚’æ—¥æœ¬èªã‚«ãƒ©ãƒ ã®ã¿ã®ã‚¹ã‚­ãƒ¼ãƒã«çµ±ä¸€ã€‚
    ä¸»ã‚­ãƒ¼ã¯æŒãŸãšã€UNIQUE(ã‚³ãƒ¼ãƒ‰, æå‡ºæ™‚åˆ») ã‚’ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã§ä¿è¨¼ã€‚
    """
    cur = conn.cursor()
    # æ–°è¦ or æ—¢å­˜ã§ã‚‚æµç”¨ã§ãã‚‹å½¢ã§ä½œæˆ
    cur.execute("""
    CREATE TABLE IF NOT EXISTS earnings_events(
        ã‚³ãƒ¼ãƒ‰       TEXT NOT NULL,
        éŠ˜æŸ„å       TEXT,
        ã‚¿ã‚¤ãƒˆãƒ«     TEXT,
        ãƒªãƒ³ã‚¯       TEXT,
        ç™ºè¡¨æ—¥æ™‚     TEXT,
        æå‡ºæ™‚åˆ»     TEXT NOT NULL,
        è¦ç´„         TEXT,
        åˆ¤å®š         TEXT,
        åˆ¤å®šã‚¹ã‚³ã‚¢   INTEGER,
        ç†ç”±JSON     TEXT,
        æŒ‡æ¨™JSON     TEXT,
        é€²æ—ç‡       REAL,
        ã‚»ãƒ³ãƒãƒ¡ãƒ³ãƒˆ TEXT,
        ç´ ç‚¹         INTEGER,
        created_at   TEXT DEFAULT (datetime('now','localtime'))
    );
    """)
    # æ—¢å­˜åˆ—ã«ä¸è¶³ãŒã‚ã‚Œã°è¿½åŠ ï¼ˆå®‰å…¨ç­–ï¼‰
    cur.execute("PRAGMA table_info(earnings_events);")
    existing = {r[1] for r in cur.fetchall()}
    def _add(col, typ):
        if col not in existing:
            cur.execute(f'ALTER TABLE earnings_events ADD COLUMN "{col}" {typ};')
    for col, typ in [
        ("ã‚³ãƒ¼ãƒ‰","TEXT NOT NULL"),
        ("éŠ˜æŸ„å","TEXT"),
        ("ã‚¿ã‚¤ãƒˆãƒ«","TEXT"),
        ("ãƒªãƒ³ã‚¯","TEXT"),
        ("ç™ºè¡¨æ—¥æ™‚","TEXT"),
        ("æå‡ºæ™‚åˆ»","TEXT NOT NULL"),
        ("è¦ç´„","TEXT"),
        ("åˆ¤å®š","TEXT"),
        ("åˆ¤å®šã‚¹ã‚³ã‚¢","INTEGER"),
        ("ç†ç”±JSON","TEXT"),
        ("æŒ‡æ¨™JSON","TEXT"),
        ("é€²æ—ç‡","REAL"),
        ("ã‚»ãƒ³ãƒãƒ¡ãƒ³ãƒˆ","TEXT"),
        ("ç´ ç‚¹","INTEGER"),
        ("created_at","TEXT"),
    ]:
        _add(col, typ)

    # å¤ã„è‹±èªç³»ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã¯å¿µã®ãŸã‚å‰Šé™¤
    cur.execute("DROP INDEX IF EXISTS idx_earn_time;")
    cur.execute("DROP INDEX IF EXISTS idx_earn_announced;")
    cur.execute("DROP INDEX IF EXISTS idx_earn_code_time;")

    # æ—¥æœ¬èªã‚«ãƒ©ãƒ ã«åˆã‚ã›ãŸã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ï¼ˆé‡è¤‡é˜²æ­¢ã¯ UNIQUEï¼‰
    cur.execute("""
      CREATE UNIQUE INDEX IF NOT EXISTS idx_earn_code_teishutsu
      ON earnings_events(ã‚³ãƒ¼ãƒ‰, æå‡ºæ™‚åˆ»);
    """)
    cur.execute("""
      CREATE INDEX IF NOT EXISTS idx_earn_teishutsu_desc
      ON earnings_events(æå‡ºæ™‚åˆ» DESC);
    """)
    conn.commit()

# ========= offerings_events: å¢—è³‡/è¡Œä½¿ ç³»ã‚¤ãƒ™ãƒ³ãƒˆä¿å­˜ =========

OFFERING_KW = [
    "å…¬å‹Ÿå¢—è³‡","ç¬¬ä¸‰è€…å‰²å½“","ç¬¬ä¸‰è€…å‰²å½“å¢—è³‡","æ–°æ ªç™ºè¡Œ","è‡ªå·±æ ªå¼ã®å‡¦åˆ†",
    "æ ªå¼ã®å£²å‡º","å‹Ÿé›†æ–°æ ªäºˆç´„æ¨©","æ–°æ ªäºˆç´„æ¨©ç™ºè¡Œ","MSãƒ¯ãƒ©ãƒ³ãƒˆ","ãƒ¯ãƒ©ãƒ³ãƒˆ",
    "è»¢æ›ç¤¾å‚µ","CB","EBå‚µ","ãƒ©ã‚¤ãƒ„ãƒ»ã‚ªãƒ•ã‚¡ãƒªãƒ³ã‚°",
    # è¡Œä½¿ç³»
    "è¡Œä½¿", "è¡Œä½¿çŠ¶æ³", "æ–°æ ªäºˆç´„æ¨©ã®è¡Œä½¿"
]

def ensure_offerings_schema(conn: sqlite3.Connection):
    cur = conn.cursor()
    cur.execute("""
    CREATE TABLE IF NOT EXISTS offerings_events(
        ã‚³ãƒ¼ãƒ‰       TEXT NOT NULL,
        éŠ˜æŸ„å       TEXT,
        ã‚¿ã‚¤ãƒˆãƒ«     TEXT,
        ãƒªãƒ³ã‚¯       TEXT,
        ç™ºè¡¨æ—¥æ™‚     TEXT,
        æå‡ºæ™‚åˆ»     TEXT NOT NULL,
        ç¨®åˆ¥         TEXT,         -- 'å¢—è³‡','è¡Œä½¿','å£²å‡º','CB' ãªã©ã®ã–ã£ãã‚Šåˆ†é¡
        å‚ç…§ID       TEXT,         -- TDnetã®idã‚„URLç­‰ï¼ˆã‚ã‚‹ã‚‚ã®ï¼‰
        ã‚»ãƒ³ãƒãƒ¡ãƒ³ãƒˆ TEXT,         -- ã‚¿ã‚¤ãƒˆãƒ«ã‹ã‚‰ã®ç°¡æ˜“æ¨å®š
        created_at   TEXT DEFAULT (datetime('now','localtime')),
        UNIQUE(ã‚³ãƒ¼ãƒ‰, æå‡ºæ™‚åˆ», ã‚¿ã‚¤ãƒˆãƒ«)
    );
    """)
    # ã‚ˆãä½¿ã†ç´¢å¼•
    cur.execute("""CREATE INDEX IF NOT EXISTS idx_offerings_date ON offerings_events(substr(æå‡ºæ™‚åˆ»,1,10));""")
    cur.execute("""CREATE INDEX IF NOT EXISTS idx_offerings_code_date ON offerings_events(ã‚³ãƒ¼ãƒ‰, substr(æå‡ºæ™‚åˆ»,1,10));""")
    conn.commit()

def load_offerings_recent_map(db_path: str, days: int = 365*3) -> dict[str, bool]:
    """
    offerings_events ã‹ã‚‰ç›´è¿‘daysæ—¥å†…ã«å¢—è³‡/è¡Œä½¿/å£²å‡º/CBãªã©ã®å±¥æ­´ãŒã‚ã£ãŸã‚³ãƒ¼ãƒ‰é›†åˆã‚’è¿”ã™ã€‚
    """
    out: dict[str, bool] = {}
    try:
        conn = sqlite3.connect(db_path)
        cur = conn.cursor()
        # offerings_events ãŒå­˜åœ¨ã—ãªã„å ´åˆã¯ç©º
        cur.execute("SELECT name FROM sqlite_master WHERE type='table' AND name='offerings_events'")
        if not cur.fetchone():
            conn.close()
            return out
        rows = cur.execute("""
            SELECT DISTINCT
                   CASE
                     WHEN length(ã‚³ãƒ¼ãƒ‰)=4 THEN ã‚³ãƒ¼ãƒ‰
                     WHEN length(ã‚³ãƒ¼ãƒ‰)=5 AND substr(ã‚³ãƒ¼ãƒ‰,1,1) BETWEEN '0' AND '9' THEN substr(ã‚³ãƒ¼ãƒ‰,1,4)
                     ELSE printf('%04d', CAST(ã‚³ãƒ¼ãƒ‰ AS INTEGER))
                   END AS code4
            FROM offerings_events
            WHERE date(æå‡ºæ™‚åˆ») >= date('now', ?)
        """, (f"-{days} day",)).fetchall()
        conn.close()
        for (c,) in rows:
            if c:
                out[str(c).zfill(4)] = True
    except Exception as e:
        print("[offerings] load_offerings_recent_map error:", e)
    return out

def _classify_offering_kind(title: str) -> str:
    t = title or ""
    if any(k in t for k in ["è¡Œä½¿","è¡Œä½¿çŠ¶æ³","æ–°æ ªäºˆç´„æ¨©ã®è¡Œä½¿"]): return "è¡Œä½¿"
    if any(k in t for k in ["å…¬å‹Ÿå¢—è³‡","ç¬¬ä¸‰è€…å‰²å½“","ç¬¬ä¸‰è€…å‰²å½“å¢—è³‡","æ–°æ ªç™ºè¡Œ","å‹Ÿé›†æ–°æ ªäºˆç´„æ¨©","æ–°æ ªäºˆç´„æ¨©ç™ºè¡Œ","MSãƒ¯ãƒ©ãƒ³ãƒˆ","ãƒ¯ãƒ©ãƒ³ãƒˆ","ãƒ©ã‚¤ãƒ„ãƒ»ã‚ªãƒ•ã‚¡ãƒªãƒ³ã‚°"]): return "å¢—è³‡"
    if any(k in t for k in ["æ ªå¼ã®å£²å‡º","è‡ªå·±æ ªå¼ã®å‡¦åˆ†"]): return "å£²å‡º/å‡¦åˆ†"
    if any(k in t for k in ["è»¢æ›ç¤¾å‚µ","CB","EBå‚µ"]): return "CB/EB"
    return "ãã®ä»–"

def upsert_offerings_events(conn: sqlite3.Connection, tdnet_items: list[dict]):
    """
    TDnet items ã‹ã‚‰ã€Œå¢—è³‡/è¡Œä½¿/å£²å‡º/CB ç­‰ã€ã ã‘ã‚’æŠ½å‡ºã—ã€offerings_events ã«UPSERTä¿å­˜ã€‚
    """
    import re, json
    cur = conn.cursor()

    def norm_code(it: dict) -> str:
        td = it.get("Tdnet", it) or {}
        raw = (td.get("company_code") or td.get("code") or td.get("company_code_raw") or "").strip()
        if re.fullmatch(r"\d{5}", raw): return raw[:4]
        if re.fullmatch(r"\d{4}", raw): return raw
        return (td.get("ticker") or "").strip()

    def norm_time(s: str) -> str:
        s = (s or "").replace("T"," ").replace("+09:00","").replace("/","-").strip()
        # ä¾‹å¤–æ™‚ã¯ç©ºã§è¿”ã™ï¼ˆNOT NULLåˆ—ã¯å¾Œã§è£œå®Œï¼‰
        return s

    count_upd = 0
    for it in tdnet_items or []:
        td = it.get("Tdnet", it) or {}
        title = (td.get("title") or "").strip()
        if not title: 
            continue
        # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒ’ãƒƒãƒˆåˆ¤å®š
        if not any(kw in title for kw in OFFERING_KW):
            continue

        code  = norm_code(it) or "0000"
        name  = (td.get("company_name") or "").strip() or code
        link  = (td.get("document_url") or td.get("pdf_url") or td.get("url") or "").strip()
        pub   = norm_time(td.get("pubdate") or td.get("publish_datetime") or "")
        tei   = pub or now_iso()  # æå‡ºæ™‚åˆ»ãŒç©ºãªã‚‰ç¾åœ¨æ™‚åˆ»ã‚’å…¥ã‚Œã¦ãŠã
        kind  = _classify_offering_kind(title)
        refid = td.get("id") or link or title
        # ã‚¿ã‚¤ãƒˆãƒ«ç°¡æ˜“ãƒ«ãƒ¼ãƒ«ã§ sentiment
        _, label, _ = _summarize_title_simple(title)

        # UPDATEï¼ˆç„¡ã‘ã‚Œã°INSERTï¼‰â€” UNIQUE(ã‚³ãƒ¼ãƒ‰,æå‡ºæ™‚åˆ»,ã‚¿ã‚¤ãƒˆãƒ«)
        cur.execute("""
            INSERT INTO offerings_events(ã‚³ãƒ¼ãƒ‰,éŠ˜æŸ„å,ã‚¿ã‚¤ãƒˆãƒ«,ãƒªãƒ³ã‚¯,ç™ºè¡¨æ—¥æ™‚,æå‡ºæ™‚åˆ»,ç¨®åˆ¥,å‚ç…§ID,ã‚»ãƒ³ãƒãƒ¡ãƒ³ãƒˆ)
            VALUES(?,?,?,?,?,?,?,?,?)
            ON CONFLICT(ã‚³ãƒ¼ãƒ‰,æå‡ºæ™‚åˆ»,ã‚¿ã‚¤ãƒˆãƒ«) DO UPDATE SET
              éŠ˜æŸ„å=excluded.éŠ˜æŸ„å,
              ãƒªãƒ³ã‚¯=excluded.ãƒªãƒ³ã‚¯,
              ç™ºè¡¨æ—¥æ™‚=excluded.ç™ºè¡¨æ—¥æ™‚,
              ç¨®åˆ¥=excluded.ç¨®åˆ¥,
              å‚ç…§ID=excluded.å‚ç…§ID,
              ã‚»ãƒ³ãƒãƒ¡ãƒ³ãƒˆ=excluded.ã‚»ãƒ³ãƒãƒ¡ãƒ³ãƒˆ
        """, (code,name,title,link,pub,tei,kind,refid,label))
        count_upd += 1

    conn.commit()
    if count_upd:
        print(f"[offerings] upsert rows: {count_upd}")


def upsert_earnings_rows(conn: sqlite3.Connection, rows: list[dict]):
    """
    TDnetæŠ½å‡º rows ã‚’æ—¥æœ¬èªã‚¹ã‚­ãƒ¼ãƒ earnings_events ã«UPSERTã€‚
    - å–ã‚Šè¾¼ã¿å¯¾è±¡ï¼šã€æ±ºç®—çŸ­ä¿¡ï¼å››åŠæœŸï¼é€šæœŸï¼æ±ºç®—ã€ã‚’å«ã¿ã€ã€å‹•ç”»ï¼èª¬æ˜è³‡æ–™ã€ã¯é™¤å¤–
    - ã‚­ãƒ¼ï¼š (ã‚³ãƒ¼ãƒ‰, æå‡ºæ™‚åˆ»)
    - UPDATE 0ä»¶ãªã‚‰ INSERT
    """
    import json, re
    cur = conn.cursor()

    # æ—¢å­˜åˆ—ï¼ˆå­˜åœ¨ã™ã‚‹åˆ—ã ã‘è©°ã‚ã‚‹ãŸã‚ï¼‰
    cur.execute("PRAGMA table_info(earnings_events);")
    cols = {r[1] for r in cur.fetchall()}

    # ãƒ’ãƒƒãƒˆæ¡ä»¶
    POS_KEYS = ("æ±ºç®—çŸ­ä¿¡","å››åŠæœŸ","é€šæœŸ","æ±ºç®—")
    EXCLUDE  = ("å‹•ç”»","èª¬æ˜è³‡æ–™")

    def norm_code(r: dict) -> str:
        cc = (r.get("company_code") or r.get("company_code_raw") or "").strip()
        if cc.isdigit() and len(cc) == 5:
            return cc[:4]
        # ãã‚Œä»¥å¤–ã¯ ticker ã‚’4æ¡ã‚¼ãƒ­åŸ‹ã‚
        return str(r.get("ticker") or "").zfill(4)

    for r in rows:
        title = (r.get("title") or "").strip()
        if not any(k in title for k in POS_KEYS):    # æ±ºç®—ç³»ã®ã¿
            continue
        if any(k in title for k in EXCLUDE):         # å‹•ç”»/èª¬æ˜è³‡æ–™ã¯ã‚¹ã‚­ãƒƒãƒ—
            continue

        code = norm_code(r)
        announced = (r.get("time") or "").replace("T"," ").replace("+09:00","").strip()
        # DBã®é‹ç”¨ã«åˆã‚ã›ã€Œæå‡ºæ™‚åˆ»ã€ã‚’ã‚­ãƒ¼ã«ã™ã‚‹ã€‚ç„¡ã‘ã‚Œã°ç™ºè¡¨æ—¥æ™‚ã§è£œå®Œ
        teishutsu = announced or datetime.now().strftime("%Y-%m-%d %H:%M:%S")

        # è¡Œãƒ‡ãƒ¼ã‚¿ï¼ˆæ—¥æœ¬èªã®ã¿ï¼‰
        jp_vals_all = {
            "ã‚³ãƒ¼ãƒ‰": code,
            "éŠ˜æŸ„å": r.get("name") or "",
            "ã‚¿ã‚¤ãƒˆãƒ«": title,
            "ãƒªãƒ³ã‚¯": r.get("link") or "",
            "ç™ºè¡¨æ—¥æ™‚": announced,
            "æå‡ºæ™‚åˆ»": teishutsu,
            "è¦ç´„": r.get("summary") or "",
            "åˆ¤å®š": (r.get("verdict") or "").strip(),
            "åˆ¤å®šã‚¹ã‚³ã‚¢": int(r.get("score_judge") or 0),
            "ç†ç”±JSON": json.dumps(r.get("reasons") or [], ensure_ascii=False),
            "æŒ‡æ¨™JSON": json.dumps(r.get("metrics") or {}, ensure_ascii=False),
            "é€²æ—ç‡": r.get("progress"),
            "ã‚»ãƒ³ãƒãƒ¡ãƒ³ãƒˆ": r.get("sentiment") or "",
            "ç´ ç‚¹": int(r.get("score") or 0),
        }

        # === UPDATE ===
        set_cols, set_vals = [], []
        for k, v in jp_vals_all.items():
            if k in cols and k not in ("ã‚³ãƒ¼ãƒ‰","æå‡ºæ™‚åˆ»") and v is not None:
                set_cols.append(f'"{k}"=?'); set_vals.append(v)

        updated = 0
        if set_cols:
            sql = (
                f'UPDATE earnings_events SET {", ".join(set_cols)} '
                f'WHERE "ã‚³ãƒ¼ãƒ‰"=? AND "æå‡ºæ™‚åˆ»"=?'
            )
            cur.execute(sql, set_vals + [jp_vals_all["ã‚³ãƒ¼ãƒ‰"], jp_vals_all["æå‡ºæ™‚åˆ»"]])
            updated = cur.rowcount

        # === INSERTï¼ˆãªã‘ã‚Œã°ï¼‰ ===
        if updated == 0:
            insert_cols, insert_vals = [], []
            for k, v in jp_vals_all.items():
                if k in cols and v is not None:
                    insert_cols.append(f'"{k}"'); insert_vals.append(v)
            placeholders = ",".join(["?"] * len(insert_cols))
            sql = f'INSERT OR IGNORE INTO earnings_events ({", ".join(insert_cols)}) VALUES ({placeholders})'
            cur.execute(sql, insert_vals)

    conn.commit()



def load_earnings_for_dashboard_from_db(db_path: str, days: int = 30, filter_mode: str = "nonnegative") -> list[dict]:
    """
    HTMLãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ç”¨ã® 'earnings' ã‚’ DB ã‹ã‚‰ç›´è¿‘Næ—¥åˆ†ã§å†æ§‹ç¯‰ã™ã‚‹ã€‚
    - filter_mode: "pos_good" | "nonnegative" | "all"
    ã•ã‚‰ã« price_history ã‹ã‚‰ ç¾åœ¨å€¤/å‰æ—¥çµ‚å€¤/å‰æ—¥æ¯”(å††) ã‚’ä»˜ä¸ã™ã‚‹ã€‚
    """
    import json as _json
    conn = sqlite3.connect(db_path)
    cur = conn.cursor()
    rows = cur.execute(
        """
        SELECT
          "ã‚³ãƒ¼ãƒ‰","éŠ˜æŸ„å","ã‚¿ã‚¤ãƒˆãƒ«","ãƒªãƒ³ã‚¯","ç™ºè¡¨æ—¥æ™‚","æå‡ºæ™‚åˆ»",
          "è¦ç´„","åˆ¤å®š","åˆ¤å®šã‚¹ã‚³ã‚¢","ç†ç”±JSON","æŒ‡æ¨™JSON","é€²æ—ç‡","ã‚»ãƒ³ãƒãƒ¡ãƒ³ãƒˆ","ç´ ç‚¹"
        FROM "v_events_dedup"
        WHERE date("æå‡ºæ™‚åˆ»") >= date('now', ?)
        ORDER BY "æå‡ºæ™‚åˆ»" DESC
        """,
        (f"-{days} day",)
    ).fetchall()

    def _to_row(r):
        try:
            reasons = _json.loads(r[9]) if r[9] else []
        except Exception:
            reasons = []
        try:
            metrics = _json.loads(r[10]) if r[10] else {}
        except Exception:
            metrics = {}
        return {
            "ticker": (r[0] or "0000"),
            "name":   r[1] or (r[0] or "TDnet"),
            "title":  r[2] or "",
            "link":   r[3] or "",
            "time":   (r[5] or "").replace("T"," ").replace("+09:00",""),
            "summary": r[6] or "",
            "verdict": (r[7] or "").strip(),
            "score_judge": r[8],
            "reasons": reasons if isinstance(reasons, list) else [str(reasons)],
            "metrics": metrics if isinstance(metrics, dict) else {},
            "progress": r[11],
            "sentiment": r[12] or "neutral",
            "score": r[13] or 0,
        }

    items = [_to_row(r) for r in rows]

    # ãƒ•ã‚£ãƒ«ã‚¿
    if filter_mode == "pos_good":
        items = [x for x in items if (x.get("sentiment") == "positive") or ((x.get("verdict") or "") == "good")]
    elif filter_mode == "nonnegative":
        items = [x for x in items if (x.get("sentiment") != "negative") or ((x.get("verdict") or "") == "good")]

    # ä¾¡æ ¼ãƒ‡ãƒ¼ã‚¿ä»˜ä¸ï¼ˆprice_history ã‹ã‚‰ï¼‰
    try:
        codes = list({str(x.get("ticker") or "0000").zfill(4) for x in items})
        quotes = load_quotes_from_price_history(conn, codes)
        for x in items:
            q = quotes.get(str(x["ticker"]).zfill(4))
            if q:
                x["quote"] = q
    except Exception as e:
        print("[earnings] quotes attach error:", e)

    conn.close()

    # ä¸¦ã¹æ›¿ãˆï¼†åˆ¶é™
    def _ts(s: str) -> float:
        from datetime import datetime
        try:
            return datetime.strptime((s or "").replace("/", "-"), "%Y-%m-%d %H:%M:%S").timestamp()
        except Exception:
            return 0.0
    items.sort(key=lambda r: _ts(r.get("time","")), reverse=True)
    return items[:300]


# ------------------ HTMLç”Ÿæˆï¼ˆæ—¥åˆ¥ã‚¿ãƒ–è¾¼ã¿ï¼‰ ------------------
def render_dashboard_html(payload: dict, api_base: str = "") -> str:
    import json
    data_json = json.dumps(payload, ensure_ascii=False)

    tpl = r"""<!doctype html>
<meta charset="utf-8" />
<title>æ³¨ç›®åº¦ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰</title>
<meta http-equiv="Cache-Control" content="no-cache, no-store, must-revalidate"/>
<meta http-equiv="Pragma" content="no-cache"/>
<meta http-equiv="Expires" content="0"/>
<style>
  :root { --br:#e5e7eb; --fg:#0f172a; --muted:#64748b; --bg:#fff; --pos:#16a34a; --neu:#6b7280; --neg:#dc2626; --tab:#3b82f6; }
  html,body{background:#fff}
  body{font-family:system-ui,-apple-system,Segoe UI,Roboto,'Noto Sans JP',sans-serif;margin:18px;color:var(--fg)}
  h1{font-size:26px;margin:0 0 10px}
  .toolbar{display:flex;gap:10px;align-items:center;margin:6px 0 14px}
  .tabs{display:flex;gap:8px}
  .tab{padding:8px 12px;border:1px solid var(--br);border-radius:999px;background:#f8fafc;color:#111;font-size:13px;cursor:pointer}
  .tab.active{background:#e6f0ff;border-color:#bfdbfe;color:#1d4ed8;font-weight:600}
  .stamp{font-size:12px;color:var(--muted);margin-left:8px}
  .card{border:1px solid var(--br);border-radius:12px;padding:12px;margin:14px 0;background:#fff}
  table{width:100%;border-collapse:collapse}
  th,td{border-bottom:1px solid #f1f5f9;padding:8px 10px;text-align:left;vertical-align:top}
  th{background:#f8fafc;color:#334155;font-weight:600;position:sticky;top:0}
  .codechip{display:inline-flex;align-items:center;gap:6px}
  .code{display:inline-block;background:#f1f5f9;border-radius:999px;padding:2px 8px;font-size:12px;color:#334155}
  .badge{display:inline-block;padding:2px 10px;border-radius:999px;color:#fff;font-size:12px;white-space:nowrap}
  .pos{background:var(--pos)} .neu{background:var(--neu)} .neg{background:var(--neg)}
  .newslist{display:grid;grid-template-columns:repeat(3,1fr);gap:10px;max-width:1100px}
  .newsitem{border:1px solid var(--br);border-radius:10px;padding:8px;background:#fff}
  .src{color:#64748b;font-size:12px}
  .btn{padding:8px 12px;border:1px solid var(--br);border-radius:10px;background:#f8fafc;cursor:pointer}
  .btn:hover{filter:brightness(0.98)}
  .small{font-size:12px;color:#64748b}
  a{color:#1d4ed8;text-decoration:none} a:hover{text-decoration:underline}
  .day-head{font-weight:700;margin:18px 0 8px}
  .muted{opacity:.85}
  td.num{text-align:right}
  td.diff.plus{color:#16a34a;font-weight:600}
  td.diff.minus{color:#dc2626;font-weight:600}
  .copybtn{cursor:pointer;text-decoration:none}
  .judge-row{ display:flex; gap:8px; align-items:center; flex-wrap:wrap; margin-top:6px; }
  .jdg{ display:inline-flex; align-items:center; gap:6px; padding:2px 10px; border-radius:999px; font-weight:700; border:1px solid var(--br); line-height:1.9; }
  .jdg .score{ font-weight:600; opacity:.8; }
  .jdg.positive{ color:#065f46; background:#ecfdf5; border-color:#a7f3d0; }
  .jdg.neutral { color:#374151; background:#f3f4f6; border-color:#e5e7eb; }
  .jdg.negative{ color:#7f1d1d; background:#fef2f2; border-color:#fecaca; }
  .ev{ padding:4px 10px; border-radius:12px; line-height:1.9; background:#f0f9ff; color:#0c4a6e; border-left:4px solid #38bdf8; }
  .ev .chip{ display:inline-block; margin:2px 6px 2px 0; padding:2px 8px; border-radius:999px; background:#e0f2fe; border:1px solid #bae6fd; font-weight:600; }
</style>

<h1>æ³¨ç›®åº¦ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰</h1>
<div class="toolbar">
  <div class="tabs">
    <button class="tab active" data-mode="total">ç·åˆ</button>
    <button class="tab" data-mode="bbs">æ²ç¤ºæ¿</button>
    <button class="tab" data-mode="earnings">æ±ºç®—</button>
    <button class="tab" data-mode="earnings_day">æ±ºç®—ï¼ˆæ—¥åˆ¥ï¼‰</button>
    <button class="tab" data-mode="leak">æ¼ã‚Œï¼Ÿ</button>
    <button class="tab" data-mode="tob">TOB</button>
  </div>
  <span id="stamp" class="stamp"></span>
  <button class="btn" id="btn-rerender">å†æç”»</button>
</div>

<div class="card">
  <table id="tbl">
    <thead id="thead"></thead>
    <tbody id="tbody"></tbody>
  </table>
</div>

<script>
  window.__DATA__ = __DATA_JSON__;
  const API_BASE = "";
  let MODE = "total";

  function esc(s){return (s??"").toString().replace(/[&<>\"']/g,m=>({"&":"&amp;","<":"&lt;",">":"&gt;","\"":"&quot;","'":"&#39;"}[m]));}
  function fmt(v){const n=Number(v||0);return isFinite(n)?n.toFixed(3):"0.000";}
  function badge(label){const cls=label==="positive"?"pos":(label==="negative"?"neg":"neu");return `<span class="badge ${cls}">${label||"neutral"}</span>`;}
  function parseJST(s){
    if(!s) return 0;
    const t = String(s).replace("T"," ").replace("+09:00","").trim();
    const m = t.match(/^(\d{4})-(\d{2})-(\d{2}) (\d{2}):(\d{2}):(\d{2})$/);
    if(m){
      const y=+m[1], mo=+m[2]-1, d=+m[3], h=+m[4], mi=+m[5], se=+m[6];
      const utcMs = Date.UTC(y, mo, d, h-9, mi, se);
      return isFinite(utcMs) ? utcMs : 0;
    }
    const dt = Date.parse(t+" +09:00");
    return isNaN(dt) ? 0 : dt;
  }
  function yQuoteUrl(code){const c=String(code||"").padStart(4,"0");return `https://finance.yahoo.co.jp/quote/${c}.T`;}
  function copyCode(code){
    navigator.clipboard?.writeText(String(code||""))?.then(()=>{})
  }
  function toNewsCards(items){
    if(!items||!items.length) return "";
    const html = items.slice(0,3).map(it=>{
      const t=esc(it.title||""); const l=esc(it.link||"#"); const s=esc(it.source||""); const p=esc(it.published||"");
      return `<div class="newsitem"><a href="${l}" target="_blank" rel="noopener">${t}</a><div class="src">${s}ã€€${p}</div></div>`;
    }).join("");
    return `<div class="newslist">${html}</div>`;
  }
  function stockCell(code,name){
    const c=String(code||"").padStart(4,"0"); const n=esc(name||c);
    return `<span class="codechip"><a class="copybtn" title="ã‚³ãƒ¼ãƒ‰ã‚’ã‚³ãƒ”ãƒ¼" href="javascript:void(0)" onclick="copyCode('${c}')">ğŸ“‹</a><span class="code">${c}</span><span>${n}</span></span>`;
  }

  function offeringCell(f){ return f ? `<span class="code">å¢—è³‡çµŒæ­´ã‚¢ãƒª</span>` : ``; }
  function verdictToLabel(v){
    if(v==="good") return "positive";
    if(v==="bad")  return "negative";
    return "neutral";
  }
  function judgeHtml(verdict, score, reasons){
    const cls = verdictToLabel(verdict||"");
    const s = (score ?? null) !== null ? `ï¼ˆscore ${esc(score)}ï¼‰` : "";
    const chips = (Array.isArray(reasons)?reasons:[])
      .filter(x=>x!=null && String(x).trim()!=="")
      .map(x=>`<span class="chip">${esc(String(x))}</span>`).join("");
    const ev = chips ? `<span class="ev">æ ¹æ‹ : ${chips}</span>` : "";
    return `<div class="judge-row"><span class="jdg ${cls}">åˆ¤å®š: ${esc(verdict||"")}<span class="score">${s}</span></span>${ev}</div>`;
  }

  function render(j){
    document.getElementById("stamp").textContent = "ç”Ÿæˆæ™‚åˆ»: " + (j.generated_at || "");
    const thead=document.getElementById("thead");
    const tbody=document.getElementById("tbody");
    tbody.innerHTML="";

    if(MODE==="earnings_day"){
      const rows=(j.earnings||[])
        .slice()
        .sort((a,b)=>{const tb=parseJST(b.time),ta=parseJST(a.time);return tb!==ta?tb-ta:(Number(b.score||0)-Number(a.score||0));})
        .slice(0, 300);
      thead.innerHTML=`<tr>
        <th>#</th><th>ã‚³ãƒ¼ãƒ‰</th><th>éŠ˜æŸ„</th><th>å¢—è³‡çµŒæ­´</th><th>Yahoo</th>
        <th class="num">ç¾åœ¨å€¤</th><th class="num">å‰æ—¥çµ‚å€¤</th><th class="num">å‰æ—¥æ¯”(å††)</th>
        <th>ã‚¹ã‚³ã‚¢</th><th>Sentiment</th><th>æ›¸é¡å / ã‚µãƒãƒª / åˆ¤å®šç†ç”±</th><th>æ™‚åˆ»</th></tr>`;
      if(!rows.length){const tr=document.createElement("tr");tr.innerHTML=`<td colspan="11" class="small">ç›´è¿‘ã§ã¯è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚</td>`;tbody.appendChild(tr);return;}
      let day=""; rows.forEach((r,idx)=>{
        const d=(r.time||"").slice(0,10);
        if(d!==day){day=d; const trh=document.createElement("tr"); trh.innerHTML=`<td colspan="11" class="day-head">${esc(day)}</td>`; tbody.appendChild(trh);}
        const q=r.quote||{};
        const cur = q.current!=null ? Number(q.current).toLocaleString() : "";
        const prev= q.prev_close!=null ? Number(q.prev_close).toLocaleString() : "";
        const diff= q.diff_yen!=null ? Number(q.diff_yen) : null;
        const diffStr = diff!=null ? ((diff>=0?"+":"")+diff.toLocaleString()) : "";
        const diffCls = diff==null ? "" : (diff>=0?"plus":"minus");

        const tr=document.createElement("tr"); const t=(h,c)=>{const el=document.createElement("td"); if(c) el.className=c; el.innerHTML=h; tr.appendChild(el);};
        t(String(idx+1));
        t(`<a class="copybtn" title="ã‚³ãƒ”ãƒ¼" href="javascript:void(0)" onclick="copyCode('${esc(r.ticker)}')">ğŸ“‹</a> ${esc(r.ticker)}`);
        t(stockCell(r.ticker,r.name)); t(offeringCell(r.has_offering_history)); t(`<a href="${yQuoteUrl(r.ticker)}" target="_blank" rel="noopener">Yahoo</a>`);
        t(cur, "num"); t(prev, "num"); t(diffStr, "num diff "+diffCls);
        t(String(r.score??0)); t(badge(r.sentiment||"neutral"));
        const titleHtml=r.link?`<a href="${esc(r.link)}" target="_blank" rel="noopener">${esc(r.title||"(ç„¡é¡Œ)")}</a>`:esc(r.title||"(ç„¡é¡Œ)");
        const summaryHtml=r.summary?`<div class="small muted" style="margin-top:6px; white-space:pre-line;">${esc(r.summary)}</div>`:"";
        const judgeBlock = judgeHtml(r.verdict||"", r.score_judge, r.reasons||[]);
        t(`${titleHtml}${summaryHtml}${judgeBlock}`);
        t(esc((r.time||"").replace("T"," ").replace("+09:00","")));
        tbody.appendChild(tr);
      });
      return;
    }

    if(MODE==="earnings"){
      const rows=(j.earnings||[])
        .slice()
        .sort((a,b)=>{const tb=parseJST(b.time),ta=parseJST(a.time);return tb!==ta?tb-ta:(Number(b.score||0)-Number(a.score||0));})
        .slice(0, 300);
      thead.innerHTML=`<tr>
        <th>#</th><th>ã‚³ãƒ¼ãƒ‰</th><th>éŠ˜æŸ„</th><th>å¢—è³‡çµŒæ­´</th><th>Yahoo</th>
        <th class="num">ç¾åœ¨å€¤</th><th class="num">å‰æ—¥çµ‚å€¤</th><th class="num">å‰æ—¥æ¯”(å††)</th>
        <th>ã‚¹ã‚³ã‚¢</th><th>Sentiment</th><th>æ›¸é¡å / ã‚µãƒãƒª / åˆ¤å®šç†ç”±</th><th>æ™‚åˆ»</th></tr>`;
      if(!rows.length){const tr=document.createElement("tr");tr.innerHTML=`<td colspan="11" class="small">ç›´è¿‘ã§ã¯è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚</td>`;tbody.appendChild(tr);return;}
      rows.forEach((r,idx)=>{
        const q=r.quote||{};
        const cur = q.current!=null ? Number(q.current).toLocaleString() : "";
        const prev= q.prev_close!=null ? Number(q.prev_close).toLocaleString() : "";
        const diff= q.diff_yen!=null ? Number(q.diff_yen) : null;
        const diffStr = diff!=null ? ((diff>=0?"+":"")+diff.toLocaleString()) : "";
        const diffCls = diff==null ? "" : (diff>=0?"plus":"minus");

        const tr=document.createElement("tr"); const t=(h,c)=>{const el=document.createElement("td"); if(c) el.className=c; el.innerHTML=h; tr.appendChild(el);};
        t(String(idx+1));
        t(`<a class="copybtn" title="ã‚³ãƒ”ãƒ¼" href="javascript:void(0)" onclick="copyCode('${esc(r.ticker)}')">ğŸ“‹</a> ${esc(r.ticker)}`);
        t(stockCell(r.ticker,r.name)); t(offeringCell(r.has_offering_history)); t(`<a href="${yQuoteUrl(r.ticker)}" target="_blank" rel="noopener">Yahoo</a>`);
        t(cur, "num"); t(prev, "num"); t(diffStr, "num diff "+diffCls);
        t(String(r.score??0)); t(badge(r.sentiment||"neutral"));
        const titleHtml=r.link?`<a href="${esc(r.link)}" target="_blank" rel="noopener">${esc(r.title||"(ç„¡é¡Œ)")}</a>`:esc(r.title||"(ç„¡é¡Œ)");
        const summaryHtml=r.summary?`<div class="small muted" style="margin-top:6px; white-space:pre-line;">${esc(r.summary)}</div>`:"";
        const judgeBlock = judgeHtml(r.verdict||"", r.score_judge, r.reasons||[]);
        t(`${titleHtml}${summaryHtml}${judgeBlock}`);
        t(esc((r.time||"").replace("T"," ").replace("+09:00","")));
        tbody.appendChild(tr);
      });
      return;
    }

    if(MODE==="bbs"){
      const rows=(j.rows||[]).slice().sort((a,b)=>{
        const a24=a?.bbs?.posts_24h||0, b24=b?.bbs?.posts_24h||0; if(b24!==a24) return b24-a24;
        const a72=a?.bbs?.posts_72h||0, b72=b?.bbs?.posts_72h||0; if(b72!==a72) return b72-a72;
        return String(a.name||a.ticker).localeCompare(String(b.name||b.ticker));
      });
      thead.innerHTML=`<tr><th>#</th><th>éŠ˜æŸ„</th><th>å¢—è³‡çµŒæ­´</th><th>BBS(24h)</th><th>BBS(72h)</th><th>å¢—åŠ ç‡</th><th>Sentiment(News)</th><th>æœ€æ–°ãƒ‹ãƒ¥ãƒ¼ã‚¹</th></tr>`;
      rows.forEach((r,idx)=>{
        const tr=document.createElement("tr"); const t=(h)=>{const el=document.createElement("td"); el.innerHTML=h; tr.appendChild(el);};
        const b24=r?.bbs?.posts_24h||0, b72=r?.bbs?.posts_72h||0, growth=(b24/Math.max(1,b72)).toFixed(2);
        const items=(r.news&&r.news.items)?r.news.items:[]; const label=(items[0]?.sentiment)||"neutral";
        t(String(idx+1)); t(`${esc(r.name||r.ticker)} <span class="code">(${esc(r.ticker)})</span>`); t(offeringCell(r.has_offering_history)); t(offeringCell(r.has_offering_history)); t(String(b24)); t(String(b72)); t(String(growth)); t(badge(label)); t(toNewsCards(items));
        tbody.appendChild(tr);
      });
      return;
    }

    // å…±é€š: æ¤œç´¢ç³»ï¼ˆæ¼ã‚Œï¼Ÿ / TOBï¼‰ãƒ¬ãƒ³ãƒ€ãƒ©
    function renderAggRows(agg){
      const rows=(agg?.rows||[]).slice().sort((a,b)=> (b.count||0)-(a.count||0));
      thead.innerHTML = `<tr>
        <th>#</th><th>éŠ˜æŸ„</th><th>ä»¶æ•°</th><th>ã‚µãƒ³ãƒ—ãƒ«ï¼ˆæœ€å¤§50ä»¶ï¼‰</th>
      </tr>`;
      if(!rows.length){
        const tr=document.createElement("tr");
        tr.innerHTML=`<td colspan="4" class="small">è©²å½“ãŒã‚ã‚Šã¾ã›ã‚“ã€‚</td>`;
        tbody.appendChild(tr);
        return;
      }
      rows.forEach((r,idx)=>{
        const tr=document.createElement("tr");
        const t=(h)=>{const el=document.createElement("td"); el.innerHTML=h; tr.appendChild(el);};
        const samples=(r.comments||[]).slice(0,50).map(c=>{
          const lab = c.sentiment||"neutral";
          const badgeCls = lab==="positive" ? "pos" : (lab==="negative"?"neg":"neu");
          const text = esc(c.text||"");
          const link = esc(c.link||"#");
          const pub  = esc((c.published||"").toString());
          return `<div class="newsitem">
                    <a href="${link}" target="_blank" rel="noopener">${text}</a>
                    <div class="src"><span class="badge ${badgeCls}">${lab}</span>ã€€${pub}</div>
                  </div>`;
        }).join("");
        t(String(idx+1));
        t(`${esc(r.name||r.ticker)} <span class="code">(${esc(r.ticker)})</span>`);
        t(String(r.count||0));
        t(`<div class="newslist">${samples}</div>`);
        tbody.appendChild(tr);
      });
    }

    if(MODE==="leak"){
      renderAggRows((j.leak||{}));
      return;
    }
    if(MODE==="tob"){
      renderAggRows((j.tob||{}));
      return;
    }

    // ç·åˆ
    const rows=(j.rows||[]).slice().sort((a,b)=>(b.score||0)-(a.score||0));
    thead.innerHTML=`<tr><th>#</th><th>éŠ˜æŸ„</th><th>å¢—è³‡çµŒæ­´</th><th>ã‚¹ã‚³ã‚¢</th><th>Trends</th><th>BBS</th><th>News(24h)</th><th>Sentiment</th><th>æœ€æ–°ãƒ‹ãƒ¥ãƒ¼ã‚¹</th></tr>`;
    rows.forEach((r,idx)=>{
      const tr=document.createElement("tr"); const t=(h)=>{const el=document.createElement("td"); el.innerHTML=h; tr.appendChild(el);};
      const items=(r.news&&r.news.items)?r.news.items:[]; const label=(items[0]?.sentiment)||"neutral";
      t(String(idx+1)); t(`${esc(r.name||r.ticker)} <span class="code">(${esc(r.ticker)})</span>`); t(fmt(r.score)); t(fmt(r.trends?.latest)); t(String(r.bbs?.posts_24h||0)); t(String(r.news?.count_24h||0)); t(badge(label)); t(toNewsCards(items));
      tbody.appendChild(tr);
    });
  }

  document.addEventListener("DOMContentLoaded", ()=>{
    document.querySelectorAll(".tab").forEach(btn=>{
      btn.addEventListener("click", ()=> { MODE=btn.dataset.mode; render(window.__DATA__); document.querySelectorAll(".tab").forEach(b=>b.classList.toggle("active", b===btn)); });
    });
    document.getElementById("btn-rerender").addEventListener("click", ()=> render(window.__DATA__));
    render(window.__DATA__);
  });
</script>
"""
    return tpl.replace("__DATA_JSON__", data_json)


import re
from datetime import datetime

def _summarize_title_simple(title: str):
    t = title or ""
    POS = [
        (r"(å¢—é…|é…å½“äºˆæƒ³ã®ä¿®æ­£.*å¢—é…)", "å¢—é…", "ã‚¿ã‚¤ãƒˆãƒ«ã«ã€å¢—é…ã€ã‚’æ¤œå‡º"),
        (r"(ä¸Šæ–¹ä¿®æ­£|ä¸Šæ–¹è¦å› |å¢—é¡ä¿®æ­£|ä¸ŠæŒ¯ã‚Œ)", "ä¸Šæ–¹ä¿®æ­£", "ã‚¿ã‚¤ãƒˆãƒ«ã«ã€ä¸Šæ–¹ä¿®æ­£/å¢—é¡ã€ã‚’æ¤œå‡º"),
        (r"(å…¬é–‹è²·ä»˜ã‘|å…¬é–‹è²·ã„ä»˜ã‘|TOB|ï¼´ï¼¯ï¼¢)", "TOB/å…¬é–‹è²·ä»˜ã‘", "TOB/å…¬é–‹è²·ä»˜ã‘ã‚’æ¤œå‡º"),
    ]
    NEG = [
        (r"(æ¸›é…|ç„¡é…|ä¸‹æ–¹ä¿®æ­£|ä¸‹æ–¹è¦å› |æ¸›é¡ä¿®æ­£|ä¸‹æŒ¯ã‚Œ)", "ä¸‹æ–¹ä¿®æ­£/æ¸›é…", "ã‚¿ã‚¤ãƒˆãƒ«ã«ã€ä¸‹æ–¹/æ¸›é…ã€ã‚’æ¤œå‡º"),
    ]
    for pat, summ, why in POS:
        if re.search(pat, t): return summ, "positive", why
    for pat, summ, why in NEG:
        if re.search(pat, t): return summ, "negative", why
    if re.search(r"(æ±ºç®—çŸ­ä¿¡|æ±ºç®—è£œè¶³|æ±ºç®—èª¬æ˜è³‡æ–™|æ±ºç®—æ¦‚è¦|æ±ºç®—çŸ­ä¿¡.*è¨‚æ­£)", t):
        return "æ±ºç®—çŸ­ä¿¡/è³‡æ–™", "neutral", "æ±ºç®—è³‡æ–™ç³»ï¼ˆæ¥µç«¯ãªãƒã‚¸/ãƒã‚¬èªãªã—ï¼‰"
    if re.search(r"(é…å½“äºˆæƒ³ã®ä¿®æ­£|é…å½“æ–¹é‡|ä¸­é–“é…å½“äºˆå®š|æœŸæœ«é…å½“äºˆå®š)", t):
        return "é…å½“é–¢é€£", "neutral", "é…å½“é–¢é€£ï¼ˆå¢—é…/æ¸›é…ã®èªã¯ç„¡ã—ï¼‰"
    if re.search(r"(æ¥­ç¸¾äºˆæƒ³ã®ä¿®æ­£|é€šæœŸæ¥­ç¸¾äºˆæƒ³|å››åŠæœŸæ¥­ç¸¾äºˆæƒ³)", t):
        return "æ¥­ç¸¾äºˆæƒ³", "neutral", "æ¥­ç¸¾äºˆæƒ³ï¼ˆä¸Šæ–¹/ä¸‹æ–¹ã®èªã¯ç„¡ã—ï¼‰"
    return "", "neutral", ""

# ========= PDF è¦ç´„ï¼†åˆ¤å®šãƒ˜ãƒ«ãƒ‘ãƒ¼ å¾©æ´»ç‰ˆ =========
import io, re, time, requests
from typing import Dict, Any, List

# ä¸€åº¦ã®å®Ÿè¡Œã§è¦ç´„ã™ã‚‹æœ€å¤§ä»¶æ•°ï¼ˆè² è·å¯¾ç­–ï¼‰
EARNINGS_SUMMARY_MAX = 40

def _download_pdf_bytes(url: str, timeout: int = 25) -> bytes:
    """
    PDFã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã€‚Yanoshinâ†’TDnetâ†’PDFç›´ãƒªãƒ³ã‚¯ã®3xxã‚’è¾¿ã‚‹ã“ã¨ãŒã‚ã‚‹ãŸã‚ allow_redirects=Trueã€‚
    """
    try:
        r = requests.get(url, timeout=timeout, headers={"User-Agent":"Mozilla/5.0"}, allow_redirects=True)
        r.raise_for_status()
        # ä¸€éƒ¨ã§ Content-Type ãŒ text/html ã®ã¨ãã‚‚PDFãŒå…¥ã£ã¦ã„ã‚‹ã®ã§ contentå„ªå…ˆ
        return r.content or b""
    except Exception as e:
        print(f"[pdf] download error: {e} url={url}")
        return b""

def _extract_text_pdfminer(pdf_bytes: bytes) -> str:
    try:
        from pdfminer.high_level import extract_text
        with io.BytesIO(pdf_bytes) as bio:
            return (extract_text(bio) or "").strip()
    except Exception as e:
        print(f"[pdfminer] extract error: {e}")
        return ""

def _extract_text_pypdf2(pdf_bytes: bytes) -> str:
    try:
        import PyPDF2
        out = []
        reader = PyPDF2.PdfReader(io.BytesIO(pdf_bytes))
        for page in reader.pages:
            try:
                out.append(page.extract_text() or "")
            except Exception:
                out.append("")
        return "\n".join(out).strip()
    except Exception as e:
        print(f"[PyPDF2] extract error: {e}")
        return ""

def _extract_text_from_pdf(pdf_bytes: bytes) -> str:
    """
    pdfminer â†’ PyPDF2 ã®é †ã§è©¦ã™ã€‚ä¸¡æ–¹ãƒ€ãƒ¡ãªã‚‰ç©ºæ–‡å­—ã€‚
    """
    if not pdf_bytes:
        return ""
    text = _extract_text_pdfminer(pdf_bytes)
    if text and len(text) > 30:
        return text
    text = _extract_text_pypdf2(pdf_bytes)
    return text

def _parse_earnings_metrics(text: str) -> Dict[str, Any]:
    """
    PDFæœ¬æ–‡ã‹ã‚‰ã‚·ãƒ³ãƒ—ãƒ«ãªæŒ‡æ¨™ã‚’æŠ½å‡ºï¼ˆå£²ä¸Šé«˜/å–¶æ¥­åˆ©ç›Š/çµŒå¸¸åˆ©ç›Š/ç´”åˆ©ç›Š/EPS/é€²æ—ç‡ï¼‰ã€‚
    æ•°å­—ã¯å„„å††/ç™¾ä¸‡å††/å††ãªã©ã®å˜ä½ã«å¤§é›‘æŠŠå¯¾å¿œã€‚è¦‹ã¤ã‹ã‚‰ãªã‘ã‚Œã°ç©ºã€‚
    """
    if not text:
        return {"metrics": {}, "progress": None}

    def _num(s):
        # ã‚«ãƒ³ãƒ/å…¨è§’/å††ã‚„%é™¤å»â†’float
        s = s.replace(",", "").replace("ï¼Œ", "").replace("ï¼…", "%").replace("â–²", "-").replace("â–³", "-")
        s = re.sub(r"[^\d\.\-\+%]", "", s)
        try:
            if s.endswith("%"):
                return float(s[:-1])
            return float(s)
        except:
            return None

    # å˜ä½ã®ã–ã£ãã‚Šè£œæ­£ï¼ˆç™¾ä¸‡å††â†’å„„å††æ›ç®—ãªã©ã¯å¿…è¦ã«å¿œã˜ã¦ï¼‰
    UNIT_PAT = r"(ç™¾ä¸‡å††|å„„å††|ä¸‡å††|å††)?"

    fields = {
        "å£²ä¸Šé«˜": r"(å£²ä¸Šé«˜)[^0-9\-ï¼‹\+\,\.ï¼…%\(\)]*([0-9,\.\-\+]+)\s*" + UNIT_PAT,
        "å–¶æ¥­åˆ©ç›Š": r"(å–¶æ¥­åˆ©ç›Š)[^0-9\-ï¼‹\+\,\.ï¼…%\(\)]*([0-9,\.\-\+]+)\s*" + UNIT_PAT,
        "çµŒå¸¸åˆ©ç›Š": r"(çµŒå¸¸åˆ©ç›Š)[^0-9\-ï¼‹\+\,\.ï¼…%\(\)]*([0-9,\.\-\+]+)\s*" + UNIT_PAT,
        "ç´”åˆ©ç›Š": r"(å½“æœŸç´”åˆ©ç›Š|è¦ªä¼šç¤¾æ ªä¸»ã«å¸°å±ã™ã‚‹å½“æœŸç´”åˆ©ç›Š|ç´”åˆ©ç›Š)[^0-9\-ï¼‹\+\,\.ï¼…%\(\)]*([0-9,\.\-\+]+)\s*" + UNIT_PAT,
        "EPS": r"(EPS|1æ ªå½“ãŸã‚Šå½“æœŸç´”åˆ©ç›Š)[^0-9\-ï¼‹\+\,\.ï¼…%\(\)]*([0-9,\.\-\+]+)\s*" + UNIT_PAT,
        "é€²æ—ç‡": r"(é€²æ—ç‡)[^0-9\-ï¼‹\+\,\.ï¼…%\(\)]*([0-9,\.\-\+]+)\s*%",
    }

    metrics = {}
    yoy = {}

    # å‰å¾Œ100ã€œ200æ–‡å­—ã‚’è¦‹ã¦YoY%ã‚’æ‹¾ã†ãƒ˜ãƒ«ãƒ‘
    def _scan_yoy_near(pos: int, text: str):
        if pos < 0: return None
        s = text[max(0, pos-120): pos+220]
        s = s.replace('ï¼…','%').replace('â–²','-').replace('â–³','-')
        # ã¾ãš % ã‚’æ‹¾ã†
        m = re.search(r'([\-+]?\d+(?:\.\d+)?)\s*%', s)
        if not m: return None
        try:
            val = float(m.group(1))
        except Exception:
            return None
        # è¿‘å‚ã«ã€Œæ¸›/æ‚ªåŒ–ã€ãŒã‚ã£ã¦æ˜ç¤ºç¬¦å·ãŒç„¡ã‘ã‚Œã°è² å·ã«
        around = s[max(0, m.start()-10): m.end()+10]
        if ('-' not in m.group(1) and '+' not in m.group(1)):
            if re.search(r'(æ¸›|æ‚ªåŒ–|ç¸®å°)', around):
                val = -val
        return val

    for key, pat in fields.items():
        m = re.search(pat, text, flags=re.IGNORECASE)
        if m:
            raw = m.group(2) if len(m.groups()) >= 2 else None
            val = _num(raw or "")
            if val is not None:
                # å˜ä½ãŒã€Œç™¾ä¸‡å††ã€ã®å ´åˆã¯ â€œå„„å††â€ æ›ç®—ï¼ˆ/100ï¼‰ã—ã¦ãŠã
                unit = m.group(3) if len(m.groups()) >= 3 else ""
                if key != "EPS" and key != "é€²æ—ç‡" and unit:
                    u = unit
                    if "ç™¾ä¸‡å††" in u:
                        val = val / 100.0  # ç™¾ä¸‡å††â†’å„„å††
                metrics[key] = val

            # YoYã‚’è¿‘å‚ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰æ¨å®š
            try:
                # æŒ‡æ¨™åã®æœ€åˆã®å‡ºç¾ä½ç½®
                pos = text.find(key)
                y = _scan_yoy_near(pos, text)
                if y is not None:
                    yoy[key] = y
            except Exception:
                pass

    progress = None
    if "é€²æ—ç‡" in metrics:
        progress = float(metrics["é€²æ—ç‡"])
    return {"metrics": metrics, "progress": progress, "yoy": yoy}

def _summarize_earnings_text(text: str, max_chars: int = 240) -> str:
    """
    é‡è¦èªã‚’å„ªå…ˆã—ã¦çŸ­ã„è¦ç´„ã‚’ä½œã‚‹ã€‚ç„¡ã‘ã‚Œã°å…ˆé ­æ•°è¡Œã‚’æŠœç²‹ã€‚
    """
    if not text:
        return ""
    lines = [l.strip() for l in text.splitlines() if l.strip()]
    # å„ªå…ˆèªãŒå«ã¾ã‚Œã‚‹è¡Œã‚’æ‹¾ã†
    KEY = [
        "ä¸Šæ–¹ä¿®æ­£", "ä¸‹æ–¹ä¿®æ­£", "é€šæœŸ", "å››åŠæœŸ", "å¢—ç›Š", "æ¸›ç›Š", "å¢—å", "æ¸›å",
        "é€²æ—ç‡", "é…å½“", "æ¥­ç¸¾äºˆæƒ³", "ä¿®æ­£"
    ]
    hits = [l for l in lines if any(k in l for k in KEY)]
    base = " / ".join(hits[:4]) if hits else " ".join(lines[:6])
    base = re.sub(r"\s+", " ", base).strip()
    return base[:max_chars]

def _grade_earnings(title: str, text: str, parsed: Dict[str, Any]) -> Dict[str, Any]:
    """
    ã‚¿ã‚¤ãƒˆãƒ«ï¼‹æœ¬æ–‡ï¼‹æŠ½å‡ºæŒ‡æ¨™ã‹ã‚‰ â€œgood/bad/neutralâ€ ã¨ã‚¹ã‚³ã‚¢ï¼†æ ¹æ‹ ã‚’è¿”ã™ã€‚
    - æ¸›ç›Š/ä¸‹æ–¹ãªã©ã¯å¼·ãƒã‚¬å„ªå…ˆ
    - ã€Œé»’å­—ã€å˜ç‹¬ã¯åŠ ç‚¹ã—ãªã„ï¼ˆé»’å­—â€œè»¢æ›â€ã®ã¿åŠ ç‚¹ï¼‰
    - YoY ã¯å¤–ã‚Œå€¤ï¼ˆÂ±120%è¶…ï¼‰ã¯ç„¡è¦–ã—ã€è¤‡æ•°åˆ©ç›Šé …ç›®ã®æ¸›å°‘ã‚’å¼·ãæ¸›ç‚¹
    """
    reasons: list[str] = []
    score = 0.0

    T = title or ""
    X = text or ""
    met = (parsed or {}).get("metrics", {})
    prog = (parsed or {}).get("progress", None)
    yoy = (parsed or {}).get("yoy", {})

    # ---- ã“ã“ã‹ã‚‰å€‹åˆ¥æ¡ç‚¹ãƒ«ãƒ¼ãƒ«ï¼ˆãƒãƒ³ãƒ‰ãƒ«åŠ ç‚¹ã‚’å»ƒæ­¢ï¼‰----
    # ãƒ«ãƒ¼ãƒ«ã¯ã€Œè¦‹å‡ºã—(T)ã€ã¨ã€Œæœ¬æ–‡(X)ã€ã®ä¸¡æ–¹ã‚’è¦‹ã‚‹
    def _hit(ws):  # ä»»æ„ã®èªãŒå«ã¾ã‚Œã‚Œã° True
        return any((w in T) or (w in X) for w in ws)

    # â–¼ ãƒã‚¸è¦å› 
    # ä¸Šæ–¹ä¿®æ­£ç³»ã¯ +2ï¼ˆé€šæœŸãƒ»ä¸ŠæœŸã®â€œå¢—é¡/ä¸Šæ–¹â€ã‚‚å«ã‚€ï¼‰
    if _hit(["ä¸Šæ–¹ä¿®æ­£", "é€šæœŸä¸Šæ–¹", "é€šæœŸå¢—é¡", "ä¸ŠæœŸäºˆæƒ³ä¿®æ­£ï¼ˆå¢—é¡ï¼‰",
             "æ¥­ç¸¾äºˆæƒ³ã®ä¿®æ­£ï¼ˆå¢—é¡ï¼‰", "é€šæœŸäºˆæƒ³ä¿®æ­£ï¼ˆå¢—é¡ï¼‰"]):
        score += 2
        reasons.append("ä¸Šæ–¹ä¿®æ­£ï¼ˆå¢—é¡ï¼‰ã§+2")

    # é…å½“/è‡ªç¤¾æ ªè²·ã„ã¯ +1
    if _hit(["å¢—é…", "å¾©é…", "è‡ªç¤¾æ ªè²·ã„", "é…å½“äºˆæƒ³ã®ä¿®æ­£ï¼ˆå¢—é¡ï¼‰"]):
        score += 1
        reasons.append("é…å½“å¢—é¡/å¾©é…/è‡ªç¤¾æ ªè²·ã„ã§+1")

    # æœ€é«˜ç›Šã¯ +1
    if _hit(["æœ€é«˜ç›Š", "éå»æœ€é«˜ç›Š"]):
        score += 1
        reasons.append("æœ€é«˜ç›Šã§+1")

    # é»’å­—åŒ–ã¯å˜ç‹¬+0ã€ãŸã ã—â€œé»’å­—è»¢æ›â€ã¯ +1ï¼ˆæœ¬æ–‡/ã‚¿ã‚¤ãƒˆãƒ«ã«æ˜ç¤ºï¼‰
    if _hit(["é»’å­—è»¢æ›"]):
        score += 1
        reasons.append("é»’å­—è»¢æ›ã§+1")

    # â–¼ ãƒã‚¬è¦å› ï¼ˆå¼·ã‚ã«æ¸›ç‚¹ï¼‰
    # ä¸‹æ–¹ä¿®æ­£ã¯ -3ï¼ˆå¼·ãƒã‚¬ï¼‰
    if _hit(["ä¸‹æ–¹ä¿®æ­£", "é€šæœŸäºˆæƒ³ä¿®æ­£ï¼ˆæ¸›é¡ï¼‰"]):
        score -= 3
        reasons.append("ä¸‹æ–¹ä¿®æ­£ï¼ˆæ¸›é¡ï¼‰ã§-3")

    # æ¸›é…ã¯ -2
    if _hit(["æ¸›é…", "é…å½“äºˆæƒ³ã®ä¿®æ­£ï¼ˆæ¸›é¡ï¼‰"]):
        score -= 2
        reasons.append("æ¸›é…ã§-2")

    # ç‰¹æã¯ -2ï¼ˆâ€œç‰¹åˆ¥æå¤±â€ãªã©ï¼‰
    if _hit(["ç‰¹æ", "ç‰¹åˆ¥æå¤±"]):
        score -= 2
        reasons.append("ç‰¹æã§-2")
    # ---- å€‹åˆ¥æ¡ç‚¹ã“ã“ã¾ã§ ----


    # é€²æ—ç‡ãƒ’ãƒ¥ãƒ¼ãƒªã‚¹ãƒ†ã‚£ã‚¯ã‚¹
    if isinstance(prog, (int, float)):
        if prog >= 70:
            score += 1
            reasons.append(f"é€²æ—ç‡{prog:.0f}%ï¼ˆé«˜é€²æ—ï¼‰")
        elif prog <= 30:
            score -= 1
            reasons.append(f"é€²æ—ç‡{prog:.0f}%ï¼ˆä½é€²æ—ï¼‰")

    # YoYï¼ˆå‰å¹´åŒæœŸæ¯”ï¼‰ï¼šå¤–ã‚Œå€¤ã‚’ç„¡è¦–ã—ã€åˆ©ç›Šç³»ã®è¤‡æ•°ãƒã‚¤ãƒŠã‚¹ã‚’å¼·ãè©•ä¾¡
    def _sane(y):
        return y if isinstance(y, (int, float)) and -120.0 <= y <= 120.0 else None

    neg_profit_yoy = 0
    pos_profit_yoy = 0
    for k in ("å£²ä¸Šé«˜", "å–¶æ¥­åˆ©ç›Š", "çµŒå¸¸åˆ©ç›Š", "ç´”åˆ©ç›Š"):
        y = _sane(yoy.get(k))
        if y is None:
            continue
        if k in ("å–¶æ¥­åˆ©ç›Š", "çµŒå¸¸åˆ©ç›Š", "ç´”åˆ©ç›Š"):
            if y <= -5:
                neg_profit_yoy += 1
            if y >= 1:
                pos_profit_yoy += 1
        # å€‹åˆ¥ã®åŠ ç‚¹æ¸›ç‚¹ï¼ˆç·©ã‚ï¼‰
        if y >= 5:
            score += 1
            reasons.append(f"{k}YoY+{y:.1f}%")
        elif y >= 1:
            score += 0.5
            reasons.append(f"{k}YoY+{y:.1f}%")
        elif y <= -5:
            score -= 1
            reasons.append(f"{k}YoY{y:.1f}%")
        elif y <= -1:
            score -= 0.5
            reasons.append(f"{k}YoY{y:.1f}%")

    if neg_profit_yoy >= 2:
        score -= 1.0
        reasons.append("åˆ©ç›ŠYoYãŒè¤‡æ•°é …ç›®ã§æ¸›å°‘ï¼ˆç·ã˜ã¦æ¸›ç›Šå‚¾å‘ï¼‰")

    # æŒ‡æ¨™ã®ç¬¦å·ï¼šé»’å­—ã¯å¼±åŠ ç‚¹ã€‚YoYãŒãƒã‚¤ãƒŠã‚¹ã®ã¨ãã¯é»’å­—åŠ ç‚¹ã‚’ç„¡åŠ¹åŒ–
    for k in ("å–¶æ¥­åˆ©ç›Š", "çµŒå¸¸åˆ©ç›Š", "ç´”åˆ©ç›Š"):
        v = met.get(k, None)
        if isinstance(v, (int, float)):
            if v > 0:
                if pos_profit_yoy > 0:  # å¢—ç›Šã®è£ä»˜ã‘ãŒã‚ã‚‹ã¨ãã®ã¿å¾®åŠ ç‚¹
                    score += 0.3
                    reasons.append(f"{k}ãŒé»’å­—ï¼ˆå¢—ç›Šå‚¾å‘ã¨æ•´åˆï¼‰")
                else:
                    reasons.append(f"{k}ãŒé»’å­—ï¼ˆå˜ç‹¬è¦ç´ ã®ãŸã‚åŠ ç‚¹ãªã—ï¼‰")
            if v < 0:
                score -= 0.5
                reasons.append(f"{k}ãŒèµ¤å­—")

    # ã€Œé»’å­—ã€å˜ç‹¬ãƒ¯ãƒ¼ãƒ‰ã¯ç„¡è¦–ã€é»’å­—è»¢æ›ã®ã¿åˆ¥é€”ãƒã‚¸ãƒ†ã‚£ãƒ–ï¼ˆæ—¢ã«POSã§åŠ ç‚¹æ¸ˆã¿ï¼‰

    # æœ€çµ‚åˆ¤å®šã®ã—ãã„å€¤ã‚’ã‚„ã‚„åºƒã’ã¦èª¤åˆ¤å®šã‚’æ¸›ã‚‰ã™
    verdict = "good" if score >= 1.5 else ("bad" if score <= -1.5 else "neutral")
    return {"verdict": verdict, "score": score, "reasons": reasons}

# ========= ã“ã“ã¾ã§ãƒ˜ãƒ«ãƒ‘ãƒ¼ =========
def fetch_tdnet_by_keywords(
    days: int = 90,
    keywords: list[str] | None = None,
    per_day_limit: int = 300,
    slice_escalation=(12, 6, 3, 1, 0.5)
) -> List[Dict[str, Any]]:
    """
    Yanoshin TDnet ä¸€è¦§APIã‚’ä½¿ã£ã¦ã€ä¸ãˆãŸã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼ˆORæ¡ä»¶ï¼‰ã§ã‚¿ã‚¤ãƒˆãƒ«ã‚’ãƒ•ã‚£ãƒ«ã‚¿ã—ã¦å–å¾—ã€‚
    - list_time / list ã®æ™‚é–“ç¯„å›²ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆãŒé€šã‚Œã°æ®µéšã‚¹ãƒ©ã‚¤ã‚¹ã€ãƒ€ãƒ¡ãªã‚‰æ—¥å˜ä½+è¿½åŠ ãƒšãƒ¼ã‚¸æ¢æŸ»ã€‚
    - keywords ãŒ None/ç©ºãªã‚‰ã€ãƒ•ã‚£ãƒ«ã‚¿ãªã—ï¼ˆå…¨ä»¶ï¼‰ã€ã§è¿”ã™ã€‚
    """
    import time
    from datetime import datetime, timedelta

    time_capable = False
    KEY_RE_LOCAL = re.compile("|".join(map(re.escape, keywords or []))) if (keywords and len(keywords)>0) else None

    def _fetch_tdnet_by_range(dt_from: datetime, dt_to: datetime) -> list[dict]:
        nonlocal time_capable
        s_day  = dt_from.strftime("%Y%m%d"); e_day  = dt_to.strftime("%Y%m%d")
        s_isoT = dt_from.strftime("%Y-%m-%dT%H:%M:%S"); e_isoT = dt_to.strftime("%Y-%m-%dT%H:%M:%S")
        s_nosep= dt_from.strftime("%Y%m%d%H%M%S");      e_nosep= dt_to.strftime("%Y%m%d%H%M%S")
        s_spc  = dt_from.strftime("%Y%m%d %H:%M:%S");   e_spc  = dt_to.strftime("%Y%m%d %H:%M:%S")

        variants = [
            (f"https://webapi.yanoshin.jp/webapi/tdnet/list/{s_nosep}-{e_nosep}.json", "list/nosep", True),
            (f"https://webapi.yanoshin.jp/webapi/tdnet/list/{s_isoT}-{e_isoT}.json",   "list/isoT",  True),
            (f"https://webapi.yanoshin.jp/webapi/tdnet/list/{quote(s_spc)}-{quote(e_spc)}.json", "list/space", True),
            (f"https://webapi.yanoshin.jp/webapi/tdnet/list_time/{s_nosep}-{e_nosep}.json", "list_time/nosep", True),
            (f"https://webapi.yanoshin.jp/webapi/tdnet/list/{s_day}-{e_day}.json",     "list/day",  False),
        ]
        for url, tag, is_time in variants:
            js = _http_get_json(url)
            items = js.get("items") if isinstance(js, dict) else (js if isinstance(js, list) else None)
            if isinstance(items, list):
                if is_time: time_capable = True
                print(f"[tdnet] {dt_from:%Y-%m-%d %H:%M}~{dt_to:%H:%M} : api={len(items)} (fmt={tag})")
                return items or []
        print(f"[tdnet] {dt_from:%Y-%m-%d %H:%M}~{dt_to:%H:%M} : api=None (all time-formats failed)")
        return []

    def _fetch_day_extra_pages(s_day: str, e_day: str, page_try: int = 15) -> list[dict]:
        collected = []
        base = f"https://webapi.yanoshin.jp/webapi/tdnet/list/{s_day}-{e_day}.json"
        patterns = ["?page={p}", "?p={p}"]
        for p in range(2, 2 + max(1, page_try)):
            hit = 0
            for fmt in patterns:
                url = base + fmt.format(p=p)
                js = _http_get_json(url)
                items = js.get("items") if isinstance(js, dict) else (js if isinstance(js, list) else None)
                if isinstance(items, list) and items:
                    collected.extend(items); hit += len(items)
                    print(f"[tdnet]   extra page p={p} via '{fmt[1:4]}' -> {len(items)}")
                    break
            if hit == 0: break
            time.sleep(0.15)
        return collected

    def _dedup_key(it: dict) -> str:
        td = it.get("Tdnet") or it
        return (td.get("id") or td.get("document_url") or td.get("title") or "") + "|" + (td.get("pubdate") or "")

    def _slice_fetch(dt_from: datetime, dt_to: datetime, level: int = 0) -> list[dict]:
        items = _fetch_tdnet_by_range(dt_from, dt_to)
        if len(items) < per_day_limit:
            return items
        if not time_capable:
            if (dt_from.hour, dt_from.minute, dt_from.second) == (0,0,0) and (dt_to.hour, dt_to.minute, dt_to.second) == (23,59,59) and dt_from.date()==dt_to.date():
                s_day = dt_from.strftime("%Y%m%d"); e_day = dt_to.strftime("%Y%m%d")
                extras = _fetch_day_extra_pages(s_day, e_day)
                if extras:
                    print(f"[tdnet] day extra merged: base=300 + extras={len(extras)}")
                    return items + extras
            print(f"[tdnet] time-capability=NO â†’ stop slicing ({dt_from:%F} {dt_from:%H:%M}~{dt_to:%H:%M}), got={len(items)}")
            return items
        if level >= len(slice_escalation):
            print(f"[tdnet] slice exhausted ({dt_from:%F} {dt_from:%H:%M}~{dt_to:%H:%M}), got={len(items)} >= limit={per_day_limit}")
            return items
        width_h = slice_escalation[level]
        step_sec = int(max(width_h * 3600, 1800))
        parts: list[dict] = []
        cur = dt_from
        while cur < dt_to:
            nxt = min(cur + timedelta(seconds=step_sec), dt_to)
            parts.extend(_slice_fetch(cur, nxt, level + 1))
            cur = nxt
            time.sleep(0.12)
        print(f"[tdnet] api-slice level={level} w={width_h}h -> merged={len(parts)}")
        return parts

    def _ts(x: dict) -> float:
        td = x.get("Tdnet") or {}
        s = (td.get("pubdate") or "").replace("/", "-")
        try:
            return datetime.strptime(s, "%Y-%m-%d %H:%M:%S").timestamp()
        except Exception:
            return 0.0

    all_items: list[dict] = []
    seen: set[str] = set()
    end = datetime.now(JST).replace(microsecond=0)

    for i in range(days):
        day_to   = (end - timedelta(days=i)).replace(hour=23, minute=59, second=59)
        day_from = (end - timedelta(days=i)).replace(hour=0,  minute=0,  second=0)
        day_items = _slice_fetch(day_from, day_to)

        kept = 0
        for it in day_items:
            td = it.get("Tdnet") or it
            title = _unescape_text(td.get("title") or "")
            # â† ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŒ‡å®šãŒã‚ã‚‹ã¨ãã ã‘ãƒ•ã‚£ãƒ«ã‚¿ã€‚ç„¡æŒ‡å®šãªã‚‰å…¨ä»¶é€šã™
            if KEY_RE_LOCAL and title and not KEY_RE_LOCAL.search(title):
                continue
            td["title"] = title
            k = _dedup_key(it)
            if not k or k in seen: 
                continue
            seen.add(k)
            all_items.append(it); kept += 1

        print(f"[tdnet] {day_from:%Y-%m-%d} kept={kept} / raw={len(day_items)}")
        time.sleep(0.2)

    all_items.sort(key=_ts, reverse=True)
    return all_items

def tdnet_items_to_earnings_rows(tdnet_items):
    """
    fetch_earnings_tdnet_only() ã®è¿”ã‚Šå€¤ï¼ˆTdneté…åˆ—ï¼‰â†’ ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ãŒèª­ã‚€è¡Œå½¢å¼ã¸æ•´å½¢
    + PDFæœ¬æ–‡ã®è¦ç´„/åˆ¤å®šã‚’å¾©æ´»ï¼ˆæœ€å¤§ EARNINGS_SUMMARY_MAX ä»¶ï¼‰
    """
    rows = []
    summarized = 0

    for it in tdnet_items:
        td = it.get("Tdnet", it) or {}

        title = (td.get("title") or "").strip()
        name  = (td.get("company_name") or "").strip()
        code_raw = str(td.get("company_code") or td.get("code") or "").strip()
        # 4æ¡ã‚³ãƒ¼ãƒ‰ã«ä¸¸ã‚
        if re.fullmatch(r"\d{5}", code_raw):
            ticker = code_raw[:4]
        elif re.fullmatch(r"\d{4}", code_raw):
            ticker = code_raw
        else:
            ticker = ""

        link = (td.get("document_url") or td.get("pdf_url") or td.get("url") or "").strip()
        time_str = (td.get("pubdate") or td.get("publish_datetime") or "").replace("T"," ").replace("+09:00","")

        # æ—¢å­˜ã®ç°¡æ˜“summary/reasonï¼ˆã‚¿ã‚¤ãƒˆãƒ«ãƒ«ãƒ¼ãƒ«ï¼‰ã‚’å°Šé‡ã—ã¤ã¤ã€
        # summary ã®æœ‰ç„¡ã«é–¢ä¿‚ãªãã‚¿ã‚¤ãƒˆãƒ«ã‹ã‚‰ sentiment ã‚’å¸¸ã«æ¨å®šã—ã¦ä½¿ã†
        summary = (td.get("summary") or "").strip()
        reason  = (td.get("reason")  or "").strip()
        label_guess = "neutral"

        s_t, label_from_title, why_t = _summarize_title_simple(title)
        if label_from_title in ("positive", "negative"):
            label_guess = label_from_title
        if not summary and s_t:
            summary = s_t
        if not reason and why_t:
            reason = why_t

        # PDFæœ¬æ–‡ã®è¦ç´„ï¼†åˆ¤å®šï¼ˆä¸Šé™ EARNINGS_SUMMARY_MAX ä»¶ï¼‰
        verdict = (td.get("verdict") or "").strip()
        score_judge = td.get("score_judge")
        reasons = td.get("reasons")
        metrics = td.get("metrics")
        progress = td.get("progress")

        should_try_pdf = bool(link)
        if should_try_pdf and summarized < EARNINGS_SUMMARY_MAX:
            try:
                pdf_bytes = _download_pdf_bytes(link)
                if pdf_bytes:
                    text = _extract_text_from_pdf(pdf_bytes)
                    if text:
                        parsed = _parse_earnings_metrics(text)
                        metrics = parsed.get("metrics", {})
                        progress = parsed.get("progress")

                        # æœ¬æ–‡è¦ç´„ã‚’å„ªå…ˆï¼ˆã‚¿ã‚¤ãƒˆãƒ«è¦ç´„ãŒç©º/å¼±ã„å ´åˆä¸Šæ›¸ãï¼‰
                        sum2 = _summarize_earnings_text(text)
                        if sum2:
                            summary = sum2

                        # è‰¯å¦åˆ¤å®š
                        judge = _grade_earnings(title, text, parsed)
                        verdict = judge["verdict"]
                        score_judge = judge["score"]
                        reasons = judge["reasons"]

                        # ã‚»ãƒ³ãƒãƒ¡ãƒ³ãƒˆæ¨å®šã‚’ verdict ã§ä¸Šæ›¸ãï¼ˆgood/bad ã®ã¨ãã ã‘ï¼‰
                        if verdict == "good":
                            label_guess = "positive"
                        elif verdict == "bad":
                            label_guess = "negative"

                        summarized += 1
                # è»½ã„ã‚¯ãƒ¼ãƒ«ãƒ€ã‚¦ãƒ³ï¼ˆTDnetå´/ä¸­ç¶™CDNã®å„ªã—ã•ï¼‰
                time.sleep(0.15)
            except Exception as e:
                print(f"[earnings] PDF summarize error: {e}")

        # reasons ã¯é…åˆ—
        if isinstance(reasons, str) and reasons:
            reasons = [reasons]
        if reasons is None:
            reasons = ([reason] if reason else [])

        # sentiment ã¯æ—¢å­˜ãŒã‚ã‚Œã°å„ªå…ˆã€‚ãªã‘ã‚Œã°ã‚¿ã‚¤ãƒˆãƒ«/åˆ¤å®šã‹ã‚‰ã®æ¨å®šã‚’åæ˜ 
        sentiment = td.get("sentiment") or label_guess or "neutral"

        row = {
            "ticker": ticker or "0000",
            "name":   name or (ticker or "TDnet"),
            "score":  0,
            "sentiment": sentiment,          # 'positive' | 'negative' | 'neutral'
            "title": title,
            "link":  link,
            "time":  time_str or "",
            "summary": summary,              # â† ãƒ•ãƒ­ãƒ³ãƒˆãŒèª­ã‚€
            "reasons": reasons,              # â† ãƒ•ãƒ­ãƒ³ãƒˆãŒèª­ã‚€ï¼ˆé…åˆ—ï¼‰
            "verdict": verdict,              # 'good'/'bad'/â€¦
            "score_judge": score_judge,      # æ•°å€¤ or None
            "metrics": metrics or {},        # è§£æã—ãŸæ•°å€¤
            "progress": progress,            # é€²æ—ç‡(%)
        }
        rows.append(row)

    # æ™‚åˆ»é™é †ã«æ•´åˆ—ï¼ˆä¿é™ºï¼‰
    def _ts(s):
        try:
            return datetime.strptime((s or "").replace("/","-"), "%Y-%m-%d %H:%M:%S").timestamp()
        except:
            return 0
    rows.sort(key=lambda r: _ts(r["time"]), reverse=True)
    return rows


# ------------------ ã‚¹ã‚³ã‚¢è¨ˆç®—ï¼ˆç·åˆ/æ²ç¤ºæ¿ï¼‰ ------------------
def build_scores(rows, weights):
    """
    ç·åˆã‚¹ã‚³ã‚¢:
      trends.latest ã‚’ãã®ã¾ã¾
      bbs_growth = posts_24h / max(1, posts_72h)
      news = count_24h
      score = w_trends * trends + w_bbs_growth * bbs_growth + w_news * news
    """
    wt = float(weights.get("trends", 1.0))
    wb = float(weights.get("bbs_growth", 1.0))
    wn = float(weights.get("news", 1.0))

    scores = []
    for r in rows:
        t = 0.0
        try:
            t = float((r.get("trends") or {}).get("latest", 0.0) or 0.0)
        except Exception:
            t = 0.0

        b24 = int((r.get("bbs") or {}).get("posts_24h", 0) or 0)
        b72 = int((r.get("bbs") or {}).get("posts_72h", 0) or 0)
        bbs_growth = b24 / max(1, b72)

        n = int((r.get("news") or {}).get("count_24h", 0) or 0)

        score = wt * t + wb * bbs_growth + wn * n
        scores.append(score)
    return scores


def build_bbs_scores(rows):
    """
    æ²ç¤ºæ¿ç”¨ã®å˜ç´”ã‚¹ã‚³ã‚¢ï¼ˆå¢—åŠ ç‡ = posts_24h / max(1, posts_72h)ï¼‰
    """
    out = []
    for r in rows:
        b24 = int((r.get("bbs") or {}).get("posts_24h", 0) or 0)
        b72 = int((r.get("bbs") or {}).get("posts_72h", 0) or 0)
        out.append(b24 / max(1, b72))
    return out


# ------------------ main ------------------
# ==== ã“ã“ã‹ã‚‰ main() å®Œå…¨ç½®ãæ›ãˆ ====

def _table_exists(conn, name: str) -> bool:
    try:
        cur = conn.execute("SELECT 1 FROM sqlite_master WHERE type='table' AND name=? LIMIT 1", (name,))
        return cur.fetchone() is not None
    except Exception:
        return False

def ensure_misc_objects(conn):
    """
    è¿½åŠ ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ / ãƒˆãƒªã‚¬ / ãƒ“ãƒ¥ãƒ¼ã®ä½œæˆï¼ˆå­˜åœ¨ãƒã‚§ãƒƒã‚¯ä»˜ãï¼‰
    - signals_log(ç¨®åˆ¥, DATE(æ—¥æ™‚)), signals_log(ã‚³ãƒ¼ãƒ‰, DATE(æ—¥æ™‚))
    - earnings_events(DATE(æå‡ºæ™‚åˆ»)), earnings_events(ã‚³ãƒ¼ãƒ‰, DATE(æå‡ºæ™‚åˆ»))
    - price_history.date ã® YYYY-MM-DD æ­£è¦åŒ–ãƒˆãƒªã‚¬
    - v_events_dedupï¼ˆåŒã‚³ãƒ¼ãƒ‰Ã—åŒæ—¥ã§æœ€æ–°ã®æå‡ºæ™‚åˆ»ã‚’æ¡ç”¨ï¼‰
    """
    cur = conn.cursor()

    # --- signals_log ã®å¼ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ï¼ˆãƒ†ãƒ¼ãƒ–ãƒ«ãŒã‚ã‚‹å ´åˆã®ã¿ï¼‰ ---
    if _table_exists(conn, "signals_log"):
        cur.execute("""
            CREATE INDEX IF NOT EXISTS idx_signals_log_type_date
            ON signals_log(ç¨®åˆ¥, substr(æ—¥æ™‚,1,10));
        """)
        cur.execute("""
            CREATE INDEX IF NOT EXISTS idx_signals_log_code_date
            ON signals_log(ã‚³ãƒ¼ãƒ‰, substr(æ—¥æ™‚,1,10));
        """)

    # --- earnings_events ã®æ—¥ä»˜ç³»ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ ---
    if _table_exists(conn, "earnings_events"):
        cur.execute("""
            CREATE INDEX IF NOT EXISTS idx_earn_date_only
            ON earnings_events(substr(æå‡ºæ™‚åˆ»,1,10));
        """)
        cur.execute("""
            CREATE INDEX IF NOT EXISTS idx_earn_code_date
            ON earnings_events(ã‚³ãƒ¼ãƒ‰, substr(æå‡ºæ™‚åˆ»,1,10));
        """)

        # --- price_history: ã€Œæ—¥ä»˜ã€orã€Œdateã€ã‚’è‡ªå‹•åˆ¤å®šã—ã¦æ­£è¦åŒ–ãƒˆãƒªã‚¬ï¼ˆYYYY-MM-DDï¼‰ ---
    if _table_exists(conn, "price_history"):
        try:
            cols = [r[1] for r in cur.execute("PRAGMA table_info(price_history)").fetchall()]
            price_date_col = "æ—¥ä»˜" if "æ—¥ä»˜" in cols else ("date" if "date" in cols else None)
        except Exception:
            price_date_col = None

        if price_date_col:
            # æ—§ãƒˆãƒªã‚¬ï¼ˆåˆ—åå›ºå®šï¼‰ã‚’è½ã¨ã—ã¦ä½œã‚Šç›´ã—
            cur.execute("DROP TRIGGER IF EXISTS trg_price_history_date_norm_ins;")
            cur.execute("DROP TRIGGER IF EXISTS trg_price_history_date_norm_upd;")

            # INSERT æ™‚ã®æ­£è¦åŒ–ï¼š'/'ã‚„'.'ã‚’'-'ã«ç›´ã—ã€date()ã§YYYY-MM-DDåŒ–ï¼ˆãƒ€ãƒ¡ãªã‚‰å…ˆé ­10æ–‡å­—ï¼‰
            cur.execute(f"""
                CREATE TRIGGER IF NOT EXISTS trg_price_history_date_norm_ins
                AFTER INSERT ON price_history
                BEGIN
                  UPDATE price_history
                     SET "{price_date_col}" =
                         COALESCE(
                           date(replace(replace(NEW."{price_date_col}",'/','-'),'.','-')),
                           substr(replace(replace(NEW."{price_date_col}",'/','-'),'.','-'),1,10)
                         )
                   WHERE rowid = NEW.rowid;
                END;
            """)

            # UPDATE æ™‚ã®æ­£è¦åŒ–
            cur.execute(f"""
                CREATE TRIGGER IF NOT EXISTS trg_price_history_date_norm_upd
                AFTER UPDATE OF "{price_date_col}" ON price_history
                BEGIN
                  UPDATE price_history
                     SET "{price_date_col}" =
                         COALESCE(
                           date(replace(replace(NEW."{price_date_col}",'/','-'),'.','-')),
                           substr(replace(replace(NEW."{price_date_col}",'/','-'),'.','-'),1,10)
                         )
                   WHERE rowid = NEW.rowid;
                END;
            """)

    # --- v_events_dedup: åŒã‚³ãƒ¼ãƒ‰Ã—åŒæ—¥ã§æœ€æ–°ã®æå‡ºæ™‚åˆ»ã®ã¿ ---
    cur.execute("""
        CREATE VIEW IF NOT EXISTS v_events_dedup AS
        WITH max_ts AS (
          SELECT
            ã‚³ãƒ¼ãƒ‰,
            substr(æå‡ºæ™‚åˆ»,1,10) AS æå‡ºæ—¥,
            MAX(æå‡ºæ™‚åˆ») AS max_æå‡ºæ™‚åˆ»
          FROM earnings_events
          GROUP BY ã‚³ãƒ¼ãƒ‰, æå‡ºæ—¥
        )
        SELECT e.*
          FROM earnings_events e
          JOIN max_ts m
            ON e.ã‚³ãƒ¼ãƒ‰ = m.ã‚³ãƒ¼ãƒ‰
           AND substr(e.æå‡ºæ™‚åˆ»,1,10) = m.æå‡ºæ—¥
           AND e.æå‡ºæ™‚åˆ» = m.max_æå‡ºæ™‚åˆ»;
    """)

    conn.commit()

def run_light_healthcheck(conn):
    """
    è»½é‡ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯ï¼š
      - price_history ã®æœ€æ–°æ—¥ä»˜ã¨ coverageï¼ˆè©²å½“æ—¥ã® distinct(ã‚³ãƒ¼ãƒ‰) ä»¶æ•°ï¼‰
      - earnings_events ã®æœ€æ–°æå‡ºæ™‚åˆ»
    signals_log ãŒã‚ã‚‹å ´åˆã¯ 'healthcheck' ã¨ã—ã¦è©³ç´°JSONã‚’æ›¸ãè¾¼ã¿
    """
    result = {"ok": True}
    try:
        # ç½®ãæ›ãˆå¾Œ
        last_date = None
        dcol = None
        if _table_exists(conn, "price_history"):
            cols = [r[1] for r in conn.execute("PRAGMA table_info(price_history)").fetchall()]
            dcol = "æ—¥ä»˜" if "æ—¥ä»˜" in cols else ("date" if "date" in cols else None)
            if dcol:
                last_date = conn.execute(f'SELECT MAX("{dcol}") FROM price_history').fetchone()[0]

        cov = None
        if last_date and dcol:
            cov = conn.execute(
                f'SELECT COUNT(DISTINCT "ã‚³ãƒ¼ãƒ‰") FROM price_history WHERE "{dcol}"=?',
                (last_date,)
            ).fetchone()[0]

        last_event_ts = None
        if _table_exists(conn, "earnings_events"):
            last_event_ts = conn.execute("SELECT MAX(æå‡ºæ™‚åˆ») FROM earnings_events").fetchone()[0]
        result.update({
            "price_last_date": last_date,
            "price_coverage_codes": cov,
            "earnings_last_ts": last_event_ts
        })
    except Exception as e:
        result["ok"] = False
        result["error"] = str(e)

    if _table_exists(conn, "signals_log"):
        try:
            from datetime import datetime, timezone, timedelta
            conn.execute(
                "INSERT INTO signals_log(ã‚³ãƒ¼ãƒ‰, ç¨®åˆ¥, æ—¥æ™‚, è©³ç´°) VALUES(?, ?, ?, ?)",
                ("", "healthcheck",
                 datetime.utcnow().isoformat(),
                 json.dumps(result, ensure_ascii=False))
            )
            conn.commit()
        except Exception:
            pass

    return result


def _detect_price_cols(conn: sqlite3.Connection):
    """
    price_history ã®åˆ—åã‚’å‹•çš„æ¤œå‡ºã™ã‚‹ã€‚
    - æ—¥ä»˜: ã€Œæ—¥ä»˜ã€å„ªå…ˆã€ãªã‘ã‚Œã°ã€Œdateã€
    - ã‚³ãƒ¼ãƒ‰: ã€Œã‚³ãƒ¼ãƒ‰ã€å„ªå…ˆã€ãªã‘ã‚Œã°ã€Œcodeã€
    - çµ‚å€¤: å€™è£œã®ã†ã¡æœ€åˆã«è¦‹ã¤ã‹ã£ãŸåˆ—ï¼ˆ'çµ‚å€¤', 'çµ‚å€¤èª¿æ•´å¾Œ', 'close', 'Close' ãªã©ï¼‰
    """
    cur = conn.cursor()
    cur.execute("PRAGMA table_info(price_history);")
    cols = [r[1] for r in cur.fetchall()]
    if not cols:
        return None, None, None

    def pick(*cands):
        for c in cands:
            if c in cols:
                return c
        return None

    date_col = pick("æ—¥ä»˜", "date")
    code_col = pick("ã‚³ãƒ¼ãƒ‰", "code", "ticker")
    price_col = pick("çµ‚å€¤", "çµ‚å€¤èª¿æ•´å¾Œ", "close", "Close", "çµ‚å€¤(å††)", "çµ‚å€¤_å††")

    return date_col, code_col, price_col


def load_quotes_from_price_history(conn: sqlite3.Connection, codes: list[str]) -> dict[str, dict]:
    """
    price_history ã‹ã‚‰å„ã‚³ãƒ¼ãƒ‰ã® æœ€æ–°çµ‚å€¤ ã¨ ç›´å‰çµ‚å€¤ ã‚’å–ã‚Šå‡ºã—ã€
    {"2792": {"current": 1234.0, "prev_close": 1200.0, "diff_yen": 34.0}, ...}
    ã‚’è¿”ã™ã€‚åˆ—åã¯å‹•çš„æ¤œå‡ºã€‚
    """
    out: dict[str, dict] = {}
    if not codes:
        return out

    date_col, code_col, price_col = _detect_price_cols(conn)
    if not all([date_col, code_col, price_col]):
        # å¿…è¦åˆ—ãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯ç©ºã§è¿”ã™
        return out

    qmarks = ",".join("?" for _ in codes)
    cur = conn.cursor()

    # å„ã‚³ãƒ¼ãƒ‰ã®æœ€æ–°æ—¥ä»˜ã‚’å–ã‚‹
    latest = dict(cur.execute(
        f'SELECT "{code_col}", MAX("{date_col}") '
        f'FROM price_history WHERE "{code_col}" IN ({qmarks}) GROUP BY "{code_col}"',
        codes
    ).fetchall())

    # æœ€æ–°æ—¥ä»˜ã¨ãã®å‰æ—¥ï¼ˆ2ä»¶ï¼‰ã‚’å–å¾—ã—ã¦ current / prev ã‚’æ±ºã‚ã‚‹
    for code, max_d in latest.items():
        rows = cur.execute(
            f'''SELECT "{price_col}" FROM price_history
                WHERE "{code_col}"=? AND "{date_col}"<=?
                ORDER BY "{date_col}" DESC LIMIT 2''',
            (code, max_d)
        ).fetchall()
        if not rows:
            continue
        cur_px = rows[0][0]
        prev_px = rows[1][0] if len(rows) >= 2 else None

        try:
            cur_px = float(cur_px) if cur_px is not None else None
        except Exception:
            cur_px = None
        try:
            prev_px = float(prev_px) if prev_px is not None else None
        except Exception:
            prev_px = None

        diff = None
        if cur_px is not None and prev_px is not None:
            diff = cur_px - prev_px

        out[str(code).zfill(4)] = {
            "current": cur_px,
            "prev_close": prev_px,
            "diff_yen": diff,
        }
    return out



def main():
    cfg = load_config(str(ROOT / "config.yaml"))

    # ãƒ‡ãƒãƒƒã‚°ã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼ˆæ—¢å­˜ã®ã‚­ãƒ¼ã‚’å°Šé‡ï¼‰
    include_dbg = bool((cfg.get("debug", {}) or {}).get("sentiment", False))
    include_bbs_samples = bool((cfg.get("debug", {}) or {}).get("bbs_samples", False))
    bbs_max_samples = int((cfg.get("bbs", {}) or {}).get("max_samples", 20))

    # ãƒ¦ãƒ‹ãƒãƒ¼ã‚¹ä½œæˆ
    symbols = build_universe(cfg)

    # ---- ç·åˆ/æ²ç¤ºæ¿ã‚¿ãƒ–ç”¨ rows ã‚’æ§‹ç¯‰ï¼ˆãƒ‹ãƒ¥ãƒ¼ã‚¹/æ²ç¤ºæ¿/ãƒˆãƒ¬ãƒ³ãƒ‰åé›† â†’ ã‚¹ã‚³ã‚¢ä»˜ä¸ï¼‰----
    rows = []
    for sym in symbols:
        ticker = sym.get("ticker")
        aliases = sym.get("aliases", [])
        yf_code = sym.get("bbs", {}).get("yahoo_finance_code", ticker)

        trends = fetch_trends(aliases, timeframe="now 7-d", geo="JP")
        news = fetch_news(
            sym.get("news_query"),
            max_items=cfg.get("news", {}).get("max_items_per_symbol", 30),
            include_debug=include_dbg,
        )
        bbs = fetch_bbs_stats(
            yf_code,
            look_threads=cfg.get("bbs", {}).get("look_threads", 50),
            include_samples=include_bbs_samples,
            max_samples=bbs_max_samples,
        )

        if include_dbg:
            pos = sum(1 for it in news["items"] if it.get("sentiment") == "positive")
            neg = sum(1 for it in news["items"] if it.get("sentiment") == "negative")
            neu = sum(1 for it in news["items"] if it.get("sentiment") == "neutral")
            print(f"[{ticker}] news pos/neu/neg = {pos}/{neu}/{neg} (24h={news['count_24h']})")

        rows.append({
            "ticker": ticker,
            "name": sym.get("name"),
            "trends": trends,
            "news": news,
            "bbs": bbs,
        })

    # æ—¢å­˜ãƒ­ã‚¸ãƒƒã‚¯ã§ã‚¹ã‚³ã‚¢ä»˜ä¸ï¼ˆç·åˆã‚¿ãƒ–ï¼†æ²ç¤ºæ¿ã‚¿ãƒ–ï¼‰
    scores = build_scores(rows, cfg.get("weights", {"trends": 1.0, "bbs_growth": 1.0, "news": 1.0}))
    for i, s in enumerate(scores):
        rows[i]["score"] = s
    bbs_scores = build_bbs_scores(rows)
    for i, s in enumerate(bbs_scores):
        rows[i]["score_bbs"] = s

    # ---- æ±ºç®—ã‚¿ãƒ–ï¼ˆTDnetã‚’æ—¥å‰²ã‚Šã§å–å¾— â†’ è¡Œå½¢å¼ã¸æ•´å½¢ï¼‰----
    try:
        # ã¾ãš 3æ—¥åˆ†ï¼ˆæ—¥å‰²ã‚Š 1æ—¥=æœ€å¤§300ä»¶ã§ç¢ºå®Ÿã«æ‹¾ã†ï¼‰
        tdnet_items = fetch_earnings_tdnet_only(days=3, per_day_limit=300)
        print(f"[earnings] raw tdnet items = {len(tdnet_items)}")

        # è¡Œå½¢å¼ã¸æ•´å½¢ï¼ˆã“ã“ã§ summary/reasons/verdict/metrics ã‚’è©°ã‚ã‚‹ï¼‰
        earnings_rows = tdnet_items_to_earnings_rows(tdnet_items)
        print(f"[earnings] rows after shaping = {len(earnings_rows)}")

        # ãƒ‡ãƒãƒƒã‚°ï¼šç›´è¿‘10ä»¶ã®ã‚¿ã‚¤ãƒˆãƒ«ã ã‘å‡ºã™
        for it in earnings_rows[:10]:
            print("  -", it.get("time",""), it.get("ticker",""), it.get("title","")[:60])

        # ä¸‡ä¸€0ä»¶ãªã‚‰ã€ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã§ç›´è¿‘3æ—¥ã‚’å†å–å¾—ï¼ˆAPIä¸èª¿å¯¾ç­–ï¼‰
        if not earnings_rows:
            print("[earnings][fallback] ç›´è¿‘7æ—¥ã§å†å–å¾—ã—ã¾ã™â€¦")
            tdnet_items_fallback = fetch_earnings_tdnet_only(days=7, per_day_limit=300)
            print(f"[earnings][fallback] raw items = {len(tdnet_items_fallback)}")
            earnings_rows = tdnet_items_to_earnings_rows(tdnet_items_fallback)
            print(f"[earnings][fallback] shaped rows = {len(earnings_rows)}")

        # ã•ã‚‰ã«0ä»¶ãªã‚‰ã€åŸå› åˆ‡ã‚Šåˆ†ã‘ç”¨ã« raw ã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã¸
        if not earnings_rows:
            
            ensure_dir(OUT_DIR)
            dump_json(tdnet_items, str(OUT_DIR / "tdnet_raw_debug.json"))
            print("[earnings][debug] tdnet_raw_debug.json ã‚’å‡ºåŠ›ã—ã¾ã—ãŸï¼ˆAPIå¿œç­”ã®ä¸­èº«ã‚’ç¢ºèªã§ãã¾ã™ï¼‰")

    except Exception as e:
        print("[earnings] å–å¾—/æ•´å½¢ã§ä¾‹å¤–:", e)
        earnings_rows = []
        
    # ---- å¢—è³‡ãƒ¬ãƒ¼ãƒ³ï¼šæ±ºç®—ã¨ã¯åˆ¥ã«å–ã‚Šã«è¡Œã ----
    offer_items = []
    try:
        offer_items = fetch_tdnet_by_keywords(days=7, keywords=OFFERING_KW, per_day_limit=300)
        print(f"[offerings] raw tdnet items (by KW) = {len(offer_items)}")
    except Exception as e:
        print("[offerings] fetch error:", e)


    # ---- DBä¿å­˜ï¼ˆæ—¥æœ¬èªã‚¹ã‚­ãƒ¼ãƒ earnings_eventsï¼‰----
    conn = sqlite3.connect(DB_PATH)
    ensure_earnings_schema(conn)
    ensure_offerings_schema(conn)
    ensure_misc_objects(conn)
    # å¢—è³‡/è¡Œä½¿/å£²å‡ºã¯ â€œå¢—è³‡ãƒ¬ãƒ¼ãƒ³â€ã® raw ã®ã¿ã‚’ä¿å­˜
    try:
        upsert_offerings_events(conn, offer_items)
    except Exception as e:
        print("[offerings] upsert error:", e)

    
    
    upsert_earnings_rows(conn, earnings_rows)
    run_light_healthcheck(conn)
    conn.close()

    # ---- å‡ºåŠ›ï¼ˆJSON/HTMLï¼‰----
    # è¡¨ç¤ºç”¨ã®æ±ºç®—ãƒ‡ãƒ¼ã‚¿ã¯ DB ã‹ã‚‰ç›´è¿‘30æ—¥åˆ†ï¼ˆpos/goodã®ã¿ï¼‰ã‚’å†æ§‹ç¯‰
    earnings_for_dash = load_earnings_for_dashboard_from_db(DB_PATH, days=30, filter_mode="nonnegative")

    # --- å¢—è³‡çµŒæ­´ãƒ•ãƒ©ã‚°ä»˜ä¸ï¼ˆç›´è¿‘3å¹´ï¼‰ ---
    try:
        offerings_map = load_offerings_recent_map(DB_PATH, days=365*3)
    except Exception as e:
        print("[offerings] map build error:", e)
        offerings_map = {}

    # ç·åˆ/æ²ç¤ºæ¿ã‚¿ãƒ– rowsï¼ˆãƒ¦ãƒ‹ãƒãƒ¼ã‚¹ï¼‰ã¸ä»˜ä¸
    try:
        for r in rows:
            code4 = str(r.get("ticker") or "").zfill(4)
            r["has_offering_history"] = bool(offerings_map.get(code4, False))
    except Exception as e:
        print("[offerings] annotate rows error:", e)

    # æ±ºç®—ã‚¿ãƒ– earnings ã¸ä»˜ä¸
    try:
        for r in earnings_for_dash:
            code4 = str(r.get("ticker") or "").zfill(4)
            r["has_offering_history"] = bool(offerings_map.get(code4, False))
    except Exception as e:
        print("[offerings] annotate earnings error:", e)
    print(f"[earnings][dash] from DB last 30d = {len(earnings_for_dash)}")
    
    # --- æ²ç¤ºæ¿æ¤œç´¢ï¼šæ¼ã‚Œã¦ã‚‹ï¼Ÿ / TOB ---
    try:
        leak = fetch_leak_comments(DB_PATH, max_pages=60)
    except Exception as e:
        print("[bbs_search][leak] error:", e)
        leak = {"rows": [], "total_comments": 0, "total_tickers": 0, "generated_at": now_iso()}
    try:
        tob = fetch_tob_comments(DB_PATH, max_pages=60)
    except Exception as e:
        print("[bbs_search][tob] error:", e)
        tob = {"rows": [], "total_comments": 0, "total_tickers": 0, "generated_at": now_iso()}
    output = {
        "generated_at": now_iso(),
        "rows": rows,                    # ç·åˆ/æ²ç¤ºæ¿ç”¨ï¼ˆå…ƒã®ã¾ã¾ï¼‰
        "earnings": earnings_for_dash,   # æ±ºç®—ã‚¿ãƒ–ï¼æ—¥åˆ¥ã‚¿ãƒ–ç”¨ï¼špositive ã®ã¿ãƒ»æœ€å¤§300ä»¶
        "leak": leak,
        "tob": tob
    }

    ensure_dir(OUT_DIR); ensure_dir(OUT_DIR / "history")
    ensure_dir(OUT_BBS_DIR); ensure_dir(OUT_BBS_DIR / "history")
    dump_json(output, str(OUT_DIR / "data.json"))
    ts = datetime.now(timezone.utc).strftime("%Y%m%d_%H%M")
    dump_json(output, str(OUT_DIR / "history" / f"{ts}.json"))
    dump_json(output, str(OUT_BBS_DIR / "data.json"))
    dump_json(output, str(OUT_BBS_DIR / "history" / f"{ts}.json"))
    print("[write]", (OUT_DIR / "data.json").resolve())

    ensure_dir(DASH_DIR)
    html_out = render_dashboard_html(output, api_base="")
    (DASH_DIR / "index.html").write_text(html_out, encoding="utf-8")
    print("[write]", (DASH_DIR / "index.html").resolve())
    print(f"[OK] {len(rows)} symbols + earnings {len(earnings_for_dash)} (filtered) â†’ data.json / index.html æ›´æ–°å®Œäº†")

    
    
# ==== ã“ã“ã¾ã§ main() å®Œå…¨ç½®ãæ›ãˆ ====

if __name__=="__main__":
    main()
