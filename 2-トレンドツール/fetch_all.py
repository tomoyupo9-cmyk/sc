#!/usr/bin/env python
# -*- coding: utf-8 -*-

# fetch_all.py â€” TDnetã®ã¿ã§æ±ºç®—å–å¾— + PDFè‡ªå‹•ã‚µãƒãƒª + è‰¯å¦åˆ¤å®šï¼ˆåˆ¤å®šç†ç”±ã‚’UIè¡¨ç¤ºï¼‰
# æ—¥åˆ¥æ•´ç†ã®UIæ”¹å–„ã‚’å«ã‚€å®Œå…¨ç‰ˆ
# 2025-10-14: TDnet APIå®‰å®šåŒ–ãƒ‘ãƒƒãƒ
#  - æ—¥å˜ä½ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ(list/day)ã‚’æœ€å„ªå…ˆ
#  - JSONä»¥å¤–(text/htmlç­‰)ã®å ´åˆã¯é™ã‹ã«å†è©¦è¡Œ/æœ€çµ‚è©¦è¡Œã®ã¿ãƒ­ã‚°
# 2025-10-17: â˜…TOBå°‚ç”¨ãƒ¬ãƒ¼ãƒ³è¿½åŠ ï¼ˆtob_eventsï¼‰ã€ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ã«ã€ŒTOB(TDNET)ã€ã‚¿ãƒ–ã‚’è¿½åŠ 
# 2025-10-17: â˜…TOB(æ²ç¤ºæ¿)ã‚¿ãƒ–ã« TDNETæ±ºå®šæ¸ˆã¿é™¤å¤–ãƒ•ã‚£ãƒ«ã‚¿ã‚’é©ç”¨

import json
from pathlib import Path
from datetime import datetime, timezone, timedelta
from typing import List, Dict, Any, Optional

# === Unified SQLite connection helper ===
try:
    import sqlite3  # ensure available
    def _get_db_conn(db_path: str, *, timeout: float = 30.0):
        """
        Open a tuned SQLite connection.
        - WAL journaling, NORMAL sync (fast, durable enough)
        - temp_store=MEMORY, mmap_size ~128MB
        - cache_size ~256MB (negative => size in KiB)
        """
        conn = _get_db_conn(db_path, timeout=timeout)
        try:
            cur = conn.cursor()
            for pragma in [
                "PRAGMA journal_mode=WAL;",
                "PRAGMA synchronous=NORMAL;",
                "PRAGMA temp_store=MEMORY;",
                "PRAGMA mmap_size=134217728;",
                "PRAGMA cache_size=-262144;"
            ]:
                try:
                    cur.execute(pragma)
                except Exception:
                    pass
            conn.commit()
        except Exception:
            # even if pragmas fail, return a usable connection
            pass
        return conn
except Exception:
    # Fallback stub to avoid import errors in limited environments
    def _get_db_conn(db_path: str, *, timeout: float = 30.0):
        raise RuntimeError("sqlite3 not available in this environment")
# === end helper ===


from concurrent.futures import ThreadPoolExecutor, as_completed
from requests.adapters import HTTPAdapter
try:
    from urllib3.util.retry import Retry
except Exception:
    from requests.packages.urllib3.util.retry import Retry

import io
import re
import html
import requests
import sqlite3

# --- æ—¢å­˜ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ï¼ˆç’°å¢ƒã®ã¾ã¾ï¼‰ ---
from src.trends import fetch_trends
from src.news import fetch_news
from src.bbs import fetch_bbs_stats, fetch_leak_comments, fetch_tob_comments
from src.discovery import build_universe
from src.common import ensure_dir, dump_json, now_iso, load_config

ROOT = Path(__file__).resolve().parent
OUT_DIR = ROOT / "out"
OUT_BBS_DIR = ROOT / "out_bbs"
DASH_DIR = ROOT / "dashboard"
JST = timezone(timedelta(hours=9))
DB_PATH = r"H:\desctop\æ ªæ”»ç•¥\1-ã‚¹ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°è‡ªå‹•åŒ–ãƒ—ãƒ­ã‚°ãƒ©ãƒ \main\db\kani2.db"

# ------------------ ç°¡æ˜“ã‚»ãƒ³ãƒãƒ¡ãƒ³ãƒˆ ------------------
POS_KW = ["ä¸Šæ–¹ä¿®æ­£","å¢—ç›Š","æœ€é«˜ç›Š","éå»æœ€é«˜","å¢—é…","é»’å­—è»¢æ›","ä¸ŠæŒ¯ã‚Œ","å¥½èª¿","å¥½æ±ºç®—","ä¸Šä¹—ã›","å¤§å¹…å¢—","å…¬é–‹è²·ä»˜ã‘","å…¬é–‹è²·ã„ä»˜ã‘","TOB","ï¼´ï¼¯ï¼¢"]
NEG_KW = ["ä¸‹æ–¹ä¿®æ­£","æ¸›ç›Š","èµ¤å­—","æ¸›é…","æœªé”","ä¸‹æŒ¯ã‚Œ","ç‰¹æ","ä¸é©æ­£","ç›£ç†","å¤§å¹…æ¸›","æ¥­ç¸¾æ‚ªåŒ–"]

def _judge_sentiment(title: str) -> (str, int, str):
    t = title or ""
    for kw in POS_KW:
        if kw in t: return ("positive", +2, kw)
    for kw in NEG_KW:
        if kw in t: return ("negative", -2, kw)
    return ("neutral", 0, "")

# ---- title summarizer (backward-compat) ------------------------------------
import re  # æ—¢ã«importæ¸ˆã¿ãªã‚‰é‡è¤‡å¯

def _normalize_spaces(s: str) -> str:
    return re.sub(r"\s+", " ", (s or "")).strip()

def summarize_title_simple(title: str) -> str:
    """
    TDnetç­‰ã®ã‚¿ã‚¤ãƒˆãƒ«ã‹ã‚‰ãƒã‚¤ã‚ºã‚’è»½ãè½ã¨ã—ã¦è¦ç´„ï¼ˆæœ€å¤§80æ–‡å­—ï¼‰ã€‚
    - å„ç¨®æ‹¬å¼§ï¼ˆå…¨è§’å«ã‚€ï¼‰å†…ã‚’ã–ã£ãã‚Šé™¤å»
    - ã€Œé©æ™‚é–‹ç¤º:ã€ã€Œã€œã®ãŠçŸ¥ã‚‰ã›ã€ç­‰ã®å®šå‹ã‚’æ•´ç†
    - é€£ç¶šç©ºç™½ã‚’1ã¤ã«
    """
    t = str(title or "")
    t = re.sub(r"ã€.*?ã€‘|\[.*?\]|\(.*?\)|ï¼ˆ.*?ï¼‰|ï¼œ.*?ï¼|ã€ˆ.*?ã€‰", "", t)
    t = re.sub(r"^\s*é©æ™‚é–‹ç¤º\s*[:ï¼š-]?\s*", "", t)
    t = re.sub(r"\s*ã®ãŠçŸ¥ã‚‰ã›\s*$", "", t)
    t = _normalize_spaces(t)
    return (t[:80] + "â€¦") if len(t) > 80 else t

def _summarize_title_simple(title: str):
    """
    æ—§ã‚³ãƒ¼ãƒ‰äº’æ›ã®æˆ»ã‚Šå€¤ï¼ˆä¸‰ã¤çµ„ï¼‰ã‚’è¿”ã™:
      (summary:str, sentiment_label:str, hit_keyword:str)
    æ—¢å­˜ã® upsert_offerings_events ç­‰ã§
      '_, label, _ = _summarize_title_simple(title)'
    ã¨å—ã‘ã‚‹æƒ³å®šã«åˆã‚ã›ã‚‹ã€‚
    """
    summary = summarize_title_simple(title)
    label, score, hit_kw = _judge_sentiment(title)  # label: "positive"/"neutral"/"negative"
    return summary, label, hit_kw
# ---------------------------------------------------------------------------


# ------------------ TDnet: å–å¾— ------------------
TDNET_LIST_JSON = "https://webapi.yanoshin.jp/webapi/tdnet/list/{date_from}-{date_to}.json"

# ç”¨é€”åˆ¥ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã«åˆ†é›¢
EARNINGS_KW = ["æ±ºç®—","çŸ­ä¿¡","å››åŠæœŸ","é€šæœŸ","ä¸Šæ–¹ä¿®æ­£","ä¸‹æ–¹ä¿®æ­£","æ¥­ç¸¾","é…å½“","é€²æ—"]

# â˜…TOBã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
TOB_KW = [
    "å…¬é–‹è²·ä»˜ã‘", "å…¬é–‹è²·ã„ä»˜ã‘", "å…¬é–‹è²·ä»˜", "å…¬é–‹è²·ã„ä»˜",
    "TOB", "ï¼´ï¼¯ï¼¢"
]

def _unescape_text(s: str) -> str:
    if not s: return ""
    try:
        if r"\u" in s:
            s = s.encode("utf-8").decode("unicode_escape")
    except Exception: pass
    return html.unescape(s).strip()

from datetime import datetime, timedelta, timezone
from typing import Dict, Any, List
import time

from urllib.parse import quote
def _http_get_json(url: str, retries: int = 3, sleep_sec: float = 0.8):
    """HTTP GET JSON with pooled session + retry."""
    global _HTTP
    try:
        _HTTP
    except NameError:
        _HTTP = requests.Session()
        try:
            _RETRY = Retry(total=3, backoff_factor=0.3, status_forcelist=[429, 500, 502, 503, 504])
            _ADPT  = HTTPAdapter(pool_connections=32, pool_maxsize=32, max_retries=_RETRY)
        except Exception:
            _ADPT  = HTTPAdapter(pool_connections=32, pool_maxsize=32)
        _HTTP.mount("http://", _ADPT); _HTTP.mount("https://", _ADPT)

    for i in range(retries):
        try:
            r = _HTTP.get(url, timeout=15, headers={"User-Agent": "Mozilla/5.0"})
            r.raise_for_status()
            ctype = (r.headers.get("Content-Type") or "").lower()
            # éJSONå¿œç­”(ä¾‹: HTMLã‚¨ãƒ©ãƒ¼ãƒšãƒ¼ã‚¸)ã¯ãƒªãƒˆãƒ©ã‚¤/æœ€å¾Œã ã‘ç°¡æ½”ãƒ­ã‚°
            if "application/json" not in ctype and not r.text.strip().startswith(("{","[")):
                if i == retries - 1:
                    print(f"[_http_get_json] non-json {ctype or '(unknown)'} url={url}")
                else:
                    import time; time.sleep(sleep_sec)
                continue
            return r.json()
        except Exception as e:
            if i == retries - 1:
                msg = str(e).splitlines()[0]
                print(f"[_http_get_json] error: {msg} url={url}")
                return None
            import time; time.sleep(sleep_sec)
    return None

def fetch_earnings_tdnet_only(days: int = 90, per_day_limit: int = 300,
                              slice_escalation=(12, 6, 3, 1, 0.5)) -> List[Dict[str, Any]]:
    # æ±ç”¨é–¢æ•°ã« EARNINGS_KW ã‚’æ¸¡ã™ã ã‘
    return fetch_tdnet_by_keywords(
        days=days,
        keywords=EARNINGS_KW,
        per_day_limit=per_day_limit,
        slice_escalation=slice_escalation
    )

def ensure_earnings_schema(conn):
    """
    earnings_events ã‚’æ—¥æœ¬èªã‚«ãƒ©ãƒ ã®ã¿ã®ã‚¹ã‚­ãƒ¼ãƒã«çµ±ä¸€ã€‚
    ä¸»ã‚­ãƒ¼ã¯æŒãŸãšã€UNIQUE(ã‚³ãƒ¼ãƒ‰, æå‡ºæ™‚åˆ») ã‚’ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã§ä¿è¨¼ã€‚
    """
    cur = conn.cursor()
    cur.execute("""
    CREATE TABLE IF NOT EXISTS earnings_events(
        ã‚³ãƒ¼ãƒ‰       TEXT NOT NULL,
        éŠ˜æŸ„å       TEXT,
        ã‚¿ã‚¤ãƒˆãƒ«     TEXT,
        ãƒªãƒ³ã‚¯       TEXT,
        ç™ºè¡¨æ—¥æ™‚     TEXT,
        æå‡ºæ™‚åˆ»     TEXT NOT NULL,
        è¦ç´„         TEXT,
        åˆ¤å®š         TEXT,
        åˆ¤å®šã‚¹ã‚³ã‚¢   INTEGER,
        ç†ç”±JSON     TEXT,
        æŒ‡æ¨™JSON     TEXT,
        é€²æ—ç‡       REAL,
        ã‚»ãƒ³ãƒãƒ¡ãƒ³ãƒˆ TEXT,
        ç´ ç‚¹         INTEGER,
        created_at   TEXT DEFAULT (datetime('now','localtime'))
    );
    """)
    cur.execute("PRAGMA table_info(earnings_events);")
    existing = {r[1] for r in cur.fetchall()}
    def _add(col, typ):
        if col not in existing:
            cur.execute(f'ALTER TABLE earnings_events ADD COLUMN "{col}" {typ};')
    for col, typ in [
        ("ã‚³ãƒ¼ãƒ‰","TEXT NOT NULL"),
        ("éŠ˜æŸ„å","TEXT"),
        ("ã‚¿ã‚¤ãƒˆãƒ«","TEXT"),
        ("ãƒªãƒ³ã‚¯","TEXT"),
        ("ç™ºè¡¨æ—¥æ™‚","TEXT"),
        ("æå‡ºæ™‚åˆ»","TEXT NOT NULL"),
        ("è¦ç´„","TEXT"),
        ("åˆ¤å®š","TEXT"),
        ("åˆ¤å®šã‚¹ã‚³ã‚¢","INTEGER"),
        ("ç†ç”±JSON","TEXT"),
        ("æŒ‡æ¨™JSON","TEXT"),
        ("é€²æ—ç‡","REAL"),
        ("ã‚»ãƒ³ãƒãƒ¡ãƒ³ãƒˆ","TEXT"),
        ("ç´ ç‚¹","INTEGER"),
        ("created_at","TEXT"),
    ]:
        _add(col, typ)

    cur.execute("DROP INDEX IF EXISTS idx_earn_time;")
    cur.execute("DROP INDEX IF EXISTS idx_earn_announced;")
    cur.execute("DROP INDEX IF EXISTS idx_earn_code_time;")
    cur.execute("""
      CREATE UNIQUE INDEX IF NOT EXISTS idx_earn_code_teishutsu
      ON earnings_events(ã‚³ãƒ¼ãƒ‰, æå‡ºæ™‚åˆ»);
    """)
    cur.execute("""
      CREATE INDEX IF NOT EXISTS idx_earn_teishutsu_desc
      ON earnings_events(æå‡ºæ™‚åˆ» DESC);
    """)
    conn.commit()

# ========= offerings_events: å¢—è³‡/è¡Œä½¿ ç³»ã‚¤ãƒ™ãƒ³ãƒˆä¿å­˜ =========

OFFERING_KW = [
    "å…¬å‹Ÿå¢—è³‡","ç¬¬ä¸‰è€…å‰²å½“","ç¬¬ä¸‰è€…å‰²å½“å¢—è³‡","æ–°æ ªç™ºè¡Œ","è‡ªå·±æ ªå¼ã®å‡¦åˆ†",
    "æ ªå¼ã®å£²å‡º","å‹Ÿé›†æ–°æ ªäºˆç´„æ¨©","æ–°æ ªäºˆç´„æ¨©ç™ºè¡Œ","MSãƒ¯ãƒ©ãƒ³ãƒˆ","ãƒ¯ãƒ©ãƒ³ãƒˆ",
    "è»¢æ›ç¤¾å‚µ","CB","EBå‚µ","ãƒ©ã‚¤ãƒ„ãƒ»ã‚ªãƒ•ã‚¡ãƒªãƒ³ã‚°",
    # è¡Œä½¿ç³»
    "è¡Œä½¿", "è¡Œä½¿çŠ¶æ³", "æ–°æ ªäºˆç´„æ¨©ã®è¡Œä½¿"
]

def ensure_offerings_schema(conn: sqlite3.Connection):
    cur = conn.cursor()
    cur.execute("""
    CREATE TABLE IF NOT EXISTS offerings_events(
        ã‚³ãƒ¼ãƒ‰       TEXT NOT NULL,
        éŠ˜æŸ„å       TEXT,
        ã‚¿ã‚¤ãƒˆãƒ«     TEXT,
        ãƒªãƒ³ã‚¯       TEXT,
        ç™ºè¡¨æ—¥æ™‚     TEXT,
        æå‡ºæ™‚åˆ»     TEXT NOT NULL,
        ç¨®åˆ¥         TEXT,
        å‚ç…§ID       TEXT,
        ã‚»ãƒ³ãƒãƒ¡ãƒ³ãƒˆ TEXT,
        created_at   TEXT DEFAULT (datetime('now','localtime')),
        UNIQUE(ã‚³ãƒ¼ãƒ‰, æå‡ºæ™‚åˆ», ã‚¿ã‚¤ãƒˆãƒ«)
    );
    """)
    cur.execute("""CREATE INDEX IF NOT EXISTS idx_offerings_date ON offerings_events(substr(æå‡ºæ™‚åˆ»,1,10));""")
    cur.execute("""CREATE INDEX IF NOT EXISTS idx_offerings_code_date ON offerings_events(ã‚³ãƒ¼ãƒ‰, substr(æå‡ºæ™‚åˆ»,1,10));""")
    conn.commit()

def load_offerings_recent_map(db_path: str, days: int = 365*3) -> dict[str, bool]:
    out: dict[str, bool] = {}
    try:
        conn = _get_db_conn(db_path)
        cur = conn.cursor()
        cur.execute("SELECT name FROM sqlite_master WHERE type='table' AND name='offerings_events'")
        if not cur.fetchone():
            conn.close()
            return out
        rows = cur.execute("""
            SELECT DISTINCT
                   CASE
                     WHEN length(ã‚³ãƒ¼ãƒ‰)=4 THEN ã‚³ãƒ¼ãƒ‰
                     WHEN length(ã‚³ãƒ¼ãƒ‰)=5 AND substr(ã‚³ãƒ¼ãƒ‰,1,1) BETWEEN '0' AND '9' THEN substr(ã‚³ãƒ¼ãƒ‰,1,4)
                     ELSE printf('%04d', CAST(ã‚³ãƒ¼ãƒ‰ AS INTEGER))
                   END AS code4
            FROM offerings_events
            WHERE date(æå‡ºæ™‚åˆ») >= date('now', ?)
        """, (f"-{days} day",)).fetchall()
        conn.close()
        for (c,) in rows:
            if c:
                out[str(c).zfill(4)] = True
    except Exception as e:
        print("[offerings] load_offerings_recent_map error:", e)
    return out

def _classify_offering_kind(title: str) -> str:
    t = title or ""
    if any(k in t for k in ["è¡Œä½¿","è¡Œä½¿çŠ¶æ³","æ–°æ ªäºˆç´„æ¨©ã®è¡Œä½¿"]): return "è¡Œä½¿"
    if any(k in t for k in ["å…¬å‹Ÿå¢—è³‡","ç¬¬ä¸‰è€…å‰²å½“","ç¬¬ä¸‰è€…å‰²å½“å¢—è³‡","æ–°æ ªç™ºè¡Œ","å‹Ÿé›†æ–°æ ªäºˆç´„æ¨©","æ–°æ ªäºˆç´„æ¨©ç™ºè¡Œ","MSãƒ¯ãƒ©ãƒ³ãƒˆ","ãƒ¯ãƒ©ãƒ³ãƒˆ","ãƒ©ã‚¤ãƒ„ãƒ»ã‚ªãƒ•ã‚¡ãƒªãƒ³ã‚°"]): return "å¢—è³‡"
    if any(k in t for k in ["æ ªå¼ã®å£²å‡º","è‡ªå·±æ ªå¼ã®å‡¦åˆ†"]): return "å£²å‡º/å‡¦åˆ†"
    if any(k in t for k in ["è»¢æ›ç¤¾å‚µ","CB","EBå‚µ"]): return "CB/EB"
    return "ãã®ä»–"

def upsert_offerings_events(conn: sqlite3.Connection, tdnet_items: list[dict]):
    import re, json
    cur = conn.cursor()

    def norm_code(it: dict) -> str:
        td = it.get("Tdnet", it) or {}
        raw = (td.get("company_code") or td.get("code") or td.get("company_code_raw") or "").strip()
        if re.fullmatch(r"\d{5}", raw): return raw[:4]
        if re.fullmatch(r"\d{4}", raw): return raw
        return (td.get("ticker") or "").strip()

    def norm_time(s: str) -> str:
        s = (s or "").replace("T"," ").replace("+09:00","").replace("/","-").strip()
        return s

    count_upd = 0
    for it in tdnet_items or []:
        td = it.get("Tdnet", it) or {}
        title = (td.get("title") or "").strip()
        if not title: 
            continue
        if not any(kw in title for kw in OFFERING_KW):
            continue

        code  = norm_code(it) or "0000"
        name  = (td.get("company_name") or "").strip() or code
        link  = (td.get("document_url") or td.get("pdf_url") or td.get("url") or "").strip()
        pub   = norm_time(td.get("pubdate") or td.get("publish_datetime") or "")
        tei   = pub or now_iso()
        kind  = _classify_offering_kind(title)
        refid = td.get("id") or link or title
        _, label, _ = _summarize_title_simple(title)

        cur.execute("""
            INSERT INTO offerings_events(ã‚³ãƒ¼ãƒ‰,éŠ˜æŸ„å,ã‚¿ã‚¤ãƒˆãƒ«,ãƒªãƒ³ã‚¯,ç™ºè¡¨æ—¥æ™‚,æå‡ºæ™‚åˆ»,ç¨®åˆ¥,å‚ç…§ID,ã‚»ãƒ³ãƒãƒ¡ãƒ³ãƒˆ)
            VALUES(?,?,?,?,?,?,?,?,?)
            ON CONFLICT(ã‚³ãƒ¼ãƒ‰,æå‡ºæ™‚åˆ»,ã‚¿ã‚¤ãƒˆãƒ«) DO UPDATE SET
              éŠ˜æŸ„å=excluded.éŠ˜æŸ„å,
              ãƒªãƒ³ã‚¯=excluded.ãƒªãƒ³ã‚¯,
              ç™ºè¡¨æ—¥æ™‚=excluded.ç™ºè¡¨æ—¥æ™‚,
              ç¨®åˆ¥=excluded.ç¨®åˆ¥,
              å‚ç…§ID=excluded.å‚ç…§ID,
              ã‚»ãƒ³ãƒãƒ¡ãƒ³ãƒˆ=excluded.ã‚»ãƒ³ãƒãƒ¡ãƒ³ãƒˆ
        """, (code,name,title,link,pub,tei,kind,refid,label))
        count_upd += 1

    conn.commit()
    if count_upd:
        print(f"[offerings] upsert rows: {count_upd}")

# ========= â˜…TOBå°‚ç”¨ãƒ¬ãƒ¼ãƒ³ï¼ˆtob_eventsï¼‰ =========

def ensure_tob_schema(conn: sqlite3.Connection):
    cur = conn.cursor()
    cur.execute("""
    CREATE TABLE IF NOT EXISTS tob_events (
      id                INTEGER PRIMARY KEY AUTOINCREMENT,
      ã‚³ãƒ¼ãƒ‰            TEXT NOT NULL,
      éŠ˜æŸ„å            TEXT,
      ã‚¿ã‚¤ãƒˆãƒ«          TEXT NOT NULL,
      ãƒªãƒ³ã‚¯            TEXT,
      ç™ºè¡¨æ—¥æ™‚          TEXT,
      æå‡ºæ™‚åˆ»          TEXT NOT NULL,
      ç¨®åˆ¥              TEXT DEFAULT 'TOB',
      è²·ä»˜ä¾¡æ ¼          REAL,
      ä¸‹é™ä¾¡æ ¼          REAL,
      ä¸Šé™ä¾¡æ ¼          REAL,
      ç›®æ¨™ä¿æœ‰æ¯”ç‡      REAL,
      æœ€ä½å¿œå‹Ÿæ ªæ•°      INTEGER,
      ãƒ¡ãƒ¢              TEXT
    );
    """)
    cur.execute("""CREATE UNIQUE INDEX IF NOT EXISTS ux_tob_unique
                   ON tob_events(ã‚³ãƒ¼ãƒ‰, æå‡ºæ™‚åˆ», ã‚¿ã‚¤ãƒˆãƒ«);""")
    cur.execute("""CREATE INDEX IF NOT EXISTS idx_tob_code_date
                   ON tob_events(ã‚³ãƒ¼ãƒ‰, substr(æå‡ºæ™‚åˆ»,1,10));""")
    cur.execute("""CREATE INDEX IF NOT EXISTS idx_tob_date_only
                   ON tob_events(substr(æå‡ºæ™‚åˆ»,1,10));""")
    cur.execute("""CREATE INDEX IF NOT EXISTS idx_tob_teishutsu_desc
                   ON tob_events(æå‡ºæ™‚åˆ» DESC);""")
    conn.commit()

_NUM = r"(?:\d{1,3}(?:,\d{3})*|\d+)"
def _to_number(s):
    try:
        return float(str(s).replace(",", ""))
    except Exception:
        return None

def parse_tob_details(text: str) -> dict:
    """ã‚¿ã‚¤ãƒˆãƒ«ã‚„PDFãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰TOBã®ä¸»è¦æ•°å€¤ã£ã½ã„ã‚‚ã®ã‚’ã‚†ã‚‹ãæŠ½å‡º"""
    t = text or ""
    out = {"è²·ä»˜ä¾¡æ ¼": None, "ä¸‹é™ä¾¡æ ¼": None, "ä¸Šé™ä¾¡æ ¼": None,
           "ç›®æ¨™ä¿æœ‰æ¯”ç‡": None, "æœ€ä½å¿œå‹Ÿæ ªæ•°": None}

    m = re.search(r"(è²·ä»˜(?:ã‘)?ä¾¡æ ¼)[ï¼š:\s]*(" + _NUM + r")\s*å††", t)
    if m: out["è²·ä»˜ä¾¡æ ¼"] = _to_number(m.group(2))

    m = re.search(r"(?:è²·ä»˜(?:ã‘)?ä»·æ ¼|è²·ä»˜(?:ã‘)?ä¾¡é¡|è²·ä»˜(?:ã‘)?ä¾¡æ ¼)[^0-9]*(" + _NUM + r")\s*å††\s*[~ã€œï¼-]\s*(" + _NUM + r")\s*å††", t)
    if m:
        out["ä¸‹é™ä¾¡æ ¼"] = _to_number(m.group(1))
        out["ä¸Šé™ä¾¡æ ¼"] = _to_number(m.group(2))

    m = re.search(r"(ç›®æ¨™ä¿æœ‰æ¯”ç‡|ä¿æœ‰æ¯”ç‡)[ï¼š:\s]*(" + _NUM + r"(?:\.\d+)?)\s*%", t)
    if m: out["ç›®æ¨™ä¿æœ‰æ¯”ç‡"] = _to_number(m.group(2))

    m = re.search(r"(æœ€ä½å¿œå‹Ÿæ ªæ•°)[ï¼š:\s]*(" + _NUM + r")\s*æ ª", t)
    if m: out["æœ€ä½å¿œå‹Ÿæ ªæ•°"] = int(_to_number(m.group(2)) or 0)

    return out

from html import unescape
def _norm_code(s: str) -> str:
    s = (s or "").strip()
    if len(s) >= 4 and s[:4].isdigit():
        return s[:4]
    return s

def tdnet_item_to_tob_row(it: dict) -> dict:
    td = it.get("Tdnet") or it  # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
    title = unescape(td.get("title") or "")
    code = _norm_code(td.get("company_code") or td.get("code") or "")
    name = td.get("company_name") or td.get("name") or ""
    link = td.get("document_url") or td.get("pdf_url") or td.get("url") or ""
    pub = td.get("publish_datetime") or td.get("pubdate") or td.get("time") or ""
    time_str = (pub or "").replace("T"," ").replace("+09:00","").replace("/","-")

    details = parse_tob_details(title)
    return {
        "ã‚³ãƒ¼ãƒ‰": code,
        "éŠ˜æŸ„å": name,
        "ã‚¿ã‚¤ãƒˆãƒ«": title,
        "ãƒªãƒ³ã‚¯": link,
        "ç™ºè¡¨æ—¥æ™‚": time_str,
        "æå‡ºæ™‚åˆ»": time_str or now_iso(),
        "ç¨®åˆ¥": "TOB",
        "è²·ä»˜ä¾¡æ ¼": details.get("è²·ä»˜ä¾¡æ ¼"),
        "ä¸‹é™ä¾¡æ ¼": details.get("ä¸‹é™ä¾¡æ ¼"),
        "ä¸Šé™ä¾¡æ ¼": details.get("ä¸Šé™ä¾¡æ ¼"),
        "ç›®æ¨™ä¿æœ‰æ¯”ç‡": details.get("ç›®æ¨™ä¿æœ‰æ¯”ç‡"),
        "æœ€ä½å¿œå‹Ÿæ ªæ•°": details.get("æœ€ä½å¿œå‹Ÿæ ªæ•°"),
        "ãƒ¡ãƒ¢": None,
    }

def fetch_tdnet_tob(days: int = 90, per_day_limit: int = 300) -> List[dict]:
    """Yanoshin TDNET list API ã‹ã‚‰ TOBã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã ã‘æŠ½å‡ºã—ã¦è¿”ã™ã€‚"""
    items = fetch_tdnet_by_keywords(days=days, keywords=TOB_KW, per_day_limit=per_day_limit)
    rows = []
    for it in items:
        td = it.get("Tdnet") or it
        title = (td.get("title") or "")
        if any(k in title for k in TOB_KW):
            rows.append(tdnet_item_to_tob_row(it))
    return rows

def upsert_tob_events(conn: sqlite3.Connection, rows: List[dict]) -> int:
    """tob_events ã¸ãƒãƒ«ã‚¯UPSERTã€‚"""
    if not rows:
        return 0
    sql = """
    INSERT INTO tob_events
      (ã‚³ãƒ¼ãƒ‰, éŠ˜æŸ„å, ã‚¿ã‚¤ãƒˆãƒ«, ãƒªãƒ³ã‚¯, ç™ºè¡¨æ—¥æ™‚, æå‡ºæ™‚åˆ», ç¨®åˆ¥,
       è²·ä»˜ä¾¡æ ¼, ä¸‹é™ä¾¡æ ¼, ä¸Šé™ä¾¡æ ¼, ç›®æ¨™ä¿æœ‰æ¯”ç‡, æœ€ä½å¿œå‹Ÿæ ªæ•°, ãƒ¡ãƒ¢)
    VALUES
      (:ã‚³ãƒ¼ãƒ‰, :éŠ˜æŸ„å, :ã‚¿ã‚¤ãƒˆãƒ«, :ãƒªãƒ³ã‚¯, :ç™ºè¡¨æ—¥æ™‚, :æå‡ºæ™‚åˆ», :ç¨®åˆ¥,
       :è²·ä»˜ä¾¡æ ¼, :ä¸‹é™ä¾¡æ ¼, :ä¸Šé™ä¾¡æ ¼, :ç›®æ¨™ä¿æœ‰æ¯”ç‡, :æœ€ä½å¿œå‹Ÿæ ªæ•°, :ãƒ¡ãƒ¢)
    ON CONFLICT(ã‚³ãƒ¼ãƒ‰, æå‡ºæ™‚åˆ», ã‚¿ã‚¤ãƒˆãƒ«) DO NOTHING
    """
    with conn:
        conn.executemany(sql, rows)
    print(f"[TOB] upsert rows: {len(rows)}")
    return len(rows)

def load_tob_for_dashboard_from_db(db_path: str, days: int = 120) -> list[dict]:
    """ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ç”¨ã«tob_eventsã‚’èª­ã¿å‡ºã—"""
    conn = _get_db_conn(db_path)
    cur = conn.cursor()
    rows = cur.execute(
        """
        SELECT ã‚³ãƒ¼ãƒ‰,éŠ˜æŸ„å,ã‚¿ã‚¤ãƒˆãƒ«,ãƒªãƒ³ã‚¯,ç™ºè¡¨æ—¥æ™‚,æå‡ºæ™‚åˆ»,ç¨®åˆ¥,
               è²·ä»˜ä¾¡æ ¼,ä¸‹é™ä¾¡æ ¼,ä¸Šé™ä¾¡æ ¼,ç›®æ¨™ä¿æœ‰æ¯”ç‡,æœ€ä½å¿œå‹Ÿæ ªæ•°
        FROM tob_events
        WHERE date(æå‡ºæ™‚åˆ») >= date('now', ?)
        ORDER BY æå‡ºæ™‚åˆ» DESC
        """,
        (f"-{days} day",)
    ).fetchall()
    conn.close()
    out = []
    for r in rows:
        out.append({
            "ticker": (r[0] or "0000"),
            "name": r[1] or "",
            "title": r[2] or "",
            "link": r[3] or "",
            "announced": (r[4] or ""),
            "submitted": (r[5] or ""),
            "kind": r[6] or "TOB",
            "buy_price": r[7],
            "range_low": r[8],
            "range_high": r[9],
            "target_ratio": r[10],
            "min_shares": r[11],
        })
    return out

# ========= â˜…æ²ç¤ºæ¿TOBã‚¿ãƒ–ã®é™¤å¤–ã«ä½¿ã†ï¼šTDNETæ±ºå®šæ¸ˆã¿ã‚³ãƒ¼ãƒ‰é›†åˆ =========
def load_tob_codes_recent(db_path: str, days: int = 180) -> set[str]:
    """
    tob_events ã«ç™»éŒ²æ¸ˆã¿ï¼ˆï¼TDNETã§TOBãŒå…¬è¡¨æ¸ˆã¿ï¼‰ã®éŠ˜æŸ„ã‚³ãƒ¼ãƒ‰ï¼ˆ4æ¡ï¼‰ã‚’ç›´è¿‘daysæ—¥åˆ†ã ã‘è¿”ã™ã€‚
    æ²ç¤ºæ¿TOBã‚¿ãƒ–ã§ã®é™¤å¤–ã«åˆ©ç”¨ã€‚
    """
    out: set[str] = set()
    try:
        conn = _get_db_conn(db_path)
        cur = conn.cursor()
        # ãƒ†ãƒ¼ãƒ–ãƒ«å­˜åœ¨ç¢ºèª
        cur.execute("SELECT name FROM sqlite_master WHERE type='table' AND name='tob_events'")
        if not cur.fetchone():
            conn.close()
            return out
        cur.execute("""
            SELECT DISTINCT
                   CASE
                     WHEN length(ã‚³ãƒ¼ãƒ‰)=4 THEN ã‚³ãƒ¼ãƒ‰
                     WHEN length(ã‚³ãƒ¼ãƒ‰)=5 AND substr(ã‚³ãƒ¼ãƒ‰,1,1) BETWEEN '0' AND '9' THEN substr(ã‚³ãƒ¼ãƒ‰,1,4)
                     ELSE printf('%04d', CAST(ã‚³ãƒ¼ãƒ‰ AS INTEGER))
                   END AS code4
            FROM tob_events
            WHERE date(æå‡ºæ™‚åˆ») >= date('now', ?)
        """, (f"-{days} day",))
        for (c,) in cur.fetchall():
            if c:
                out.add(str(c).zfill(4))
        conn.close()
    except Exception as e:
        print("[TOB] load_tob_codes_recent error:", e)
    return out

# ========= æ±ºç®—UPSERT/ãƒ­ãƒ¼ãƒ‰ =========

def upsert_earnings_rows(conn: sqlite3.Connection, rows: list[dict]):
    """
    TDnetæŠ½å‡º rows ã‚’æ—¥æœ¬èªã‚¹ã‚­ãƒ¼ãƒ earnings_events ã«UPSERTã€‚
    - å–ã‚Šè¾¼ã¿å¯¾è±¡ï¼šã€æ±ºç®—çŸ­ä¿¡ï¼å››åŠæœŸï¼é€šæœŸï¼æ±ºç®—ã€ã‚’å«ã¿ã€ã€å‹•ç”»ï¼èª¬æ˜è³‡æ–™ã€ã¯é™¤å¤–
    - ã‚­ãƒ¼ï¼š (ã‚³ãƒ¼ãƒ‰, æå‡ºæ™‚åˆ»)
    - UPDATE 0ä»¶ãªã‚‰ INSERT
    """
    import json, re
    cur = conn.cursor()

    cur.execute("PRAGMA table_info(earnings_events);")
    cols = {r[1] for r in cur.fetchall()}

    POS_KEYS = ("æ±ºç®—çŸ­ä¿¡","å››åŠæœŸ","é€šæœŸ","æ±ºç®—")
    EXCLUDE  = ("å‹•ç”»","èª¬æ˜è³‡æ–™")

    def norm_code(r: dict) -> str:
        cc = (r.get("company_code") or r.get("company_code_raw") or "").strip()
        if cc.isdigit() and len(cc) == 5:
            return cc[:4]
        return str(r.get("ticker") or "").zfill(4)

    for r in rows:
        title = (r.get("title") or "").strip()
        if not any(k in title for k in POS_KEYS):
            continue
        if any(k in title for k in EXCLUDE):
            continue

        code = norm_code(r)
        announced = (r.get("time") or "").replace("T"," ").replace("+09:00","").strip()
        teishutsu = announced or datetime.now().strftime("%Y-%m-%d %H:%M:%S")

        jp_vals_all = {
            "ã‚³ãƒ¼ãƒ‰": code,
            "éŠ˜æŸ„å": r.get("name") or "",
            "ã‚¿ã‚¤ãƒˆãƒ«": title,
            "ãƒªãƒ³ã‚¯": r.get("link") or "",
            "ç™ºè¡¨æ—¥æ™‚": announced,
            "æå‡ºæ™‚åˆ»": teishutsu,
            "è¦ç´„": r.get("summary") or "",
            "åˆ¤å®š": (r.get("verdict") or "").strip(),
            "åˆ¤å®šã‚¹ã‚³ã‚¢": int(r.get("score_judge") or 0),
            "ç†ç”±JSON": json.dumps(r.get("reasons") or [], ensure_ascii=False),
            "æŒ‡æ¨™JSON": json.dumps(r.get("metrics") or {}, ensure_ascii=False),
            "é€²æ—ç‡": r.get("progress"),
            "ã‚»ãƒ³ãƒãƒ¡ãƒ³ãƒˆ": r.get("sentiment") or "",
            "ç´ ç‚¹": int(r.get("score") or 0),
        }

        set_cols, set_vals = [], []
        for k, v in jp_vals_all.items():
            if k in cols and k not in ("ã‚³ãƒ¼ãƒ‰","æå‡ºæ™‚åˆ»") and v is not None:
                set_cols.append(f'"{k}"=?'); set_vals.append(v)

        updated = 0
        if set_cols:
            sql = (
                f'UPDATE earnings_events SET {", ".join(set_cols)} '
                f'WHERE "ã‚³ãƒ¼ãƒ‰"=? AND "æå‡ºæ™‚åˆ»"=?'
            )
            cur.execute(sql, set_vals + [jp_vals_all["ã‚³ãƒ¼ãƒ‰"], jp_vals_all["æå‡ºæ™‚åˆ»"]])
            updated = cur.rowcount

        if updated == 0:
            insert_cols, insert_vals = [], []
            for k, v in jp_vals_all.items():
                if k in cols and v is not None:
                    insert_cols.append(f'"{k}"'); insert_vals.append(v)
            placeholders = ",".join(["?"] * len(insert_cols))
            sql = f'INSERT OR IGNORE INTO earnings_events ({", ".join(insert_cols)}) VALUES ({placeholders})'
            cur.execute(sql, insert_vals)

    conn.commit()

def load_earnings_for_dashboard_from_db(db_path: str, days: int = 30, filter_mode: str = "nonnegative") -> list[dict]:
    import json as _json
    conn = _get_db_conn(db_path)
    cur = conn.cursor()
    rows = cur.execute(
        """
        SELECT
          "ã‚³ãƒ¼ãƒ‰","éŠ˜æŸ„å","ã‚¿ã‚¤ãƒˆãƒ«","ãƒªãƒ³ã‚¯","ç™ºè¡¨æ—¥æ™‚","æå‡ºæ™‚åˆ»",
          "è¦ç´„","åˆ¤å®š","åˆ¤å®šã‚¹ã‚³ã‚¢","ç†ç”±JSON","æŒ‡æ¨™JSON","é€²æ—ç‡","ã‚»ãƒ³ãƒãƒ¡ãƒ³ãƒˆ","ç´ ç‚¹"
        FROM "v_events_dedup"
        WHERE date("æå‡ºæ™‚åˆ»") >= date('now', ?)
        ORDER BY "æå‡ºæ™‚åˆ»" DESC
        """,
        (f"-{days} day",)
    ).fetchall()

    def _to_row(r):
        try:
            reasons = _json.loads(r[9]) if r[9] else []
        except Exception:
            reasons = []
        try:
            metrics = _json.loads(r[10]) if r[10] else {}
        except Exception:
            metrics = {}
        return {
            "ticker": (r[0] or "0000"),
            "name":   r[1] or (r[0] or "TDnet"),
            "title":  r[2] or "",
            "link":   r[3] or "",
            "time":   (r[5] or "").replace("T"," ").replace("+09:00",""),
            "summary": r[6] or "",
            "verdict": (r[7] or "").strip(),
            "score_judge": r[8],
            "reasons": reasons if isinstance(reasons, list) else [str(reasons)],
            "metrics": metrics if isinstance(metrics, dict) else {},
            "progress": r[11],
            "sentiment": r[12] or "neutral",
            "score": r[13] or 0,
        }

    items = [_to_row(r) for r in rows]

    if filter_mode == "pos_good":
        items = [x for x in items if (x.get("sentiment") == "positive") or ((x.get("verdict") or "") == "good")]
    elif filter_mode == "nonnegative":
        items = [x for x in items if (x.get("sentiment") != "negative") or ((x.get("verdict") or "") == "good")]

    try:
        codes = list({str(x.get("ticker") or "0000").zfill(4) for x in items})
        quotes = load_quotes_from_price_history(conn, codes)
        for x in items:
            q = quotes.get(str(x["ticker"]).zfill(4))
            if q:
                x["quote"] = q
    except Exception as e:
        print("[earnings] quotes attach error:", e)

    conn.close()

    def _ts(s: str) -> float:
        from datetime import datetime
        try:
            return datetime.strptime((s or "").replace("/", "-"), "%Y-%m-%d %H:%M:%S").timestamp()
        except Exception:
            return 0.0
    items.sort(key=lambda r: _ts(r.get("time","")), reverse=True)
    return items[:300]

# ------------------ HTMLç”Ÿæˆï¼ˆæ—¥åˆ¥ã‚¿ãƒ–è¾¼ã¿ + â˜…TOB(TDNET)ã‚¿ãƒ–ï¼‰ ------------------
def render_dashboard_html(payload: dict, api_base: str = "") -> str:
    import json
    data_json = json.dumps(payload, ensure_ascii=False)

    tpl = r"""<!doctype html>
<meta charset="utf-8" />
<title>æ³¨ç›®åº¦ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰</title>
<meta http-equiv="Cache-Control" content="no-cache, no-store, must-revalidate"/>
<meta http-equiv="Pragma" content="no-cache"/>
<meta http-equiv="Expires" content="0"/>
<style>
  :root { --br:#e5e7eb; --fg:#0f172a; --muted:#64748b; --bg:#fff; --pos:#16a34a; --neu:#6b7280; --neg:#dc2626; --tab:#3b82f6; }
  html,body{background:#fff}
  body{font-family:system-ui,-apple-system,Segoe UI,Roboto,'Noto Sans JP',sans-serif;margin:18px;color:var(--fg)}
  h1{font-size:26px;margin:0 0 10px}
  .toolbar{display:flex;gap:10px;align-items:center;margin:6px 0 14px}
  .tabs{display:flex;gap:8px}
  .tab{padding:8px 12px;border:1px solid var(--br);border-radius:999px;background:#f8fafc;color:#111;font-size:13px;cursor:pointer}
  .tab.active{background:#e6f0ff;border-color:#bfdbfe;color:#1d4ed8;font-weight:600}
  .stamp{font-size:12px;color:var(--muted);margin-left:8px}
  .card{border:1px solid var(--br);border-radius:12px;padding:12px;margin:14px 0;background:#fff}
  table{width:100%;border-collapse:collapse}
  th,td{border-bottom:1px solid #f1f5f9;padding:8px 10px;text-align:left;vertical-align:top}
  th{background:#f8fafc;color:#334155;font-weight:600;position:sticky;top:0}
  .codechip{display:inline-flex;align-items:center;gap:6px}
  .code{display:inline-block;background:#f1f5f9;border-radius:999px;padding:2px 8px;font-size:12px;color:#334155}
  .badge{display:inline-block;padding:2px 10px;border-radius:999px;color:#fff;font-size:12px;white-space:nowrap}
  .pos{background:var(--pos)} .neu{background:var(--neu)} .neg{background:var(--neg)}
  .newslist{display:grid;grid-template-columns:repeat(3,1fr);gap:10px;max-width:1100px}
  .newsitem{border:1px solid var(--br);border-radius:10px;padding:8px;background:#fff}
  .src{color:#64748b;font-size:12px}
  .btn{padding:8px 12px;border:1px solid var(--br);border-radius:10px;background:#f8fafc;cursor:pointer}
  .btn:hover{filter:brightness(0.98)}
  .small{font-size:12px;color:#64748b}
  a{color:#1d4ed8;text-decoration:none} a:hover{text-decoration:underline}
  .day-head{font-weight:700;margin:18px 0 8px}
  .muted{opacity:.85}
  td.num{text-align:right}
  td.diff.plus{color:#16a34a;font-weight:600}
  td.diff.minus{color:#dc2626;font-weight:600}
  .copybtn{cursor:pointer;text-decoration:none}
  .judge-row{ display:flex; gap:8px; align-items:center; flex-wrap:wrap; margin-top:6px; }
  .jdg{ display:inline-flex; align-items:center; gap:6px; padding:2px 10px; border-radius:999px; font-weight:700; border:1px solid var(--br); line-height:1.9; }
  .jdg .score{ font-weight:600; opacity:.8; }
  .jdg.positive{ color:#065f46; background:#ecfdf5; border-color:#a7f3d0; }
  .jdg.neutral { color:#374151; background:#f3f4f6; border-color:#e5e7eb; }
  .jdg.negative{ color:#7f1d1d; background:#fef2f2; border-color:#fecaca; }
  .ev{ padding:4px 10px; border-radius:12px; line-height:1.9; background:#f0f9ff; color:#0c4a6e; border-left:4px solid #38bdf8; }
  .ev .chip{ display:inline-block; margin:2px 6px 2px 0; padding:2px 8px; border-radius:999px; background:#e0f2fe; border:1px solid #bae6fd; font-weight:600; }
</style>

<h1>æ³¨ç›®åº¦ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰</h1>
<div class="toolbar">
  <div class="tabs">
    <button class="tab active" data-mode="total">ç·åˆ</button>
    <button class="tab" data-mode="bbs">æ²ç¤ºæ¿</button>
    <button class="tab" data-mode="earnings">æ±ºç®—</button>
    <button class="tab" data-mode="earnings_day">æ±ºç®—ï¼ˆæ—¥åˆ¥ï¼‰</button>
    <button class="tab" data-mode="leak">æ¼ã‚Œï¼Ÿ</button>
    <button class="tab" data-mode="tob">TOB</button>
    <button class="tab" data-mode="tob_tdnet">TOB(TDNET)</button>
  </div>
  <span id="stamp" class="stamp"></span>
  <button class="btn" id="btn-rerender">å†æç”»</button>
</div>

<div class="card">
  <table id="tbl">
    <thead id="thead"></thead>
    <tbody id="tbody"></tbody>
  </table>
</div>

<script>
  window.__DATA__ = __DATA_JSON__;
  const API_BASE = "";
  let MODE = "total";

  function esc(s){return (s??"").toString().replace(/[&<>\"']/g,m=>({"&":"&amp;","<":"&lt;",">":"&gt;","\"":"&quot;","'":"&#39;"}[m]));}
  function fmt(v){const n=Number(v||0);return isFinite(n)?n.toFixed(3):"0.000";}
  function badge(label){const cls=label==="positive"?"pos":(label==="negative"?"neg":"neu");return `<span class="badge ${cls}">${label||"neutral"}</span>`;}
  function parseJST(s){
    if(!s) return 0;
    const t = String(s).replace("T"," ").replace("+09:00","").trim();
    const m = t.match(/^(\d{4})-(\d{2})-(\d{2}) (\d{2}):(\d{2}):(\d{2})$/);
    if(m){
      const y=+m[1], mo=+m[2]-1, d=+m[3], h=+m[4], mi=+m[5], se=+m[6];
      const utcMs = Date.UTC(y, mo, d, h-9, mi, se);
      return isFinite(utcMs) ? utcMs : 0;
    }
    const dt = Date.parse(t+" +09:00");
    return isNaN(dt) ? 0 : dt;
  }
  function yQuoteUrl(code){const c=String(code||"").padStart(4,"0");return `https://finance.yahoo.co.jp/quote/${c}.T`;}
  function copyCode(code){
    navigator.clipboard?.writeText(String(code||""))?.then(()=>{})
  }
  function toNewsCards(items){
    if(!items||!items.length) return "";
    const html = items.slice(0,3).map(it=>{
      const t=esc(it.title||""); const l=esc(it.link||"#"); const s=esc(it.source||""); const p=esc(it.published||"");
      return `<div class="newsitem"><a href="${l}" target="_blank" rel="noopener">${t}</a><div class="src">${s}ã€€${p}</div></div>`;
    }).join("");
    return `<div class="newslist">${html}</div>`;
  }
  function stockCell(code,name){
    const c=String(code||"").padStart(4,"0"); const n=esc(name||c);
    return `<span class="codechip"><a class="copybtn" title="ã‚³ãƒ¼ãƒ‰ã‚’ã‚³ãƒ”ãƒ¼" href="javascript:void(0)" onclick="copyCode('${c}')">ğŸ“‹</a><span class="code">${c}</span><span>${n}</span></span>`;
  }

  function offeringCell(f){ return f ? `<span class="code">å¢—è³‡çµŒæ­´ã‚¢ãƒª</span>` : ``; }
  function verdictToLabel(v){
    if(v==="good") return "positive";
    if(v==="bad")  return "negative";
    return "neutral";
  }
  function judgeHtml(verdict, score, reasons){
    const cls = verdictToLabel(verdict||"");
    const s = (score ?? null) !== null ? `ï¼ˆscore ${esc(score)}ï¼‰` : "";
    const chips = (Array.isArray(reasons)?reasons:[])
      .filter(x=>x!=null && String(x).trim()!=="")
      .map(x=>`<span class="chip">${esc(String(x))}</span>`).join("");
    const ev = chips ? `<span class="ev">æ ¹æ‹ : ${chips}</span>` : "";
    return `<div class="judge-row"><span class="jdg ${cls}">åˆ¤å®š: ${esc(verdict||"")}<span class="score">${s}</span></span>${ev}</div>`;
  }

function render(j){
  document.getElementById("stamp").textContent = "ç”Ÿæˆæ™‚åˆ»: " + (j.generated_at || "");
  const thead=document.getElementById("thead");
  const tbody=document.getElementById("tbody");
  tbody.innerHTML="";

  if(MODE==="earnings_day"){
    const rows=(j.earnings||[])
      .slice()
      .sort((a,b)=>{const tb=parseJST(b.time),ta=parseJST(a.time);return tb!==ta?tb-ta:(Number(b.score_judge||0)-Number(a.score_judge||0));})
      .slice(0, 300);
    thead.innerHTML=`<tr>
      <th>#</th><th>ã‚³ãƒ¼ãƒ‰</th><th>éŠ˜æŸ„</th><th>å¢—è³‡çµŒæ­´</th><th>Yahoo</th>
      <th class="num">ç¾åœ¨å€¤</th><th class="num">å‰æ—¥çµ‚å€¤</th><th class="num">å‰æ—¥æ¯”(å††)</th>
      <th>åˆ¤å®šã‚¹ã‚³ã‚¢</th><th>Sentiment</th><th>æ›¸é¡å / ã‚µãƒãƒª / åˆ¤å®šç†ç”±</th><th>æ™‚åˆ»</th></tr>`;
    if(!rows.length){const tr=document.createElement("tr");tr.innerHTML=`<td colspan="12" class="small">ç›´è¿‘ã§ã¯è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚</td>`;tbody.appendChild(tr);return;}
    let day=""; rows.forEach((r,idx)=>{
      const d=(r.time||"").slice(0,10);
      if(d!==day){day=d; const trh=document.createElement("tr"); trh.innerHTML=`<td colspan="12" class="day-head">${esc(day)}</td>`; tbody.appendChild(trh);}
      const q=r.quote||{};
      const cur = q.current!=null ? Number(q.current).toLocaleString() : "";
      const prev= q.prev_close!=null ? Number(q.prev_close).toLocaleString() : "";
      const diff= q.diff_yen!=null ? Number(q.diff_yen) : null;
      const diffStr = diff!=null ? ((diff>=0?"+":"")+diff.toLocaleString()) : "";
      const diffCls = diff==null ? "" : (diff>=0?"plus":"minus");

      const tr=document.createElement("tr"); const t=(h,c)=>{const el=document.createElement("td"); if(c) el.className=c; el.innerHTML=h; tr.appendChild(el);};
      t(String(idx+1));
      t(`<a class="copybtn" title="ã‚³ãƒ”ãƒ¼" href="javascript:void(0)" onclick="copyCode('${esc(r.ticker)}')">ğŸ“‹</a> ${esc(r.ticker)}`);
      t(stockCell(r.ticker,r.name)); t(offeringCell(r.has_offering_history)); t(`<a href="${yQuoteUrl(r.ticker)}" target="_blank" rel="noopener">Yahoo</a>`);
      t(cur, "num"); t(prev, "num"); t(diffStr, "num diff "+diffCls);
      t(String(r.score_judge??0)); t(badge(r.sentiment||"neutral"));
      const titleHtml=r.link?`<a href="${esc(r.link)}" target="_blank" rel="noopener">${esc(r.title||"(ç„¡é¡Œ)")}</a>`:esc(r.title||"(ç„¡é¡Œ)");
      const summaryHtml=r.summary?`<div class="small muted" style="margin-top:6px; white-space:pre-line;">${esc(r.summary)}</div>`:"";
      const judgeBlock = judgeHtml(r.verdict||"", r.score_judge, r.reasons||[]);
      t(`${titleHtml}${summaryHtml}${judgeBlock}`);
      t(esc((r.time||"").replace("T"," ").replace("+09:00","")));
      tbody.appendChild(tr);
    });
    return;
  }

  if(MODE==="earnings"){
    const rows=(j.earnings||[])
      .slice()
      .sort((a,b)=>{const tb=parseJST(b.time),ta=parseJST(a.time);return tb!==ta?tb-ta:(Number(b.score_judge||0)-Number(a.score_judge||0));})
      .slice(0, 300);
    thead.innerHTML=`<tr>
      <th>#</th><th>ã‚³ãƒ¼ãƒ‰</th><th>éŠ˜æŸ„</th><th>å¢—è³‡çµŒæ­´</th><th>Yahoo</th>
      <th class="num">ç¾åœ¨å€¤</th><th class="num">å‰æ—¥çµ‚å€¤</th><th class="num">å‰æ—¥æ¯”(å††)</th>
      <th>åˆ¤å®šã‚¹ã‚³ã‚¢</th><th>Sentiment</th><th>æ›¸é¡å / ã‚µãƒãƒª / åˆ¤å®šç†ç”±</th><th>æ™‚åˆ»</th></tr>`;
    if(!rows.length){const tr=document.createElement("tr");tr.innerHTML=`<td colspan="12" class="small">ç›´è¿‘ã§ã¯è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚</td>`;tbody.appendChild(tr);return;}
    rows.forEach((r,idx)=>{
      const q=r.quote||{};
      const cur = q.current!=null ? Number(q.current).toLocaleString() : "";
      const prev= q.prev_close!=null ? Number(q.prev_close).toLocaleString() : "";
      const diff= q.diff_yen!=null ? Number(q.diff_yen) : null;
      const diffStr = diff!=null ? ((diff>=0?"+":"")+diff.toLocaleString()) : "";
      const diffCls = diff==null ? "" : (diff>=0?"plus":"minus");

      const tr=document.createElement("tr"); const t=(h,c)=>{const el=document.createElement("td"); if(c) el.className=c; el.innerHTML=h; tr.appendChild(el);};
      t(String(idx+1));
      t(`<a class="copybtn" title="ã‚³ãƒ”ãƒ¼" href="javascript:void(0)" onclick="copyCode('${esc(r.ticker)}')">ğŸ“‹</a> ${esc(r.ticker)}`);
      t(stockCell(r.ticker,r.name)); t(offeringCell(r.has_offering_history)); t(`<a href="${yQuoteUrl(r.ticker)}" target="_blank" rel="noopener">Yahoo</a>`);
      t(cur, "num"); t(prev, "num"); t(diffStr, "num diff "+diffCls);
      t(String(r.score_judge??0)); t(badge(r.sentiment||"neutral"));
      const titleHtml=r.link?`<a href="${esc(r.link)}" target="_blank" rel="noopener">${esc(r.title||"(ç„¡é¡Œ)")}</a>`:esc(r.title||"(ç„¡é¡Œ)");
      const summaryHtml=r.summary?`<div class="small muted" style="margin-top:6px; white-space:pre-line;">${esc(r.summary)}</div>`:"";
      const judgeBlock = judgeHtml(r.verdict||"", r.score_judge, r.reasons||[]);
      t(`${titleHtml}${summaryHtml}${judgeBlock}`);
      t(esc((r.time||"").replace("T"," ").replace("+09:00","")));
      tbody.appendChild(tr);
    });
    return;
  }

  if(MODE==="bbs"){
    const rows=(j.rows||[]).slice().sort((a,b)=>{
      const a24=a?.bbs?.posts_24h||0, b24=b?.bbs?.posts_24h||0; if(b24!==a24) return b24-a24;
      const a72=a?.bbs?.posts_72h||0, b72=b?.bbs?.posts_72h||0; if(b72!==a72) return b72-a72;
      return String(a.name||a.ticker).localeCompare(String(b.name||b.ticker));
    });
    thead.innerHTML=`<tr><th>#</th><th>éŠ˜æŸ„</th><th>å¢—è³‡çµŒæ­´</th><th>BBS(24h)</th><th>BBS(72h)</th><th>å¢—åŠ ç‡</th><th>Sentiment(News)</th><th>æœ€æ–°ãƒ‹ãƒ¥ãƒ¼ã‚¹</th></tr>`;
    rows.forEach((r,idx)=>{
      const tr=document.createElement("tr"); const t=(h)=>{const el=document.createElement("td"); el.innerHTML=h; tr.appendChild(el);};
      const b24=r?.bbs?.posts_24h||0, b72=r?.bbs?.posts_72h||0, growth=(b24/Math.max(1,b72)).toFixed(2);
      const items=(r.news&&r.news.items)?r.news.items:[]; const label=(items[0]?.sentiment)||"neutral";
      t(String(idx+1));
      t(`${esc(r.name||r.ticker)} <span class="code">(${esc(r.ticker)})</span>`);
      t(offeringCell(r.has_offering_history));
      t(String(b24));
      t(String(b72));
      t(String(growth));
      t(badge(label));
      t(toNewsCards(items));
      tbody.appendChild(tr);
    });
    return;
  }

  // å…±é€š: æ¤œç´¢ç³»ï¼ˆæ¼ã‚Œã¦ã‚‹ï¼Ÿ / TOB[æ²ç¤ºæ¿]ï¼‰ãƒ¬ãƒ³ãƒ€ãƒ©
  function renderAggRows(agg){
    const rows=(agg?.rows||[]).slice().sort((a,b)=> (b.count||0)-(a.count||0));
    thead.innerHTML = `<tr>
      <th>#</th><th>éŠ˜æŸ„</th><th>ä»¶æ•°</th><th>ã‚µãƒ³ãƒ—ãƒ«ï¼ˆæœ€å¤§6ä»¶ï¼‰</th>
    </tr>`;

    if(!rows.length){
      const tr=document.createElement("tr");
      tr.innerHTML=`<td colspan="4" class="small">è©²å½“ãŒã‚ã‚Šã¾ã›ã‚“ã€‚</td>`;
      tbody.appendChild(tr);
      return;
    }
    rows.forEach((r,idx)=>{
      const tr=document.createElement("tr");
      const t=(h)=>{const el=document.createElement("td"); el.innerHTML=h; tr.appendChild(el);};
      const samples=(r.comments||[]).slice(0,6).map(c=>{
        const lab = c.sentiment||"neutral";
        const badgeCls = lab==="positive" ? "pos" : (lab==="negative"?"neg":"neu");
        const text = esc(c.text||"");
        const link = esc(c.link||"#");
        const pub  = esc((c.published||"").toString());
        return `<div class="newsitem">
                  <a href="${link}" target="_blank" rel="noopener">${text}</a>
                  <div class="src"><span class="badge ${badgeCls}">${lab}</span>ã€€${pub}</div>
                </div>`;
      }).join("");
      t(String(idx+1));
      t(`${esc(r.name||r.ticker)} <span class="code">(${esc(r.ticker)})</span>`);
      t(String(r.count||0));
      t(`<div class="newslist">${samples}</div>`);
      tbody.appendChild(tr);
    });
  }

  if(MODE==="leak"){
    renderAggRows((j.leak||{}));
    return;
  }
  if(MODE==="tob"){
    renderAggRows((j.tob||{}));
    return;
  }

  // â˜…TDNETç”±æ¥ã®TOBè¡¨ç¤º
  if(MODE==="tob_tdnet"){
    const rows=(j.tob_tdnet||[]).slice();
    thead.innerHTML=`<tr>
      <th>#</th><th>ã‚³ãƒ¼ãƒ‰</th><th>éŠ˜æŸ„</th><th>æ›¸é¡å</th>
      <th class="num">è²·ä»˜ä¾¡æ ¼(å††)</th><th class="num">ãƒ¬ãƒ³ã‚¸</th><th class="num">ç›®æ¨™æ¯”ç‡(%)</th>
      <th class="num">æœ€ä½å¿œå‹Ÿæ ªæ•°</th><th>æå‡ºæ™‚åˆ»</th>
    </tr>`;
    if(!rows.length){
      const tr=document.createElement("tr");
      tr.innerHTML=`<td colspan="9" class="small">ç›´è¿‘ã§ã¯TOB( TDNET )ã¯è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚</td>`;
      tbody.appendChild(tr);
      return;
    }
    rows.forEach((r,idx)=>{
      const tr=document.createElement("tr");
      const t=(h,c)=>{const el=document.createElement("td"); if(c) el.className=c; el.innerHTML=h; tr.appendChild(el);};
      t(String(idx+1));
      t(`<a class="copybtn" title="ã‚³ãƒ”ãƒ¼" href="javascript:void(0)" onclick="copyCode('${esc(r.ticker)}')">ğŸ“‹</a> ${esc(r.ticker)}`);
      t(stockCell(r.ticker, r.name));
      const titleHtml=r.link?`<a href="${esc(r.link)}" target="_blank" rel="noopener">${esc(r.title||"(ç„¡é¡Œ)")}</a>`:esc(r.title||"(ç„¡é¡Œ)");
      t(titleHtml);
      t(r.buy_price!=null?Number(r.buy_price).toLocaleString():"", "num");
      const rg = (r.range_low!=null||r.range_high!=null) ? `${r.range_low!=null?Number(r.range_low).toLocaleString():""}ã€œ${r.range_high!=null?Number(r.range_high).toLocaleString():""}` : "";
      t(rg, "num");
      t(r.target_ratio!=null?Number(r.target_ratio).toLocaleString(): "", "num");
      t(r.min_shares!=null?Number(r.min_shares).toLocaleString():"", "num");
      t(esc((r.submitted||"").replace("T"," ").replace("+09:00","")));
      tbody.appendChild(tr);
    });
    return;
  }

  // ç·åˆ
  const rows=(j.rows||[]).slice().sort((a,b)=>(b.score||0)-(a.score||0));
  thead.innerHTML=`<tr><th>#</th><th>éŠ˜æŸ„</th><th>å¢—è³‡çµŒæ­´</th><th>ã‚¹ã‚³ã‚¢</th><th>Trends</th><th>BBS</th><th>News(24h)</th><th>Sentiment</th><th>æœ€æ–°ãƒ‹ãƒ¥ãƒ¼ã‚¹</th></tr>`;
  rows.forEach((r,idx)=>{
    const tr=document.createElement("tr"); const t=(h)=>{const el=document.createElement("td"); el.innerHTML=h; tr.appendChild(el);};
    const items=(r.news&&r.news.items)?r.news.items:[]; const label=(items[0]?.sentiment)||"neutral";
    t(String(idx+1));
    t(`${esc(r.name||r.ticker)} <span class="code">(${esc(r.ticker)})</span>`);
    t(offeringCell(r.has_offering_history));
    t(fmt(r.score));
    t(fmt(r.trends?.latest));
    t(String(r.bbs?.posts_24h||0));
    t(String(r.news?.count_24h||0));
    t(badge(label));
    t(toNewsCards(items));
    tbody.appendChild(tr);
  });
}

  document.addEventListener("DOMContentLoaded", ()=>{
    document.querySelectorAll(".tab").forEach(btn=>{
      btn.addEventListener("click", ()=> { MODE=btn.dataset.mode; render(window.__DATA__); document.querySelectorAll(".tab").forEach(b=>b.classList.toggle("active", b===btn)); });
    });
    document.getElementById("btn-rerender").addEventListener("click", ()=> render(window.__DATA__));
    render(window.__DATA__);
  });
</script>
    """

    return tpl.replace("__DATA_JSON__", data_json)

import io, re, time, requests
from typing import Dict, Any, List

# ä¸€åº¦ã®å®Ÿè¡Œã§è¦ç´„ã™ã‚‹æœ€å¤§ä»¶æ•°ï¼ˆè² è·å¯¾ç­–ï¼‰
EARNINGS_SUMMARY_MAX = 40

def _download_pdf_bytes(url: str, timeout: int = 25) -> bytes:
    try:
        r = requests.get(url, timeout=timeout, headers={"User-Agent":"Mozilla/5.0"}, allow_redirects=True)
        r.raise_for_status()
        return r.content or b""
    except Exception as e:
        print(f"[pdf] download error: {e} url={url}")
        return b""

def _extract_text_pdfminer(pdf_bytes: bytes) -> str:
    try:
        from pdfminer_high_level import extract_text
    except Exception:
        try:
            from pdfminer.high_level import extract_text
        except Exception:
            extract_text = None
    if extract_text is None:
        return ""
    try:
        with io.BytesIO(pdf_bytes) as bio:
            return (extract_text(bio) or "").strip()
    except Exception as e:
        print(f"[pdfminer] extract error: {e}")
        return ""

def _extract_text_pypdf2(pdf_bytes: bytes) -> str:
    try:
        import PyPDF2
        out = []
        reader = PyPDF2.PdfReader(io.BytesIO(pdf_bytes))
        for page in reader.pages:
            try:
                out.append(page.extract_text() or "")
            except Exception:
                out.append("")
        return "\n".join(out).strip()
    except Exception as e:
        print(f"[PyPDF2] extract error: {e}")
        return ""

def _extract_text_from_pdf(pdf_bytes: bytes) -> str:
    if not pdf_bytes:
        return ""
    text = _extract_text_pdfminer(pdf_bytes)
    if text and len(text) > 30:
        return text
    text = _extract_text_pypdf2(pdf_bytes)
    return text
    
def _safe_extract_pdf_text(pdf_bytes: bytes) -> str:
    try:
        txt = _extract_text_from_pdf(pdf_bytes)
        if isinstance(txt, str) and txt.strip():
            return txt
    except Exception:
        pass
    return ""


def _parse_earnings_metrics(text: str) -> Dict[str, Any]:
    if not text:
        return {"metrics": {}, "progress": None}

    def _num(s):
        s = s.replace(",", "").replace("ï¼Œ", "").replace("ï¼…", "%").replace("â–²", "-").replace("â–³", "-")
        s = re.sub(r"[^\d\.\-\+%]", "", s)
        try:
            if s.endswith("%"):
                return float(s[:-1])
            return float(s)
        except:
            return None

    UNIT_PAT = r"(ç™¾ä¸‡å††|å„„å††|ä¸‡å††|å††)?"

    fields = {
        "å£²ä¸Šé«˜": r"(å£²ä¸Šé«˜)[^0-9\-ï¼‹\+\,\.ï¼…%\(\)]*([0-9,\.\-\+]+)\s*" + UNIT_PAT,
        "å–¶æ¥­åˆ©ç›Š": r"(å–¶æ¥­åˆ©ç›Š)[^0-9\-ï¼‹\+\,\.ï¼…%\(\)]*([0-9,\.\-\+]+)\s*" + UNIT_PAT,
        "çµŒå¸¸åˆ©ç›Š": r"(çµŒå¸¸åˆ©ç›Š)[^0-9\-ï¼‹\+\,\.ï¼…%\(\)]*([0-9,\.\-\+]+)\s*" + UNIT_PAT,
        "ç´”åˆ©ç›Š": r"(å½“æœŸç´”åˆ©ç›Š|è¦ªä¼šç¤¾æ ªä¸»ã«å¸°å±ã™ã‚‹å½“æœŸç´”åˆ©ç›Š|ç´”åˆ©ç›Š)[^0-9\-ï¼‹\+\,\.ï¼…%\(\)]*([0-9,\.\-\+]+)\s*" + UNIT_PAT,
        "EPS": r"(EPS|1æ ªå½“ãŸã‚Šå½“æœŸç´”åˆ©ç›Š)[^0-9\-ï¼‹\+\,\.ï¼…%\(\)]*([0-9,\.\-\+]+)\s*" + UNIT_PAT,
        "é€²æ—ç‡": r"(é€²æ—ç‡)[^0-9\-ï¼‹\+\,\.ï¼…%\(\)]*([0-9,\.\-\+]+)\s*%",
    }

    metrics = {}
    yoy = {}

    def _scan_yoy_near(pos: int, text: str):
        if pos < 0: return None
        s = text[max(0, pos-120): pos+220]
        s = s.replace('ï¼…','%').replace('â–²','-').replace('â–³','-')
        m = re.search(r'([\-+]?\d+(?:\.\d+)?)\s*%', s)
        if not m: return None
        try:
            val = float(m.group(1))
        except Exception:
            return None
        around = s[max(0, m.start()-10): m.end()+10]
        if ('-' not in m.group(1) and '+' not in m.group(1)):
            if re.search(r'(æ¸›|æ‚ªåŒ–|ç¸®å°)', around):
                val = -val
        return val

    for key, pat in fields.items():
        m = re.search(pat, text, flags=re.IGNORECASE)
        if m:
            raw = m.group(2) if len(m.groups()) >= 2 else None
            val = _num(raw or "")
            if val is not None:
                unit = m.group(3) if len(m.groups()) >= 3 else ""
                if key != "EPS" and key != "é€²æ—ç‡" and unit:
                    u = unit
                    if "ç™¾ä¸‡å††" in u:
                        val = val / 100.0
                metrics[key] = val

            try:
                pos = text.find(key)
                y = _scan_yoy_near(pos, text)
                if y is not None:
                    yoy[key] = y
            except Exception:
                pass

    progress = None
    if "é€²æ—ç‡" in metrics:
        progress = float(metrics["é€²æ—ç‡"])
    return {"metrics": metrics, "progress": progress, "yoy": yoy}

def _summarize_earnings_text(text: str, max_chars: int = 240) -> str:
    if not text:
        return ""
    lines = [l.strip() for l in text.splitlines() if l.strip()]
    KEY = [
        "ä¸Šæ–¹ä¿®æ­£", "ä¸‹æ–¹ä¿®æ­£", "é€šæœŸ", "å››åŠæœŸ", "å¢—ç›Š", "æ¸›ç›Š", "å¢—å", "æ¸›å",
        "é€²æ—ç‡", "é…å½“", "æ¥­ç¸¾äºˆæƒ³", "ä¿®æ­£"
    ]
    hits = [l for l in lines if any(k in l for k in KEY)]
    base = " / ".join(hits[:4]) if hits else " ".join(lines[:6])
    base = re.sub(r"\s+", " ", base).strip()
    return base[:max_chars]

def _grade_earnings(title: str, text: str, parsed: Dict[str, Any]) -> Dict[str, Any]:
    reasons: list[str] = []
    score = 0.0

    T = title or ""
    X = text or ""
    met = (parsed or {}).get("metrics", {})
    prog = (parsed or {}).get("progress", None)
    yoy = (parsed or {}).get("yoy", {})

    def _hit(ws):
        return any((w in T) or (w in X) for w in ws)

    if _hit(["ä¸Šæ–¹ä¿®æ­£", "é€šæœŸä¸Šæ–¹", "é€šæœŸå¢—é¡", "ä¸ŠæœŸäºˆæƒ³ä¿®æ­£ï¼ˆå¢—é¡ï¼‰",
             "æ¥­ç¸¾äºˆæƒ³ã®ä¿®æ­£ï¼ˆå¢—é¡ï¼‰", "é€šæœŸäºˆæƒ³ä¿®æ­£ï¼ˆå¢—é¡ï¼‰"]):
        score += 2; reasons.append("ä¸Šæ–¹ä¿®æ­£ï¼ˆå¢—é¡ï¼‰ã§+2")

    if _hit(["å¢—é…", "å¾©é…", "è‡ªç¤¾æ ªè²·ã„", "é…å½“äºˆæƒ³ã®ä¿®æ­£ï¼ˆå¢—é¡ï¼‰"]):
        score += 1; reasons.append("é…å½“å¢—é¡/å¾©é…/è‡ªç¤¾æ ªè²·ã„ã§+1")

    if _hit(["æœ€é«˜ç›Š", "éå»æœ€é«˜ç›Š"]):
        score += 1; reasons.append("æœ€é«˜ç›Šã§+1")

    if _hit(["é»’å­—è»¢æ›"]):
        score += 1; reasons.append("é»’å­—è»¢æ›ã§+1")

    if _hit(["ä¸‹æ–¹ä¿®æ­£", "é€šæœŸäºˆæƒ³ä¿®æ­£ï¼ˆæ¸›é¡ï¼‰"]):
        score -= 3; reasons.append("ä¸‹æ–¹ä¿®æ­£ï¼ˆæ¸›é¡ï¼‰ã§-3")

    if _hit(["æ¸›é…", "é…å½“äºˆæƒ³ã®ä¿®æ­£ï¼ˆæ¸›é¡ï¼‰"]):
        score -= 2; reasons.append("æ¸›é…ã§-2")

    if _hit(["ç‰¹æ", "ç‰¹åˆ¥æå¤±"]):
        score -= 2; reasons.append("ç‰¹æã§-2")

    if isinstance(prog, (int, float)):
        if prog >= 70:
            score += 1; reasons.append(f"é€²æ—ç‡{prog:.0f}%ï¼ˆé«˜é€²æ—ï¼‰")
        elif prog <= 30:
            score -= 1; reasons.append(f"é€²æ—ç‡{prog:.0f}%ï¼ˆä½é€²æ—ï¼‰")

    def _sane(y):
        return y if isinstance(y, (int, float)) and -120.0 <= y <= 120.0 else None

    neg_profit_yoy = 0
    pos_profit_yoy = 0
    for k in ("å£²ä¸Šé«˜", "å–¶æ¥­åˆ©ç›Š", "çµŒå¸¸åˆ©ç›Š", "ç´”åˆ©ç›Š"):
        y = _sane(yoy.get(k))
        if y is None:
            continue
        if k in ("å–¶æ¥­åˆ©ç›Š", "çµŒå¸¸åˆ©ç›Š", "ç´”åˆ©ç›Š"):
            if y <= -5: neg_profit_yoy += 1
            if y >= 1:  pos_profit_yoy += 1
        if y >= 5:
            score += 1;   reasons.append(f"{k}YoY+{y:.1f}%")
        elif y >= 1:
            score += 0.5; reasons.append(f"{k}YoY+{y:.1f}%")
        elif y <= -5:
            score -= 1;   reasons.append(f"{k}YoY{y:.1f}%")
        elif y <= -1:
            score -= 0.5; reasons.append(f"{k}YoY{y:.1f}%")

    if neg_profit_yoy >= 2:
        score -= 1.0; reasons.append("åˆ©ç›ŠYoYãŒè¤‡æ•°é …ç›®ã§æ¸›å°‘ï¼ˆç·ã˜ã¦æ¸›ç›Šå‚¾å‘ï¼‰")

    for k in ("å–¶æ¥­åˆ©ç›Š", "çµŒå¸¸åˆ©ç›Š", "ç´”åˆ©ç›Š"):
        v = met.get(k, None)
        if isinstance(v, (int, float)):
            if v > 0:
                if pos_profit_yoy > 0:
                    score += 0.3; reasons.append(f"{k}ãŒé»’å­—ï¼ˆå¢—ç›Šå‚¾å‘ã¨æ•´åˆï¼‰")
                else:
                    reasons.append(f"{k}ãŒé»’å­—ï¼ˆå˜ç‹¬è¦ç´ ã®ãŸã‚åŠ ç‚¹ãªã—ï¼‰")
            if v < 0:
                score -= 0.5; reasons.append(f"{k}ãŒèµ¤å­—")

    verdict = "good" if score >= 1.5 else ("bad" if score <= -1.5 else "neutral")
    return {"verdict": verdict, "score": score, "reasons": reasons}

# ========= ã“ã“ã¾ã§ãƒ˜ãƒ«ãƒ‘ãƒ¼ =========
def fetch_tdnet_by_keywords(
    days: int = 90,
    keywords: list[str] | None = None,
    per_day_limit: int = 300,
    slice_escalation=(12, 6, 3, 1, 0.5)
) -> List[Dict[str, Any]]:
    import time
    from datetime import datetime, timedelta

    time_capable = False
    KEY_RE_LOCAL = re.compile("|".join(map(re.escape, keywords or []))) if (keywords and len(keywords) > 0) else None

    def _fetch_tdnet_by_range(dt_from: datetime, dt_to: datetime) -> list[dict]:
        nonlocal time_capable
        from urllib.parse import quote

        s_day  = dt_from.strftime("%Y%m%d"); e_day  = dt_to.strftime("%Y%m%d")
        s_isoT = dt_from.strftime("%Y-%m-%dT%H:%M:%S"); e_isoT = dt_to.strftime("%Y-%m-%dT%H:%M:%S")
        s_nosep= dt_from.strftime("%Y%m%d%H%M%S");      e_nosep= dt_to.strftime("%Y%m%d%H%M%S")
        s_spc  = dt_from.strftime("%Y%m%d %H:%M:%S");   e_spc  = dt_to.strftime("%Y%m%d %H:%M:%S")

        # ã¾ãšã¯å¸¸ã«å®‰å®šã™ã‚‹æ—¥å˜ä½API
        url_day = f"https://webapi.yanoshin.jp/webapi/tdnet/list/{s_day}-{e_day}.json"
        js_day = _http_get_json(url_day)
        items_day = js_day.get("items") if isinstance(js_day, dict) else (js_day if isinstance(js_day, list) else None)
        if isinstance(items_day, list):
            print(f"[tdnet] {dt_from:%Y-%m-%d %H:%M}~{dt_to:%H:%M} : api={len(items_day)} (fmt=list/day)")
            return items_day or []

        variants = [
            (f"https://webapi.yanoshin.jp/webapi/tdnet/list/{s_day}-{e_day}.json",     "list/day",  False),
            (f"https://webapi.yanoshin.jp/webapi/tdnet/list/{s_nosep}-{e_nosep}.json", "list/nosep", True),
            (f"https://webapi.yanoshin.jp/webapi/tdnet/list/{s_isoT}-{e_isoT}.json",   "list/isoT",  True),
            (f"https://webapi.yanoshin.jp/webapi/tdnet/list/{quote(s_spc)}-{quote(e_spc)}.json", "list/space", True),
            (f"https://webapi.yanoshin.jp/webapi/tdnet/list_time/{s_nosep}-{e_nosep}.json", "list_time/nosep", True),
        ]
        for url, tag, is_time in variants:
            js = _http_get_json(url)
            items = js.get("items") if isinstance(js, dict) else (js if isinstance(js, list) else None)
            if isinstance(items, list):
                if is_time:
                    time_capable = True
                print(f"[tdnet] {dt_from:%Y-%m-%d %H:%M}~{dt_to:%H:%M} : api={len(items)} (fmt={tag})")
                return items or []
        print(f"[tdnet] {dt_from:%Y-%m-%d %H:%M}~{dt_to:%H:%M} : api=None (all time-formats failed)")
        return []

    def _fetch_day_extra_pages(s_day: str, e_day: str, page_try: int = 15) -> list[dict]:
        collected = []
        base = f"https://webapi.yanoshin.jp/webapi/tdnet/list/{s_day}-{e_day}.json"
        patterns = ["?page={p}", "?p={p}"]
        for p in range(2, 2 + max(1, page_try)):
            hit = 0
            for fmt in patterns:
                url = base + fmt.format(p=p)
                js = _http_get_json(url)
                items = js.get("items") if isinstance(js, dict) else (js if isinstance(js, list) else None)
                if isinstance(items, list) and items:
                    collected.extend(items); hit += len(items)
                    print(f"[tdnet]   extra page p={p} via '{fmt[1:4]}' -> {len(items)}")
                    break
            if hit == 0:
                break
            time.sleep(0.15)
        return collected

    def _dedup_key(it: dict) -> str:
        td = it.get("Tdnet") or it
        return (td.get("id") or td.get("document_url") or td.get("title") or "") + "|" + (td.get("pubdate") or "")

    def _slice_fetch(dt_from: datetime, dt_to: datetime, level: int = 0) -> list[dict]:
        items = _fetch_tdnet_by_range(dt_from, dt_to)
        if len(items) < per_day_limit:
            return items
        if not time_capable:
            if (dt_from.hour, dt_from.minute, dt_from.second) == (0, 0, 0) and (dt_to.hour, dt_to.minute, dt_to.second) == (23, 59, 59) and dt_from.date() == dt_to.date():
                s_day = dt_from.strftime("%Y%m%d"); e_day = dt_to.strftime("%Y%m%d")
                extras = _fetch_day_extra_pages(s_day, e_day)
                if extras:
                    print(f"[tdnet] day extra merged: base=300 + extras={len(extras)}")
                    return items + extras
            print(f"[tdnet] time-capability=NO â†’ stop slicing ({dt_from:%F} {dt_from:%H:%M}~{dt_to:%H:%M}), got={len(items)}")
            return items
        if level >= len(slice_escalation):
            print(f"[tdnet] slice exhausted ({dt_from:%F} {dt_from:%H:%M}~{dt_to:%H:%M}), got={len(items)} >= limit={per_day_limit}")
            return items
        width_h = slice_escalation[level]
        step_sec = int(max(width_h * 3600, 1800))
        parts: list[dict] = []
        cur = dt_from
        while cur < dt_to:
            nxt = min(cur + timedelta(seconds=step_sec), dt_to)
            parts.extend(_slice_fetch(cur, nxt, level + 1))
            cur = nxt
            time.sleep(0.12)
        print(f"[tdnet] api-slice level={level} w={width_h}h -> merged={len(parts)}")
        return parts

    def _ts(x: dict) -> float:
        td = x.get("Tdnet") or {}
        s = (td.get("pubdate") or "").replace("/", "-")
        try:
            return datetime.strptime(s, "%Y-%m-%d %H:%M:%S").timestamp()
        except Exception:
            return 0.0

    all_items: list[dict] = []
    seen: set[str] = set()
    end = datetime.now(JST).replace(microsecond=0)

    for i in range(days):
        day_to   = (end - timedelta(days=i)).replace(hour=23, minute=59, second=59)
        day_from = (end - timedelta(days=i)).replace(hour=0,  minute=0,  second=0)
        day_items = _slice_fetch(day_from, day_to)

        kept = 0
        for it in day_items:
            td = it.get("Tdnet") or it
            title = _unescape_text(td.get("title") or "")
            if KEY_RE_LOCAL and title and not KEY_RE_LOCAL.search(title):
                continue
            td["title"] = title
            k = _dedup_key(it)
            if not k or k in seen:
                continue
            seen.add(k)
            all_items.append(it); kept += 1

        print(f"[tdnet] {day_from:%Y-%m-%d} kept={kept} / raw={len(day_items)}")
        time.sleep(0.2)

    all_items.sort(key=_ts, reverse=True)
    return all_items

def tdnet_items_to_earnings_rows(tdnet_items):
    rows = []
    summarized = 0

    for it in tdnet_items:
        td = it.get("Tdnet", it) or {}

        title = (td.get("title") or "").strip()
        name  = (td.get("company_name") or "").strip()
        code_raw = str(td.get("company_code") or td.get("code") or "").strip()
        if re.fullmatch(r"\d{5}", code_raw):
            ticker = code_raw[:4]
        elif re.fullmatch(r"\d{4}", code_raw):
            ticker = code_raw
        else:
            ticker = ""

        link = (td.get("document_url") or td.get("pdf_url") or td.get("url") or "").strip()
        time_str = (td.get("pubdate") or td.get("publish_datetime") or "").replace("T"," ").replace("+09:00","")

        summary = (td.get("summary") or "").strip()
        reason  = (td.get("reason")  or "").strip()
        label_guess = "neutral"

        s_t, label_from_title, why_t = _summarize_title_simple(title)
        if label_from_title in ("positive", "negative"):
            label_guess = label_from_title
        if not summary and s_t:
            summary = s_t
        if not reason and why_t:
            reason = why_t

        verdict = (td.get("verdict") or "").strip()
        score_judge = td.get("score_judge")
        reasons = td.get("reasons")
        metrics = td.get("metrics")
        progress = td.get("progress")

        should_try_pdf = bool(link)
        if should_try_pdf and summarized < EARNINGS_SUMMARY_MAX:
            try:
                pdf_bytes = _download_pdf_bytes(link)
                if pdf_bytes:
                    text = _extract_text_from_pdf(pdf_bytes)
                    if text:
                        parsed = _parse_earnings_metrics(text)
                        metrics = parsed.get("metrics", {})
                        progress = parsed.get("progress")
                        sum2 = _summarize_earnings_text(text)
                        if sum2:
                            summary = sum2
                        judge = _grade_earnings(title, text, parsed)
                        verdict = judge["verdict"]
                        score_judge = judge["score"]
                        reasons = judge["reasons"]
                        if verdict == "good":
                            label_guess = "positive"
                        elif verdict == "bad":
                            label_guess = "negative"
                        summarized += 1
                time.sleep(0.15)
            except Exception as e:
                print(f"[earnings] PDF summarize error: {e}")

        if isinstance(reasons, str) and reasons:
            reasons = [reasons]
        if reasons is None:
            reasons = ([reason] if reason else [])

        sentiment = td.get("sentiment") or label_guess or "neutral"

        row = {
            "ticker": ticker or "0000",
            "name":   name or (ticker or "TDnet"),
            "score":  0,
            "sentiment": sentiment,
            "title": title,
            "link":  link,
            "time":  time_str or "",
            "summary": summary,
            "reasons": reasons,
            "verdict": verdict,
            "score_judge": score_judge,
            "metrics": metrics or {},
            "progress": progress,
        }
        rows.append(row)

    def _ts(s):
        try:
            return datetime.strptime((s or "").replace("/","-"), "%Y-%m-%d %H:%M:%S").timestamp()
        except:
            return 0
    rows.sort(key=lambda r: _ts(r["time"]), reverse=True)

    try:
        rows = summarize_earnings_rows_parallel(rows, max_items=EARNINGS_SUMMARY_MAX, max_workers=8)
    except Exception as _e:
        print('[parallel] skipped:', _e)
    return rows

# ------------------ ã‚¹ã‚³ã‚¢è¨ˆç®—ï¼ˆç·åˆ/æ²ç¤ºæ¿ï¼‰ ------------------
def build_scores(rows, weights):
    wt = float(weights.get("trends", 1.0))
    wb = float(weights.get("bbs_growth", 1.0))
    wn = float(weights.get("news", 1.0))

    scores = []
    for r in rows:
        t = 0.0
        try:
            t = float((r.get("trends") or {}).get("latest", 0.0) or 0.0)
        except Exception:
            t = 0.0

        b24 = int((r.get("bbs") or {}).get("posts_24h", 0) or 0)
        b72 = int((r.get("bbs") or {}).get("posts_72h", 0) or 0)
        bbs_growth = b24 / max(1, b72)
        n = int((r.get("news") or {}).get("count_24h", 0) or 0)

        score = wt * t + wb * bbs_growth + wn * n
        scores.append(score)
    return scores

def build_bbs_scores(rows):
    out = []
    for r in rows:
        b24 = int((r.get("bbs") or {}).get("posts_24h", 0) or 0)
        b72 = int((r.get("bbs") or {}).get("posts_72h", 0) or 0)
        out.append(b24 / max(1, b72))
    return out

# ------------------ main ------------------

def _table_exists(conn, name: str) -> bool:
    try:
        cur = conn.execute("SELECT 1 FROM sqlite_master WHERE type='table' AND name=? LIMIT 1", (name,))
        return cur.fetchone() is not None
    except Exception:
        return False

def ensure_misc_objects(conn):
    """
    è¿½åŠ ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ / ãƒˆãƒªã‚¬ / ãƒ“ãƒ¥ãƒ¼ã®ä½œæˆï¼ˆå­˜åœ¨ãƒã‚§ãƒƒã‚¯ä»˜ãï¼‰
    """
    cur = conn.cursor()

    if _table_exists(conn, "signals_log"):
        cur.execute("""
            CREATE INDEX IF NOT EXISTS idx_signals_log_type_date
            ON signals_log(ç¨®åˆ¥, substr(æ—¥æ™‚,1,10));
        """)
        cur.execute("""
            CREATE INDEX IF NOT EXISTS idx_signals_log_code_date
            ON signals_log(ã‚³ãƒ¼ãƒ‰, substr(æ—¥æ™‚,1,10));
        """)

    if _table_exists(conn, "earnings_events"):
        cur.execute("""
            CREATE INDEX IF NOT EXISTS idx_earn_date_only
            ON earnings_events(substr(æå‡ºæ™‚åˆ»,1,10));
        """)
        cur.execute("""
            CREATE INDEX IF NOT EXISTS idx_earn_code_date
            ON earnings_events(ã‚³ãƒ¼ãƒ‰, substr(æå‡ºæ™‚åˆ»,1,10));
        """)

    if _table_exists(conn, "price_history"):
        try:
            cols = [r[1] for r in cur.execute("PRAGMA table_info(price_history)").fetchall()]
            price_date_col = "æ—¥ä»˜" if "æ—¥ä»˜" in cols else ("date" if "date" in cols else None)
        except Exception:
            price_date_col = None

        if price_date_col:
            cur.execute("DROP TRIGGER IF EXISTS trg_price_history_date_norm_ins;")
            cur.execute("DROP TRIGGER IF EXISTS trg_price_history_date_norm_upd;")
            cur.execute(f"""
                CREATE TRIGGER IF NOT EXISTS trg_price_history_date_norm_ins
                AFTER INSERT ON price_history
                BEGIN
                  UPDATE price_history
                     SET "{price_date_col}" =
                         COALESCE(
                           date(replace(replace(NEW."{price_date_col}",'/','-'),'.','-')),
                           substr(replace(replace(NEW."{price_date_col}",'/','-'),'.','-'),1,10)
                         )
                   WHERE rowid = NEW.rowid;
                END;
            """)
            cur.execute(f"""
                CREATE TRIGGER IF NOT EXISTS trg_price_history_date_norm_upd
                AFTER UPDATE OF "{price_date_col}" ON price_history
                BEGIN
                  UPDATE price_history
                     SET "{price_date_col}" =
                         COALESCE(
                           date(replace(replace(NEW."{price_date_col}",'/','-'),'.','-')),
                           substr(replace(replace(NEW."{price_date_col}",'/','-'),'.','-'),1,10)
                         )
                   WHERE rowid = NEW.rowid;
                END;
            """)

    cur.execute("""
        CREATE VIEW IF NOT EXISTS v_events_dedup AS
        WITH max_ts AS (
          SELECT
            ã‚³ãƒ¼ãƒ‰,
            substr(æå‡ºæ™‚åˆ»,1,10) AS æå‡ºæ—¥,
            MAX(æå‡ºæ™‚åˆ») AS max_æå‡ºæ™‚åˆ»
          FROM earnings_events
          GROUP BY ã‚³ãƒ¼ãƒ‰, æå‡ºæ—¥
        )
        SELECT e.*
          FROM earnings_events e
          JOIN max_ts m
            ON e.ã‚³ãƒ¼ãƒ‰ = m.ã‚³ãƒ¼ãƒ‰
           AND substr(e.æå‡ºæ™‚åˆ»,1,10) = m.æå‡ºæ—¥
           AND e.æå‡ºæ™‚åˆ» = m.max_æå‡ºæ™‚åˆ»;
    """)
    conn.commit()

def run_light_healthcheck(conn):
    result = {"ok": True}
    try:
        last_date = None
        dcol = None
        if _table_exists(conn, "price_history"):
            cols = [r[1] for r in conn.execute("PRAGMA table_info(price_history)").fetchall()]
            dcol = "æ—¥ä»˜" if "æ—¥ä»˜" in cols else ("date" if "date" in cols else None)
            if dcol:
                last_date = conn.execute(f'SELECT MAX("{dcol}") FROM price_history').fetchone()[0]

        cov = None
        if last_date and dcol:
            cov = conn.execute(
                f'SELECT COUNT(DISTINCT "ã‚³ãƒ¼ãƒ‰") FROM price_history WHERE "{dcol}"=?',
                (last_date,)
            ).fetchone()[0]

        last_event_ts = None
        if _table_exists(conn, "earnings_events"):
            last_event_ts = conn.execute("SELECT MAX(æå‡ºæ™‚åˆ») FROM earnings_events").fetchone()[0]
        result.update({
            "price_last_date": last_date,
            "price_coverage_codes": cov,
            "earnings_last_ts": last_event_ts
        })
    except Exception as e:
        result["ok"] = False
        result["error"] = str(e)

    if _table_exists(conn, "signals_log"):
        try:
            from datetime import datetime
            conn.execute(
                "INSERT INTO signals_log(ã‚³ãƒ¼ãƒ‰, ç¨®åˆ¥, æ—¥æ™‚, è©³ç´°) VALUES(?, ?, ?, ?)",
                ("", "healthcheck",
                 datetime.utcnow().isoformat(),
                 json.dumps(result, ensure_ascii=False))
            )
            conn.commit()
        except Exception:
            pass

    return result

def _detect_price_cols(conn: sqlite3.Connection):
    cur = conn.cursor()
    cur.execute("PRAGMA table_info(price_history);")
    cols = [r[1] for r in cur.fetchall()]
    if not cols:
        return None, None, None

    def pick(*cands):
        for c in cands:
            if c in cols:
                return c
        return None

    date_col = pick("æ—¥ä»˜", "date")
    code_col = pick("ã‚³ãƒ¼ãƒ‰", "code", "ticker")
    price_col = pick("çµ‚å€¤", "çµ‚å€¤èª¿æ•´å¾Œ", "close", "Close", "çµ‚å€¤(å††)", "çµ‚å€¤_å††")

    return date_col, code_col, price_col

def load_quotes_from_price_history(conn: sqlite3.Connection, codes: list[str]) -> dict[str, dict]:
    out: dict[str, dict] = {}
    if not codes:
        return out

    date_col, code_col, price_col = _detect_price_cols(conn)
    if not all([date_col, code_col, price_col]):
        return out

    qmarks = ",".join("?" for _ in codes)
    cur = conn.cursor()

    latest = dict(cur.execute(
        f'SELECT "{code_col}", MAX("{date_col}") '
        f'FROM price_history WHERE "{code_col}" IN ({qmarks}) GROUP BY "{code_col}"',
        codes
    ).fetchall())

    for code, max_d in latest.items():
        rows = cur.execute(
            f'''SELECT "{price_col}" FROM price_history
                WHERE "{code_col}"=? AND "{date_col}"<=?
                ORDER BY "{date_col}" DESC LIMIT 2''',
            (code, max_d)
        ).fetchall()
        if not rows:
            continue
        cur_px = rows[0][0]
        prev_px = rows[1][0] if len(rows) >= 2 else None

        try:
            cur_px = float(cur_px) if cur_px is not None else None
        except Exception:
            cur_px = None
        try:
            prev_px = float(prev_px) if prev_px is not None else None
        except Exception:
            prev_px = None

        diff = None
        if cur_px is not None and prev_px is not None:
            diff = cur_px - prev_px

        out[str(code).zfill(4)] = {
            "current": cur_px,
            "prev_close": prev_px,
            "diff_yen": diff,
        }
    return out

# === Parallel PDF summarization for earnings rows ===
def _summarize_one_pdf_row(row: dict) -> dict:
    link = (row or {}).get("link") or (row or {}).get("pdf") or ""
    if not link:
        return row
    try:
        b = _download_pdf_bytes(link)
        if not b:
            return row
        try:
            text = _safe_extract_pdf_text(b)
        except NameError:
            return row
        if not text:
            return row

        parsed = {}
        if "_parse_earnings_metrics" in globals():
            try:
                parsed = _parse_earnings_metrics(text) or {}
            except Exception as e:
                print("[earnings] _parse_earnings_metrics err:", e)

        sum2 = None
        if "_summarize_earnings_text" in globals():
            try:
                sum2 = _summarize_earnings_text(text)
            except Exception as e:
                print("[earnings] _summarize_earnings_text err:", e)

        judge = {"verdict": None, "score": None, "reasons": []}
        if "_grade_earnings" in globals():
            try:
                judge = _grade_earnings(row.get("title",""), text, parsed)
            except Exception as e:
                print("[earnings] _grade_earnings err:", e)

        out = dict(row)
        if sum2: out["summary"] = sum2
        if isinstance(parsed, dict):
            out["metrics"]  = parsed.get("metrics", parsed)
            out["progress"] = parsed.get("progress", out.get("progress"))
        if isinstance(judge, dict):
            out["verdict"]     = judge.get("verdict", out.get("verdict"))
            out["score_judge"] = judge.get("score", out.get("score_judge"))
            out["reasons"]     = judge.get("reasons", out.get("reasons", []))
            if out.get("verdict") == "good":
                out["sentiment"] = "positive"
            elif out.get("verdict") == "bad":
                out["sentiment"] = "negative"
        return out
    except Exception as e:
        print(f"[earnings] summarize error: {e} url={link}")
        return row

def summarize_earnings_rows_parallel(rows: list, max_items: int = None, max_workers: int = 8):
    if not isinstance(rows, list) or not rows:
        return rows
    try:
        N = int(EARNINGS_SUMMARY_MAX)
    except Exception:
        N = 20
    if isinstance(max_items, int) and max_items > 0:
        N = max_items

    targets = [(i, r) for i, r in enumerate(rows[:N]) if isinstance(r, dict) and (r.get("link") or r.get("pdf"))]
    if not targets:
        return rows
    try:
        with ThreadPoolExecutor(max_workers=max_workers) as ex:
            fut2idx = {ex.submit(_summarize_one_pdf_row, r): i for i, r in targets}
            for fut in as_completed(fut2idx):
                i = fut2idx[fut]
                try:
                    rows[i] = fut.result()
                except Exception as e:
                    print("[earnings] future err:", e)
    except Exception as e:
        print("[earnings] parallel block err:", e)
    return rows

def upsert_earnings_rows_fast(conn, rows):
    """Fast path: WAL + single transaction. Call original upsert_earnings_rows()."""
    cur = conn.cursor()
    try:
        cur.execute("PRAGMA journal_mode=WAL;")
    except Exception:
        pass
    try:
        cur.execute("PRAGMA synchronous=NORMAL;")
        cur.execute("PRAGMA temp_store=MEMORY;")
        cur.execute("PRAGMA mmap_size=134217728;")
    except Exception:
        pass
    conn.commit()

    conn.execute("BEGIN IMMEDIATE;")
    try:
        upsert_earnings_rows(conn, rows)
        conn.commit()
    except Exception:
        conn.rollback()
        raise
    return None

def main():
    cfg = load_config(str(ROOT / "config.yaml"))

    include_dbg = bool((cfg.get("debug", {}) or {}).get("sentiment", False))
    include_bbs_samples = bool((cfg.get("debug", {}) or {}).get("bbs_samples", False))
    bbs_max_samples = int((cfg.get("bbs", {}) or {}).get("max_samples", 20))

    symbols = build_universe(cfg)

    # ---- ç·åˆ/æ²ç¤ºæ¿ã‚¿ãƒ– ----
    rows = []
    for sym in symbols:
        ticker = sym.get("ticker")
        aliases = sym.get("aliases", [])
        yf_code = sym.get("bbs", {}).get("yahoo_finance_code", ticker)

        try:
            trends = fetch_trends(aliases, timeframe="now 7-d", geo="JP")
        except Exception as e:
            print("[trends][WARN] skip due to error:", e)
            trends = {"latest": 0.0, "series": []}
        news = fetch_news(
            sym.get("news_query"),
            max_items=cfg.get("news", {}).get("max_items_per_symbol", 30),
            include_debug=include_dbg,
        )
        bbs = fetch_bbs_stats(
            yf_code,
            look_threads=cfg.get("bbs", {}).get("look_threads", 50),
            include_samples=include_bbs_samples,
            max_samples=bbs_max_samples,
        )

        if include_dbg:
            pos = sum(1 for it in news["items"] if it.get("sentiment") == "positive")
            neg = sum(1 for it in news["items"] if it.get("sentiment") == "negative")
            neu = sum(1 for it in news["items"] if it.get("sentiment") == "neutral")
            print(f"[{ticker}] news pos/neu/neg = {pos}/{neu}/{neg} (24h={news['count_24h']})")

        rows.append({
            "ticker": ticker,
            "name": sym.get("name"),
            "trends": trends,
            "news": news,
            "bbs": bbs,
        })

    scores = build_scores(rows, cfg.get("weights", {"trends": 1.0, "bbs_growth": 1.0, "news": 1.0}))
    for i, s in enumerate(scores):
        rows[i]["score"] = s
    bbs_scores = build_bbs_scores(rows)
    for i, s in enumerate(bbs_scores):
        rows[i]["score_bbs"] = s

    # ---- æ±ºç®—ï¼ˆTDnetï¼‰----
    try:
        tdnet_items = fetch_earnings_tdnet_only(days=14, per_day_limit=300)
        print(f"[earnings] raw tdnet items = {len(tdnet_items)}")

        earnings_rows = tdnet_items_to_earnings_rows(tdnet_items)
        print(f"[earnings] rows after shaping = {len(earnings_rows)}")

        for it in earnings_rows[:10]:
            print("  -", it.get("time",""), it.get("ticker",""), it.get("title","")[:60])

        if not earnings_rows:
            print("[earnings][fallback] ç›´è¿‘7æ—¥ã§å†å–å¾—ã—ã¾ã™â€¦")
            tdnet_items_fallback = fetch_earnings_tdnet_only(days=30, per_day_limit=300)
            print(f"[earnings][fallback] raw items = {len(tdnet_items_fallback)}")
            earnings_rows = tdnet_items_to_earnings_rows(tdnet_items_fallback)
            print(f"[earnings][fallback] shaped rows = {len(earnings_rows)}")

        if not earnings_rows:
            ensure_dir(OUT_DIR)
            dump_json(tdnet_items, str(OUT_DIR / "tdnet_raw_debug.json"))
            print("[earnings][debug] tdnet_raw_debug.json ã‚’å‡ºåŠ›ã—ã¾ã—ãŸï¼ˆAPIå¿œç­”ã®ä¸­èº«ã‚’ç¢ºèªã§ãã¾ã™ï¼‰")

    except Exception as e:
        print("[earnings] å–å¾—/æ•´å½¢ã§ä¾‹å¤–:", e)
        earnings_rows = []
        
    # ---- å¢—è³‡ãƒ¬ãƒ¼ãƒ³ ----
    offer_items = []
    try:
        offer_items = fetch_tdnet_by_keywords(days=3, keywords=OFFERING_KW, per_day_limit=300)
        print(f"[offerings] raw tdnet items (by KW) = {len(offer_items)}")
    except Exception as e:
        print("[offerings] fetch error:", e)

    # ---- â˜…TOBãƒ¬ãƒ¼ãƒ³ï¼ˆTDNETï¼‰----
    tob_rows = []
    try:
        tob_rows = fetch_tdnet_tob(days=3, per_day_limit=300)
        print(f"[TOB] tdnet rows (by KW) = {len(tob_rows)}")
    except Exception as e:
        print("[TOB] fetch error:", e)

    # ---- DBä¿å­˜ ----
    conn = _get_db_conn(DB_PATH)
    ensure_earnings_schema(conn)
    ensure_offerings_schema(conn)
    ensure_tob_schema(conn)         # â˜…TOBã‚¹ã‚­ãƒ¼ãƒ
    ensure_misc_objects(conn)
    try:
        upsert_offerings_events(conn, offer_items)
    except Exception as e:
        print("[offerings] upsert error:", e)

    try:
        upsert_tob_events(conn, tob_rows)  # â˜…TOB UPSERT
    except Exception as e:
        print("[TOB] upsert error:", e)

    upsert_earnings_rows_fast(conn, earnings_rows)
    run_light_healthcheck(conn)
    conn.close()

    # ---- å‡ºåŠ›ï¼ˆJSON/HTMLï¼‰----
    earnings_for_dash = load_earnings_for_dashboard_from_db(DB_PATH, days=30, filter_mode="nonnegative")

    # --- å¢—è³‡çµŒæ­´ãƒ•ãƒ©ã‚°ä»˜ä¸ï¼ˆç›´è¿‘3å¹´ï¼‰ ---
    try:
        offerings_map = load_offerings_recent_map(DB_PATH, days=365*3)
    except Exception as e:
        print("[offerings] map build error:", e)
        offerings_map = {}

    try:
        for r in rows:
            code4 = str(r.get("ticker") or "").zfill(4)
            r["has_offering_history"] = bool(offerings_map.get(code4, False))
    except Exception as e:
        print("[offerings] annotate rows error:", e)

    try:
        for r in earnings_for_dash:
            code4 = str(r.get("ticker") or "").zfill(4)
            r["has_offering_history"] = bool(offerings_map.get(code4, False))
    except Exception as e:
        print("[offerings] annotate earnings error:", e)
    print(f"[earnings][dash] from DB last 30d = {len(earnings_for_dash)}")
    
    # --- æ²ç¤ºæ¿æ¤œç´¢ï¼šæ¼ã‚Œã¦ã‚‹ï¼Ÿ / TOB(æ²ç¤ºæ¿) ---
    try:
        leak = fetch_leak_comments(DB_PATH, max_pages=60)
    except Exception as e:
        print("[bbs_search][leak] error:", e)
        leak = {"rows": [], "total_comments": 0, "total_tickers": 0, "generated_at": now_iso()}
    try:
        tob_bbs = fetch_tob_comments(DB_PATH, max_pages=60)
    except Exception as e:
        print("[bbs_search][tob] error:", e)
        tob_bbs = {"rows": [], "total_comments": 0, "total_tickers": 0, "generated_at": now_iso()}

    # â–¼â–¼ è¿½åŠ ï¼šTDNETã§æ—¢ã«TOBãŒæ±ºã¾ã£ã¦ã„ã‚‹éŠ˜æŸ„ã‚’æ²ç¤ºæ¿TOBã‚¿ãƒ–ã‹ã‚‰é™¤å¤– â–¼â–¼
    try:
        decided_codes = load_tob_codes_recent(DB_PATH, days=180)  # æœŸé–“ã¯å¿…è¦ã«å¿œã˜ã¦
        if isinstance(tob_bbs, dict) and "rows" in tob_bbs and isinstance(tob_bbs["rows"], list):
            before = len(tob_bbs["rows"])
            tob_bbs["rows"] = [
                r for r in tob_bbs["rows"]
                if str(r.get("ticker") or "").zfill(4) not in decided_codes
            ]
            # ä»¶æ•°ã‚’æ›´æ–°
            tob_bbs["total_tickers"] = len(tob_bbs["rows"])
            print(f"[TOB][filter] decided={len(decided_codes)} codes, rows {before} -> {len(tob_bbs['rows'])}")
    except Exception as e:
        print("[TOB][filter] error:", e)
    # â–²â–² è¿½åŠ ã“ã“ã¾ã§ â–²â–²

    # --- â˜…TDNET TOBã®ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ç”¨ãƒ‡ãƒ¼ã‚¿ ---
    tob_for_dash = []
    try:
        tob_for_dash = load_tob_for_dashboard_from_db(DB_PATH, days=120)
    except Exception as e:
        print("[TOB] load dash error:", e)

    output = {
        "generated_at": now_iso(),
        "rows": rows,
        "earnings": earnings_for_dash,
        "leak": leak,
        "tob": tob_bbs,         # æ²ç¤ºæ¿é›†è¨ˆï¼ˆTDNETæ±ºå®šæ¸ˆã¿ã‚’é™¤å¤–æ¸ˆã¿ï¼‰
        "tob_tdnet": tob_for_dash  # â˜…TDNETç”±æ¥
    }

    ensure_dir(OUT_DIR); ensure_dir(OUT_DIR / "history")
    ensure_dir(OUT_BBS_DIR); ensure_dir(OUT_BBS_DIR / "history")
    dump_json(output, str(OUT_DIR / "data.json"))
    ts = datetime.now(timezone.utc).strftime("%Y%m%d_%H%M")
    dump_json(output, str(OUT_DIR / "history" / f"{ts}.json"))
    dump_json(output, str(OUT_BBS_DIR / "data.json"))
    dump_json(output, str(OUT_BBS_DIR / "history" / f"{ts}.json"))
    print("[write]", (OUT_DIR / "data.json").resolve())

    ensure_dir(DASH_DIR)
    html_out = render_dashboard_html(output, api_base="")
    (DASH_DIR / "index.html").write_text(html_out, encoding="utf-8")
    print("[write]", (DASH_DIR / "index.html").resolve())
    print(f"[OK] {len(rows)} symbols + earnings {len(earnings_for_dash)} (filtered) + TOB(TDNET) {len(tob_for_dash)} â†’ data.json / index.html æ›´æ–°å®Œäº†")

if __name__ == "__main__":
    main()
